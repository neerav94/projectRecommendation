Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Even in traditional general linear models, there is also always the choice of model speciﬁcation in both

variables and functional form. Although multimodality brings new issues to the table, the responsibility

of the researcher to carefully validate the chosen model is fundamentally the same. This is true regardless

of whether the choice between competing models arises due to a non-convex latent variable model or

due to the selection of an important model tuning parameter in a globally convex problem. Thus even if

multimodality is an unfamiliar problem, social scientists can draw on the same set of best practices that

they employ throughout their research.

An important practical contribution of this chapter is that it extends the set of tools available to

scholars using topic models in applied research. While we have focused on STM, many of the procedures

we use will be helpful for a broader class of latent variable models. For instance, the approaches to

aligning topics and calculating stability across runs can all be applied directly to the broader class of

statistical topic models and with minor modiﬁcations to most latent variable models.

We see great potential for the analysis of “big” data in the social sciences, but rather than focus on

the data we have taken a more methodological focus. We think this has important implications for both

methodological development but also could structure the types of questions we ask, and the types of data

sets we seek to build. Methodologically, we think that there will be important advances in areas such

as optimal initialization strategies, which is especially important as our data sets grow in size. From an

applied perspective, users will be unlikely to want to wait for extended periods of time in order to get

even a single set of results. Advances in computational power needs to be matched with smart ways to

leverage that power. From a research design perspective, we think more focus should be put on bringing

greater structure to so called “unstructured” data. In the STM we focus on the inclusion of metadata

for modeling and hypothesis testing, but this is only one possible use. Can more direct supervision help

us with issues of multimodality? Of course, in the end, big data will be at its best when there is active

dialogue between those who pose the big question and those who might provide the big answers.

References
Airoldi, Edoardo M, David M Blei, Stephen E Fienberg and Eric P Xing. 2009. Mixed membership

stochastic blockmodels. In Advances in Neural Information Processing Systems. pp. 33–40.

Anandkumar, Anima, Rong Ge, Daniel Hsu and Sham M Kakade. 2013. “A tensor spectral approach to

learning mixed membership community models.” arXiv preprint arXiv:1302.2684 .

36

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Even in traditional general linear models, there is also always the choice of model speciﬁcation in both

variables and functional form. Although multimodality brings new issues to the table, the responsibility

of the researcher to carefully validate the chosen model is fundamentally the same. This is true regardless

of whether the choice between competing models arises due to a non-convex latent variable model or

due to the selection of an important model tuning parameter in a globally convex problem. Thus even if

multimodality is an unfamiliar problem, social scientists can draw on the same set of best practices that

they employ throughout their research.

An important practical contribution of this chapter is that it extends the set of tools available to

scholars using topic models in applied research. While we have focused on STM, many of the procedures

we use will be helpful for a broader class of latent variable models. For instance, the approaches to

aligning topics and calculating stability across runs can all be applied directly to the broader class of

statistical topic models and with minor modiﬁcations to most latent variable models.

We see great potential for the analysis of “big” data in the social sciences, but rather than focus on

the data we have taken a more methodological focus. We think this has important implications for both

methodological development but also could structure the types of questions we ask, and the types of data

sets we seek to build. Methodologically, we think that there will be important advances in areas such

as optimal initialization strategies, which is especially important as our data sets grow in size. From an

applied perspective, users will be unlikely to want to wait for extended periods of time in order to get

even a single set of results. Advances in computational power needs to be matched with smart ways to

leverage that power. From a research design perspective, we think more focus should be put on bringing

greater structure to so called “unstructured” data. In the STM we focus on the inclusion of metadata

for modeling and hypothesis testing, but this is only one possible use. Can more direct supervision help

us with issues of multimodality? Of course, in the end, big data will be at its best when there is active

dialogue between those who pose the big question and those who might provide the big answers.

References
Airoldi, Edoardo M, David M Blei, Stephen E Fienberg and Eric P Xing. 2009. Mixed membership

stochastic blockmodels. In Advances in Neural Information Processing Systems. pp. 33–40.

Anandkumar, Anima, Rong Ge, Daniel Hsu and Sham M Kakade. 2013. “A tensor spectral approach to

learning mixed membership community models.” arXiv preprint arXiv:1302.2684 .

36

Anandkumar, Anima, Rong Ge, Daniel Hsu, Sham M Kakade and Matus Telgarsky. 2012. “Tensor

decompositions for learning latent variable models.” arXiv preprint arXiv:1210.7559 .

Anandkumar, Anima, Yi-kai Liu, Daniel J Hsu, Dean P Foster and Sham M Kakade. 2012. A spec-
tral algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems.
pp. 917–925.

Anandkumar, Animashree, Daniel Hsu, Adel Javanmard and Sham Kakade. 2013. Learning linear
bayesian networks with latent variables. In Proceedings of The 30th International Conference on Ma-
chine Learning. pp. 249–257.

Anandkumar, Animashree, Daniel Hsu and Sham M Kakade. 2012. “A method of moments for mixture

models and hidden Markov models.” arXiv preprint arXiv:1203.0683 .

Arora, Sanjeev, Aditya Bhaskara, Rong Ge and Tengyu Ma. 2013. “Provable bounds for learning some

deep representations.” arXiv preprint .

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2012. Learning topic models–going beyond svd. In Foun-

dations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE pp. 1–10.

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2013. “New algorithms for learning incoherent and over-

complete dictionaries.” arXiv preprint arXiv:1308.6273 .

Arora, Sanjeev, Rong Ge, Ankur Moitra and Sushant Sachdeva. 2012. Provable ICA with unknown
Gaussian noise, with implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems. pp. 2375–2383.

Arora, Sanjeev, Rong Ge, Ravindran Kannan and Ankur Moitra. 2012. Computing a nonnegative ma-
trix factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of
computing. ACM pp. 145–162.

Arora, Sanjeev, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu
and Michael Zhu. 2013. A Practical Algorithm for Topic Modeling with Provable Guarantees. In
Proceedings of The 30th International Conference on Machine Learning. pp. 280–288.

Arthur, David and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial
and Applied Mathematics pp. 1027–1035.

Asuncion Jr, Arthur Uy. 2011. Distributed and accelerated inference algorithms for probabilistic graph-

ical models. Technical report California State University at Long Beach.

Bahmani, Bahman, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei Vassilvitskii. 2012.

“Scalable k-means++.” Proceedings of the VLDB Endowment 5(7):622–633.

Beck, Nathaniel, Gary King and Langche Zeng. 2000. “Improving quantitative studies of international

conﬂict: A conjecture.” American Political Science Review pp. 21–35.

Belloni, Alexandre, Victor Chernozhukov and Christian Hansen. 2014. “High-Dimensional Methods and

Inference on Structural and Treatment Effects.” The Journal of Economic Perspectives 28(2):29–50.

37

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Even in traditional general linear models, there is also always the choice of model speciﬁcation in both

variables and functional form. Although multimodality brings new issues to the table, the responsibility

of the researcher to carefully validate the chosen model is fundamentally the same. This is true regardless

of whether the choice between competing models arises due to a non-convex latent variable model or

due to the selection of an important model tuning parameter in a globally convex problem. Thus even if

multimodality is an unfamiliar problem, social scientists can draw on the same set of best practices that

they employ throughout their research.

An important practical contribution of this chapter is that it extends the set of tools available to

scholars using topic models in applied research. While we have focused on STM, many of the procedures

we use will be helpful for a broader class of latent variable models. For instance, the approaches to

aligning topics and calculating stability across runs can all be applied directly to the broader class of

statistical topic models and with minor modiﬁcations to most latent variable models.

We see great potential for the analysis of “big” data in the social sciences, but rather than focus on

the data we have taken a more methodological focus. We think this has important implications for both

methodological development but also could structure the types of questions we ask, and the types of data

sets we seek to build. Methodologically, we think that there will be important advances in areas such

as optimal initialization strategies, which is especially important as our data sets grow in size. From an

applied perspective, users will be unlikely to want to wait for extended periods of time in order to get

even a single set of results. Advances in computational power needs to be matched with smart ways to

leverage that power. From a research design perspective, we think more focus should be put on bringing

greater structure to so called “unstructured” data. In the STM we focus on the inclusion of metadata

for modeling and hypothesis testing, but this is only one possible use. Can more direct supervision help

us with issues of multimodality? Of course, in the end, big data will be at its best when there is active

dialogue between those who pose the big question and those who might provide the big answers.

References
Airoldi, Edoardo M, David M Blei, Stephen E Fienberg and Eric P Xing. 2009. Mixed membership

stochastic blockmodels. In Advances in Neural Information Processing Systems. pp. 33–40.

Anandkumar, Anima, Rong Ge, Daniel Hsu and Sham M Kakade. 2013. “A tensor spectral approach to

learning mixed membership community models.” arXiv preprint arXiv:1302.2684 .

36

Anandkumar, Anima, Rong Ge, Daniel Hsu, Sham M Kakade and Matus Telgarsky. 2012. “Tensor

decompositions for learning latent variable models.” arXiv preprint arXiv:1210.7559 .

Anandkumar, Anima, Yi-kai Liu, Daniel J Hsu, Dean P Foster and Sham M Kakade. 2012. A spec-
tral algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems.
pp. 917–925.

Anandkumar, Animashree, Daniel Hsu, Adel Javanmard and Sham Kakade. 2013. Learning linear
bayesian networks with latent variables. In Proceedings of The 30th International Conference on Ma-
chine Learning. pp. 249–257.

Anandkumar, Animashree, Daniel Hsu and Sham M Kakade. 2012. “A method of moments for mixture

models and hidden Markov models.” arXiv preprint arXiv:1203.0683 .

Arora, Sanjeev, Aditya Bhaskara, Rong Ge and Tengyu Ma. 2013. “Provable bounds for learning some

deep representations.” arXiv preprint .

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2012. Learning topic models–going beyond svd. In Foun-

dations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE pp. 1–10.

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2013. “New algorithms for learning incoherent and over-

complete dictionaries.” arXiv preprint arXiv:1308.6273 .

Arora, Sanjeev, Rong Ge, Ankur Moitra and Sushant Sachdeva. 2012. Provable ICA with unknown
Gaussian noise, with implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems. pp. 2375–2383.

Arora, Sanjeev, Rong Ge, Ravindran Kannan and Ankur Moitra. 2012. Computing a nonnegative ma-
trix factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of
computing. ACM pp. 145–162.

Arora, Sanjeev, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu
and Michael Zhu. 2013. A Practical Algorithm for Topic Modeling with Provable Guarantees. In
Proceedings of The 30th International Conference on Machine Learning. pp. 280–288.

Arthur, David and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial
and Applied Mathematics pp. 1027–1035.

Asuncion Jr, Arthur Uy. 2011. Distributed and accelerated inference algorithms for probabilistic graph-

ical models. Technical report California State University at Long Beach.

Bahmani, Bahman, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei Vassilvitskii. 2012.

“Scalable k-means++.” Proceedings of the VLDB Endowment 5(7):622–633.

Beck, Nathaniel, Gary King and Langche Zeng. 2000. “Improving quantitative studies of international

conﬂict: A conjecture.” American Political Science Review pp. 21–35.

Belloni, Alexandre, Victor Chernozhukov and Christian Hansen. 2014. “High-Dimensional Methods and

Inference on Structural and Treatment Effects.” The Journal of Economic Perspectives 28(2):29–50.

37

Bishop, Christopher M et al. 2006. Pattern recognition and machine learning. Vol. 1 springer New York.

Blei, David M. 2012. “Probabilistic topic models.” Communications of the ACM 55(4):77–84.

Blei, David M. 2014. “Build, compute, critique, repeat: data analysis with latent variable models.”

Annual Review of Statistics and Its Application 1:203–232.

Blei, David M, Andrew Y Ng and Michael I Jordan. 2003. “Latent dirichlet allocation.” the Journal of

machine Learning research 3:993–1022.

Boyd, Stephen and Lieven Vandenberghe. 2009. Convex optimization. Cambridge university press.

Carpenter, Bob. 2010. Integrating out multinomial parameters in latent Dirichlet allocation and naive

Bayes for collapsed Gibbs sampling. Technical report Technical report, LingPipe.

Chaney, Allison June-Barlow and David M Blei. 2012. Visualizing Topic Models. In ICWSM.

Chang, Jonathan. 2012. lda: Collapsed Gibbs sampling methods for topic models. R package version

1.3.2.
URL: http://CRAN.R-project.org/package=lda

Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-graber and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In Advances in neural information processing systems.
pp. 288–296.

Chuang, Jason, Christopher D Manning and Jeffrey Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of the International Working Conference on Advanced
Visual Interfaces. ACM pp. 74–77.

Chuang, Jason, Sonal Gupta, Christopher Manning and Jeffrey Heer. 2013. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In Proceedings of the 30th International Conference
on Machine Learning (ICML-13). pp. 612–620.

Cochocki, A and Rolf Unbehauen. 1993. Neural networks for optimization and signal processing. John

Wiley & Sons, Inc.

Council, National Research. 2013. Frontiers in Massive Data Analysis. The National Academies Press.

de Klerk, Etienne and Dmitrii V Pasechnik. 2002. “Approximation of the stability number of a graph via

copositive programming.” SIAM Journal on Optimization 12(4):875–892.

De Marchi, Scott, Christopher Gelpi and Jeffrey D Grynaviski. 2004. “Untangling neural nets.” American

Political Science Review 98(02):371–378.

Dean, Jeffrey and Sanjay Ghemawat. 2008. “MapReduce: simpliﬁed data processing on large clusters.”

Communications of the ACM 51(1):107–113.

Deb, Partha and Pravin K Trivedi. 2002. “The structure of demand for health care: latent class versus

two-part models.” Journal of health economics 21(4):601–625.

Dempster, Arthur P, Nan M Laird and Donald B Rubin. 1977. “Maximum likelihood from incomplete
data via the EM algorithm.” Journal of the Royal Statistical Society. Series B (Methodological) pp. 1–
38.

38

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Even in traditional general linear models, there is also always the choice of model speciﬁcation in both

variables and functional form. Although multimodality brings new issues to the table, the responsibility

of the researcher to carefully validate the chosen model is fundamentally the same. This is true regardless

of whether the choice between competing models arises due to a non-convex latent variable model or

due to the selection of an important model tuning parameter in a globally convex problem. Thus even if

multimodality is an unfamiliar problem, social scientists can draw on the same set of best practices that

they employ throughout their research.

An important practical contribution of this chapter is that it extends the set of tools available to

scholars using topic models in applied research. While we have focused on STM, many of the procedures

we use will be helpful for a broader class of latent variable models. For instance, the approaches to

aligning topics and calculating stability across runs can all be applied directly to the broader class of

statistical topic models and with minor modiﬁcations to most latent variable models.

We see great potential for the analysis of “big” data in the social sciences, but rather than focus on

the data we have taken a more methodological focus. We think this has important implications for both

methodological development but also could structure the types of questions we ask, and the types of data

sets we seek to build. Methodologically, we think that there will be important advances in areas such

as optimal initialization strategies, which is especially important as our data sets grow in size. From an

applied perspective, users will be unlikely to want to wait for extended periods of time in order to get

even a single set of results. Advances in computational power needs to be matched with smart ways to

leverage that power. From a research design perspective, we think more focus should be put on bringing

greater structure to so called “unstructured” data. In the STM we focus on the inclusion of metadata

for modeling and hypothesis testing, but this is only one possible use. Can more direct supervision help

us with issues of multimodality? Of course, in the end, big data will be at its best when there is active

dialogue between those who pose the big question and those who might provide the big answers.

References
Airoldi, Edoardo M, David M Blei, Stephen E Fienberg and Eric P Xing. 2009. Mixed membership

stochastic blockmodels. In Advances in Neural Information Processing Systems. pp. 33–40.

Anandkumar, Anima, Rong Ge, Daniel Hsu and Sham M Kakade. 2013. “A tensor spectral approach to

learning mixed membership community models.” arXiv preprint arXiv:1302.2684 .

36

Anandkumar, Anima, Rong Ge, Daniel Hsu, Sham M Kakade and Matus Telgarsky. 2012. “Tensor

decompositions for learning latent variable models.” arXiv preprint arXiv:1210.7559 .

Anandkumar, Anima, Yi-kai Liu, Daniel J Hsu, Dean P Foster and Sham M Kakade. 2012. A spec-
tral algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems.
pp. 917–925.

Anandkumar, Animashree, Daniel Hsu, Adel Javanmard and Sham Kakade. 2013. Learning linear
bayesian networks with latent variables. In Proceedings of The 30th International Conference on Ma-
chine Learning. pp. 249–257.

Anandkumar, Animashree, Daniel Hsu and Sham M Kakade. 2012. “A method of moments for mixture

models and hidden Markov models.” arXiv preprint arXiv:1203.0683 .

Arora, Sanjeev, Aditya Bhaskara, Rong Ge and Tengyu Ma. 2013. “Provable bounds for learning some

deep representations.” arXiv preprint .

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2012. Learning topic models–going beyond svd. In Foun-

dations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE pp. 1–10.

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2013. “New algorithms for learning incoherent and over-

complete dictionaries.” arXiv preprint arXiv:1308.6273 .

Arora, Sanjeev, Rong Ge, Ankur Moitra and Sushant Sachdeva. 2012. Provable ICA with unknown
Gaussian noise, with implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems. pp. 2375–2383.

Arora, Sanjeev, Rong Ge, Ravindran Kannan and Ankur Moitra. 2012. Computing a nonnegative ma-
trix factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of
computing. ACM pp. 145–162.

Arora, Sanjeev, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu
and Michael Zhu. 2013. A Practical Algorithm for Topic Modeling with Provable Guarantees. In
Proceedings of The 30th International Conference on Machine Learning. pp. 280–288.

Arthur, David and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial
and Applied Mathematics pp. 1027–1035.

Asuncion Jr, Arthur Uy. 2011. Distributed and accelerated inference algorithms for probabilistic graph-

ical models. Technical report California State University at Long Beach.

Bahmani, Bahman, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei Vassilvitskii. 2012.

“Scalable k-means++.” Proceedings of the VLDB Endowment 5(7):622–633.

Beck, Nathaniel, Gary King and Langche Zeng. 2000. “Improving quantitative studies of international

conﬂict: A conjecture.” American Political Science Review pp. 21–35.

Belloni, Alexandre, Victor Chernozhukov and Christian Hansen. 2014. “High-Dimensional Methods and

Inference on Structural and Treatment Effects.” The Journal of Economic Perspectives 28(2):29–50.

37

Bishop, Christopher M et al. 2006. Pattern recognition and machine learning. Vol. 1 springer New York.

Blei, David M. 2012. “Probabilistic topic models.” Communications of the ACM 55(4):77–84.

Blei, David M. 2014. “Build, compute, critique, repeat: data analysis with latent variable models.”

Annual Review of Statistics and Its Application 1:203–232.

Blei, David M, Andrew Y Ng and Michael I Jordan. 2003. “Latent dirichlet allocation.” the Journal of

machine Learning research 3:993–1022.

Boyd, Stephen and Lieven Vandenberghe. 2009. Convex optimization. Cambridge university press.

Carpenter, Bob. 2010. Integrating out multinomial parameters in latent Dirichlet allocation and naive

Bayes for collapsed Gibbs sampling. Technical report Technical report, LingPipe.

Chaney, Allison June-Barlow and David M Blei. 2012. Visualizing Topic Models. In ICWSM.

Chang, Jonathan. 2012. lda: Collapsed Gibbs sampling methods for topic models. R package version

1.3.2.
URL: http://CRAN.R-project.org/package=lda

Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-graber and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In Advances in neural information processing systems.
pp. 288–296.

Chuang, Jason, Christopher D Manning and Jeffrey Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of the International Working Conference on Advanced
Visual Interfaces. ACM pp. 74–77.

Chuang, Jason, Sonal Gupta, Christopher Manning and Jeffrey Heer. 2013. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In Proceedings of the 30th International Conference
on Machine Learning (ICML-13). pp. 612–620.

Cochocki, A and Rolf Unbehauen. 1993. Neural networks for optimization and signal processing. John

Wiley & Sons, Inc.

Council, National Research. 2013. Frontiers in Massive Data Analysis. The National Academies Press.

de Klerk, Etienne and Dmitrii V Pasechnik. 2002. “Approximation of the stability number of a graph via

copositive programming.” SIAM Journal on Optimization 12(4):875–892.

De Marchi, Scott, Christopher Gelpi and Jeffrey D Grynaviski. 2004. “Untangling neural nets.” American

Political Science Review 98(02):371–378.

Dean, Jeffrey and Sanjay Ghemawat. 2008. “MapReduce: simpliﬁed data processing on large clusters.”

Communications of the ACM 51(1):107–113.

Deb, Partha and Pravin K Trivedi. 2002. “The structure of demand for health care: latent class versus

two-part models.” Journal of health economics 21(4):601–625.

Dempster, Arthur P, Nan M Laird and Donald B Rubin. 1977. “Maximum likelihood from incomplete
data via the EM algorithm.” Journal of the Royal Statistical Society. Series B (Methodological) pp. 1–
38.

38

Ding, Weicong, Mohammad H Rohban, Prakash Ishwar and Venkatesh Saligrama. 2013. “Topic discov-

ery through data dependent and random projections.” arXiv preprint arXiv:1303.3664 .

Ding, Weicong, Prakash Ishwar, Mohammad H Rohban and Venkatesh Saligrama. 2013. “Necessary
and Sufﬁcient Conditions for Novel Word Detection in Separable Topic Models.” arXiv preprint
arXiv:1310.7994 .

Donoho, David and Victoria Stodden. 2003. When does non-negative matrix factorization give a correct

decomposition into parts? In Advances in neural information processing systems. p. None.

Doshi-Velez, Finale, Yaorong Ge and Isaac Kohane. 2014. “Comorbidity clusters in autism spectrum

disorders: an electronic health record time-series analysis.” Pediatrics 133(1):e54–e63.

Drton, Mathias and Thomas S Richardson. 2004. “Multimodality of the likelihood in the bivariate

seemingly unrelated regressions model.” Biometrika 91(2):383–392.

DuMouchel, William. 1999. “Bayesian data mining in large frequency tables, with an application to the

FDA spontaneous reporting system.” The American Statistician 53(3):177–190.

Efron, Bradley et al. 1978. “The geometry of exponential families.” The Annals of Statistics 6(2):362–

376.

Eisenstein, Jacob and Eric Xing. 2010. “The CMU 2008 Political Blog Corpus.”.

Fan, Jianqing, Fang Han and Han Liu. 2014. “Challenges of Big Data analysis.” National Science Review

p. nwt032.

Foulds, J. R. and P. Smyth. 2014. Annealing Paths for the Evaluation of Topic Models. In Proceedings

of the Thirtieth Conference Conference on Uncertainty in Artiﬁcial Intelligence.

Gardner, Matthew J, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric Ringger and Kevin Seppi.
2010. The topic browser: An interactive tool for browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.

Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari and Donald B Rubin. 2013.

Bayesian data analysis. CRC press.

Gillis, Nicolas. 2014.

arXiv:1401.5226 .

“The why and how of nonnegative matrix factorization.” arXiv preprint

Gillis, Nicolas and Robert Luce. 2013. “Robust near-separable nonnegative matrix factorization using

linear optimization.” arXiv preprint arXiv:1302.4385 .

Goldstone, AndrewUnderwood et al. 2014. “The Quiet Transformations of Literary Studies: What

Thirteen Thousand Scholars Could Tell Us.”.

Grifﬁths, DMBTL and MIJJB Tenenbaum. 2004. “Hierarchical topic models and the nested Chinese

restaurant process.” Advances in neural information processing systems 16:17.

Grifﬁths, Thomas L and Mark Steyvers. 2004. “Finding scientiﬁc topics.” Proceedings of the National

academy of Sciences of the United States of America 101(Suppl 1):5228–5235.

39

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Even in traditional general linear models, there is also always the choice of model speciﬁcation in both

variables and functional form. Although multimodality brings new issues to the table, the responsibility

of the researcher to carefully validate the chosen model is fundamentally the same. This is true regardless

of whether the choice between competing models arises due to a non-convex latent variable model or

due to the selection of an important model tuning parameter in a globally convex problem. Thus even if

multimodality is an unfamiliar problem, social scientists can draw on the same set of best practices that

they employ throughout their research.

An important practical contribution of this chapter is that it extends the set of tools available to

scholars using topic models in applied research. While we have focused on STM, many of the procedures

we use will be helpful for a broader class of latent variable models. For instance, the approaches to

aligning topics and calculating stability across runs can all be applied directly to the broader class of

statistical topic models and with minor modiﬁcations to most latent variable models.

We see great potential for the analysis of “big” data in the social sciences, but rather than focus on

the data we have taken a more methodological focus. We think this has important implications for both

methodological development but also could structure the types of questions we ask, and the types of data

sets we seek to build. Methodologically, we think that there will be important advances in areas such

as optimal initialization strategies, which is especially important as our data sets grow in size. From an

applied perspective, users will be unlikely to want to wait for extended periods of time in order to get

even a single set of results. Advances in computational power needs to be matched with smart ways to

leverage that power. From a research design perspective, we think more focus should be put on bringing

greater structure to so called “unstructured” data. In the STM we focus on the inclusion of metadata

for modeling and hypothesis testing, but this is only one possible use. Can more direct supervision help

us with issues of multimodality? Of course, in the end, big data will be at its best when there is active

dialogue between those who pose the big question and those who might provide the big answers.

References
Airoldi, Edoardo M, David M Blei, Stephen E Fienberg and Eric P Xing. 2009. Mixed membership

stochastic blockmodels. In Advances in Neural Information Processing Systems. pp. 33–40.

Anandkumar, Anima, Rong Ge, Daniel Hsu and Sham M Kakade. 2013. “A tensor spectral approach to

learning mixed membership community models.” arXiv preprint arXiv:1302.2684 .

36

Anandkumar, Anima, Rong Ge, Daniel Hsu, Sham M Kakade and Matus Telgarsky. 2012. “Tensor

decompositions for learning latent variable models.” arXiv preprint arXiv:1210.7559 .

Anandkumar, Anima, Yi-kai Liu, Daniel J Hsu, Dean P Foster and Sham M Kakade. 2012. A spec-
tral algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems.
pp. 917–925.

Anandkumar, Animashree, Daniel Hsu, Adel Javanmard and Sham Kakade. 2013. Learning linear
bayesian networks with latent variables. In Proceedings of The 30th International Conference on Ma-
chine Learning. pp. 249–257.

Anandkumar, Animashree, Daniel Hsu and Sham M Kakade. 2012. “A method of moments for mixture

models and hidden Markov models.” arXiv preprint arXiv:1203.0683 .

Arora, Sanjeev, Aditya Bhaskara, Rong Ge and Tengyu Ma. 2013. “Provable bounds for learning some

deep representations.” arXiv preprint .

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2012. Learning topic models–going beyond svd. In Foun-

dations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE pp. 1–10.

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2013. “New algorithms for learning incoherent and over-

complete dictionaries.” arXiv preprint arXiv:1308.6273 .

Arora, Sanjeev, Rong Ge, Ankur Moitra and Sushant Sachdeva. 2012. Provable ICA with unknown
Gaussian noise, with implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems. pp. 2375–2383.

Arora, Sanjeev, Rong Ge, Ravindran Kannan and Ankur Moitra. 2012. Computing a nonnegative ma-
trix factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of
computing. ACM pp. 145–162.

Arora, Sanjeev, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu
and Michael Zhu. 2013. A Practical Algorithm for Topic Modeling with Provable Guarantees. In
Proceedings of The 30th International Conference on Machine Learning. pp. 280–288.

Arthur, David and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial
and Applied Mathematics pp. 1027–1035.

Asuncion Jr, Arthur Uy. 2011. Distributed and accelerated inference algorithms for probabilistic graph-

ical models. Technical report California State University at Long Beach.

Bahmani, Bahman, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei Vassilvitskii. 2012.

“Scalable k-means++.” Proceedings of the VLDB Endowment 5(7):622–633.

Beck, Nathaniel, Gary King and Langche Zeng. 2000. “Improving quantitative studies of international

conﬂict: A conjecture.” American Political Science Review pp. 21–35.

Belloni, Alexandre, Victor Chernozhukov and Christian Hansen. 2014. “High-Dimensional Methods and

Inference on Structural and Treatment Effects.” The Journal of Economic Perspectives 28(2):29–50.

37

Bishop, Christopher M et al. 2006. Pattern recognition and machine learning. Vol. 1 springer New York.

Blei, David M. 2012. “Probabilistic topic models.” Communications of the ACM 55(4):77–84.

Blei, David M. 2014. “Build, compute, critique, repeat: data analysis with latent variable models.”

Annual Review of Statistics and Its Application 1:203–232.

Blei, David M, Andrew Y Ng and Michael I Jordan. 2003. “Latent dirichlet allocation.” the Journal of

machine Learning research 3:993–1022.

Boyd, Stephen and Lieven Vandenberghe. 2009. Convex optimization. Cambridge university press.

Carpenter, Bob. 2010. Integrating out multinomial parameters in latent Dirichlet allocation and naive

Bayes for collapsed Gibbs sampling. Technical report Technical report, LingPipe.

Chaney, Allison June-Barlow and David M Blei. 2012. Visualizing Topic Models. In ICWSM.

Chang, Jonathan. 2012. lda: Collapsed Gibbs sampling methods for topic models. R package version

1.3.2.
URL: http://CRAN.R-project.org/package=lda

Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-graber and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In Advances in neural information processing systems.
pp. 288–296.

Chuang, Jason, Christopher D Manning and Jeffrey Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of the International Working Conference on Advanced
Visual Interfaces. ACM pp. 74–77.

Chuang, Jason, Sonal Gupta, Christopher Manning and Jeffrey Heer. 2013. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In Proceedings of the 30th International Conference
on Machine Learning (ICML-13). pp. 612–620.

Cochocki, A and Rolf Unbehauen. 1993. Neural networks for optimization and signal processing. John

Wiley & Sons, Inc.

Council, National Research. 2013. Frontiers in Massive Data Analysis. The National Academies Press.

de Klerk, Etienne and Dmitrii V Pasechnik. 2002. “Approximation of the stability number of a graph via

copositive programming.” SIAM Journal on Optimization 12(4):875–892.

De Marchi, Scott, Christopher Gelpi and Jeffrey D Grynaviski. 2004. “Untangling neural nets.” American

Political Science Review 98(02):371–378.

Dean, Jeffrey and Sanjay Ghemawat. 2008. “MapReduce: simpliﬁed data processing on large clusters.”

Communications of the ACM 51(1):107–113.

Deb, Partha and Pravin K Trivedi. 2002. “The structure of demand for health care: latent class versus

two-part models.” Journal of health economics 21(4):601–625.

Dempster, Arthur P, Nan M Laird and Donald B Rubin. 1977. “Maximum likelihood from incomplete
data via the EM algorithm.” Journal of the Royal Statistical Society. Series B (Methodological) pp. 1–
38.

38

Ding, Weicong, Mohammad H Rohban, Prakash Ishwar and Venkatesh Saligrama. 2013. “Topic discov-

ery through data dependent and random projections.” arXiv preprint arXiv:1303.3664 .

Ding, Weicong, Prakash Ishwar, Mohammad H Rohban and Venkatesh Saligrama. 2013. “Necessary
and Sufﬁcient Conditions for Novel Word Detection in Separable Topic Models.” arXiv preprint
arXiv:1310.7994 .

Donoho, David and Victoria Stodden. 2003. When does non-negative matrix factorization give a correct

decomposition into parts? In Advances in neural information processing systems. p. None.

Doshi-Velez, Finale, Yaorong Ge and Isaac Kohane. 2014. “Comorbidity clusters in autism spectrum

disorders: an electronic health record time-series analysis.” Pediatrics 133(1):e54–e63.

Drton, Mathias and Thomas S Richardson. 2004. “Multimodality of the likelihood in the bivariate

seemingly unrelated regressions model.” Biometrika 91(2):383–392.

DuMouchel, William. 1999. “Bayesian data mining in large frequency tables, with an application to the

FDA spontaneous reporting system.” The American Statistician 53(3):177–190.

Efron, Bradley et al. 1978. “The geometry of exponential families.” The Annals of Statistics 6(2):362–

376.

Eisenstein, Jacob and Eric Xing. 2010. “The CMU 2008 Political Blog Corpus.”.

Fan, Jianqing, Fang Han and Han Liu. 2014. “Challenges of Big Data analysis.” National Science Review

p. nwt032.

Foulds, J. R. and P. Smyth. 2014. Annealing Paths for the Evaluation of Topic Models. In Proceedings

of the Thirtieth Conference Conference on Uncertainty in Artiﬁcial Intelligence.

Gardner, Matthew J, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric Ringger and Kevin Seppi.
2010. The topic browser: An interactive tool for browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.

Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari and Donald B Rubin. 2013.

Bayesian data analysis. CRC press.

Gillis, Nicolas. 2014.

arXiv:1401.5226 .

“The why and how of nonnegative matrix factorization.” arXiv preprint

Gillis, Nicolas and Robert Luce. 2013. “Robust near-separable nonnegative matrix factorization using

linear optimization.” arXiv preprint arXiv:1302.4385 .

Goldstone, AndrewUnderwood et al. 2014. “The Quiet Transformations of Literary Studies: What

Thirteen Thousand Scholars Could Tell Us.”.

Grifﬁths, DMBTL and MIJJB Tenenbaum. 2004. “Hierarchical topic models and the nested Chinese

restaurant process.” Advances in neural information processing systems 16:17.

Grifﬁths, Thomas L and Mark Steyvers. 2004. “Finding scientiﬁc topics.” Proceedings of the National

academy of Sciences of the United States of America 101(Suppl 1):5228–5235.

39

Grimmer, Justin. 2010a. “A Bayesian hierarchical topic model for political texts: Measuring expressed

agendas in Senate press releases.” Political Analysis 18(1):1–35.

Grimmer, Justin. 2010b. “An introduction to Bayesian inference via variational approximations.” Politi-

cal Analysis p. mpq027.

Grimmer, Justin. 2013. Representational Style in Congress: What Legislators Say and why it Matters.

Cambridge University Press.

Grimmer, Justin and Brandon M Stewart. 2013. “Text as data: The promise and pitfalls of automatic

content analysis methods for political texts.” Political Analysis p. mps028.

Grimmer, Justin and Gary King. 2011. “General purpose computer-assisted clustering and conceptual-

ization.” Proceedings of the National Academy of Sciences 108(7):2643–2650.

Hambleton, Ronald K. 1991. Fundamentals of item response theory. Vol. 2 Sage publications.

Hamilton, James D. 1989. “A new approach to the economic analysis of nonstationary time series and

the business cycle.” Econometrica: Journal of the Econometric Society pp. 357–384.

Hoff, Peter D, Adrian E Raftery and Mark S Handcock. 2002. “Latent space approaches to social network

analysis.” Journal of the american Statistical association 97(460):1090–1098.

Hopkins, Daniel J and Gary King. 2010. “A method of automated nonparametric content analysis for

social science.” American Journal of Political Science 54(1):229–247.

Hornik, Kurt. 2005. “A clue for cluster ensembles.” Journal of Statistical Software 14(12).

Hsu, Daniel and Sham M Kakade. 2013. Learning mixtures of spherical gaussians: moment methods and
spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer
Science. ACM pp. 11–20.

Hu, Yuening, Jordan Boyd-Graber, Brianna Satinoff and Alison Smith. 2014. “Interactive topic model-

ing.” Machine Learning 95(3):423–469.

Huang, Furong, UN Niranjan, M Hakeem and Animashree Anandkumar. 2013. “Fast Detection of

Overlapping Communities via Online Tensor Methods.”.

Jordan, Michael I, Zoubin Ghahramani, Tommi S Jaakkola and Lawrence K Saul. 1998. An introduction

to variational methods for graphical models. Springer.

Kalai, Adam Tauman, Ankur Moitra and Gregory Valiant. 2012. “Disentangling gaussians.” Communi-

cations of the ACM 55(2):113–120.

Kass, Robert E and Adrian E Raftery. 1995. “Bayes factors.” Journal of the american statistical associ-

ation 90(430):773–795.

King, Gary. 1989. “Unifying political methodology.” New York: Cambridge .

King, Gary, Jennifer Pan and Margaret E Roberts. 2013. “How censorship in China allows government

criticism but silences collective expression.” American Political Science Review 107(02):326–343.

40

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Even in traditional general linear models, there is also always the choice of model speciﬁcation in both

variables and functional form. Although multimodality brings new issues to the table, the responsibility

of the researcher to carefully validate the chosen model is fundamentally the same. This is true regardless

of whether the choice between competing models arises due to a non-convex latent variable model or

due to the selection of an important model tuning parameter in a globally convex problem. Thus even if

multimodality is an unfamiliar problem, social scientists can draw on the same set of best practices that

they employ throughout their research.

An important practical contribution of this chapter is that it extends the set of tools available to

scholars using topic models in applied research. While we have focused on STM, many of the procedures

we use will be helpful for a broader class of latent variable models. For instance, the approaches to

aligning topics and calculating stability across runs can all be applied directly to the broader class of

statistical topic models and with minor modiﬁcations to most latent variable models.

We see great potential for the analysis of “big” data in the social sciences, but rather than focus on

the data we have taken a more methodological focus. We think this has important implications for both

methodological development but also could structure the types of questions we ask, and the types of data

sets we seek to build. Methodologically, we think that there will be important advances in areas such

as optimal initialization strategies, which is especially important as our data sets grow in size. From an

applied perspective, users will be unlikely to want to wait for extended periods of time in order to get

even a single set of results. Advances in computational power needs to be matched with smart ways to

leverage that power. From a research design perspective, we think more focus should be put on bringing

greater structure to so called “unstructured” data. In the STM we focus on the inclusion of metadata

for modeling and hypothesis testing, but this is only one possible use. Can more direct supervision help

us with issues of multimodality? Of course, in the end, big data will be at its best when there is active

dialogue between those who pose the big question and those who might provide the big answers.

References
Airoldi, Edoardo M, David M Blei, Stephen E Fienberg and Eric P Xing. 2009. Mixed membership

stochastic blockmodels. In Advances in Neural Information Processing Systems. pp. 33–40.

Anandkumar, Anima, Rong Ge, Daniel Hsu and Sham M Kakade. 2013. “A tensor spectral approach to

learning mixed membership community models.” arXiv preprint arXiv:1302.2684 .

36

Anandkumar, Anima, Rong Ge, Daniel Hsu, Sham M Kakade and Matus Telgarsky. 2012. “Tensor

decompositions for learning latent variable models.” arXiv preprint arXiv:1210.7559 .

Anandkumar, Anima, Yi-kai Liu, Daniel J Hsu, Dean P Foster and Sham M Kakade. 2012. A spec-
tral algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems.
pp. 917–925.

Anandkumar, Animashree, Daniel Hsu, Adel Javanmard and Sham Kakade. 2013. Learning linear
bayesian networks with latent variables. In Proceedings of The 30th International Conference on Ma-
chine Learning. pp. 249–257.

Anandkumar, Animashree, Daniel Hsu and Sham M Kakade. 2012. “A method of moments for mixture

models and hidden Markov models.” arXiv preprint arXiv:1203.0683 .

Arora, Sanjeev, Aditya Bhaskara, Rong Ge and Tengyu Ma. 2013. “Provable bounds for learning some

deep representations.” arXiv preprint .

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2012. Learning topic models–going beyond svd. In Foun-

dations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE pp. 1–10.

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2013. “New algorithms for learning incoherent and over-

complete dictionaries.” arXiv preprint arXiv:1308.6273 .

Arora, Sanjeev, Rong Ge, Ankur Moitra and Sushant Sachdeva. 2012. Provable ICA with unknown
Gaussian noise, with implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems. pp. 2375–2383.

Arora, Sanjeev, Rong Ge, Ravindran Kannan and Ankur Moitra. 2012. Computing a nonnegative ma-
trix factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of
computing. ACM pp. 145–162.

Arora, Sanjeev, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu
and Michael Zhu. 2013. A Practical Algorithm for Topic Modeling with Provable Guarantees. In
Proceedings of The 30th International Conference on Machine Learning. pp. 280–288.

Arthur, David and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial
and Applied Mathematics pp. 1027–1035.

Asuncion Jr, Arthur Uy. 2011. Distributed and accelerated inference algorithms for probabilistic graph-

ical models. Technical report California State University at Long Beach.

Bahmani, Bahman, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei Vassilvitskii. 2012.

“Scalable k-means++.” Proceedings of the VLDB Endowment 5(7):622–633.

Beck, Nathaniel, Gary King and Langche Zeng. 2000. “Improving quantitative studies of international

conﬂict: A conjecture.” American Political Science Review pp. 21–35.

Belloni, Alexandre, Victor Chernozhukov and Christian Hansen. 2014. “High-Dimensional Methods and

Inference on Structural and Treatment Effects.” The Journal of Economic Perspectives 28(2):29–50.

37

Bishop, Christopher M et al. 2006. Pattern recognition and machine learning. Vol. 1 springer New York.

Blei, David M. 2012. “Probabilistic topic models.” Communications of the ACM 55(4):77–84.

Blei, David M. 2014. “Build, compute, critique, repeat: data analysis with latent variable models.”

Annual Review of Statistics and Its Application 1:203–232.

Blei, David M, Andrew Y Ng and Michael I Jordan. 2003. “Latent dirichlet allocation.” the Journal of

machine Learning research 3:993–1022.

Boyd, Stephen and Lieven Vandenberghe. 2009. Convex optimization. Cambridge university press.

Carpenter, Bob. 2010. Integrating out multinomial parameters in latent Dirichlet allocation and naive

Bayes for collapsed Gibbs sampling. Technical report Technical report, LingPipe.

Chaney, Allison June-Barlow and David M Blei. 2012. Visualizing Topic Models. In ICWSM.

Chang, Jonathan. 2012. lda: Collapsed Gibbs sampling methods for topic models. R package version

1.3.2.
URL: http://CRAN.R-project.org/package=lda

Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-graber and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In Advances in neural information processing systems.
pp. 288–296.

Chuang, Jason, Christopher D Manning and Jeffrey Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of the International Working Conference on Advanced
Visual Interfaces. ACM pp. 74–77.

Chuang, Jason, Sonal Gupta, Christopher Manning and Jeffrey Heer. 2013. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In Proceedings of the 30th International Conference
on Machine Learning (ICML-13). pp. 612–620.

Cochocki, A and Rolf Unbehauen. 1993. Neural networks for optimization and signal processing. John

Wiley & Sons, Inc.

Council, National Research. 2013. Frontiers in Massive Data Analysis. The National Academies Press.

de Klerk, Etienne and Dmitrii V Pasechnik. 2002. “Approximation of the stability number of a graph via

copositive programming.” SIAM Journal on Optimization 12(4):875–892.

De Marchi, Scott, Christopher Gelpi and Jeffrey D Grynaviski. 2004. “Untangling neural nets.” American

Political Science Review 98(02):371–378.

Dean, Jeffrey and Sanjay Ghemawat. 2008. “MapReduce: simpliﬁed data processing on large clusters.”

Communications of the ACM 51(1):107–113.

Deb, Partha and Pravin K Trivedi. 2002. “The structure of demand for health care: latent class versus

two-part models.” Journal of health economics 21(4):601–625.

Dempster, Arthur P, Nan M Laird and Donald B Rubin. 1977. “Maximum likelihood from incomplete
data via the EM algorithm.” Journal of the Royal Statistical Society. Series B (Methodological) pp. 1–
38.

38

Ding, Weicong, Mohammad H Rohban, Prakash Ishwar and Venkatesh Saligrama. 2013. “Topic discov-

ery through data dependent and random projections.” arXiv preprint arXiv:1303.3664 .

Ding, Weicong, Prakash Ishwar, Mohammad H Rohban and Venkatesh Saligrama. 2013. “Necessary
and Sufﬁcient Conditions for Novel Word Detection in Separable Topic Models.” arXiv preprint
arXiv:1310.7994 .

Donoho, David and Victoria Stodden. 2003. When does non-negative matrix factorization give a correct

decomposition into parts? In Advances in neural information processing systems. p. None.

Doshi-Velez, Finale, Yaorong Ge and Isaac Kohane. 2014. “Comorbidity clusters in autism spectrum

disorders: an electronic health record time-series analysis.” Pediatrics 133(1):e54–e63.

Drton, Mathias and Thomas S Richardson. 2004. “Multimodality of the likelihood in the bivariate

seemingly unrelated regressions model.” Biometrika 91(2):383–392.

DuMouchel, William. 1999. “Bayesian data mining in large frequency tables, with an application to the

FDA spontaneous reporting system.” The American Statistician 53(3):177–190.

Efron, Bradley et al. 1978. “The geometry of exponential families.” The Annals of Statistics 6(2):362–

376.

Eisenstein, Jacob and Eric Xing. 2010. “The CMU 2008 Political Blog Corpus.”.

Fan, Jianqing, Fang Han and Han Liu. 2014. “Challenges of Big Data analysis.” National Science Review

p. nwt032.

Foulds, J. R. and P. Smyth. 2014. Annealing Paths for the Evaluation of Topic Models. In Proceedings

of the Thirtieth Conference Conference on Uncertainty in Artiﬁcial Intelligence.

Gardner, Matthew J, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric Ringger and Kevin Seppi.
2010. The topic browser: An interactive tool for browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.

Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari and Donald B Rubin. 2013.

Bayesian data analysis. CRC press.

Gillis, Nicolas. 2014.

arXiv:1401.5226 .

“The why and how of nonnegative matrix factorization.” arXiv preprint

Gillis, Nicolas and Robert Luce. 2013. “Robust near-separable nonnegative matrix factorization using

linear optimization.” arXiv preprint arXiv:1302.4385 .

Goldstone, AndrewUnderwood et al. 2014. “The Quiet Transformations of Literary Studies: What

Thirteen Thousand Scholars Could Tell Us.”.

Grifﬁths, DMBTL and MIJJB Tenenbaum. 2004. “Hierarchical topic models and the nested Chinese

restaurant process.” Advances in neural information processing systems 16:17.

Grifﬁths, Thomas L and Mark Steyvers. 2004. “Finding scientiﬁc topics.” Proceedings of the National

academy of Sciences of the United States of America 101(Suppl 1):5228–5235.

39

Grimmer, Justin. 2010a. “A Bayesian hierarchical topic model for political texts: Measuring expressed

agendas in Senate press releases.” Political Analysis 18(1):1–35.

Grimmer, Justin. 2010b. “An introduction to Bayesian inference via variational approximations.” Politi-

cal Analysis p. mpq027.

Grimmer, Justin. 2013. Representational Style in Congress: What Legislators Say and why it Matters.

Cambridge University Press.

Grimmer, Justin and Brandon M Stewart. 2013. “Text as data: The promise and pitfalls of automatic

content analysis methods for political texts.” Political Analysis p. mps028.

Grimmer, Justin and Gary King. 2011. “General purpose computer-assisted clustering and conceptual-

ization.” Proceedings of the National Academy of Sciences 108(7):2643–2650.

Hambleton, Ronald K. 1991. Fundamentals of item response theory. Vol. 2 Sage publications.

Hamilton, James D. 1989. “A new approach to the economic analysis of nonstationary time series and

the business cycle.” Econometrica: Journal of the Econometric Society pp. 357–384.

Hoff, Peter D, Adrian E Raftery and Mark S Handcock. 2002. “Latent space approaches to social network

analysis.” Journal of the american Statistical association 97(460):1090–1098.

Hopkins, Daniel J and Gary King. 2010. “A method of automated nonparametric content analysis for

social science.” American Journal of Political Science 54(1):229–247.

Hornik, Kurt. 2005. “A clue for cluster ensembles.” Journal of Statistical Software 14(12).

Hsu, Daniel and Sham M Kakade. 2013. Learning mixtures of spherical gaussians: moment methods and
spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer
Science. ACM pp. 11–20.

Hu, Yuening, Jordan Boyd-Graber, Brianna Satinoff and Alison Smith. 2014. “Interactive topic model-

ing.” Machine Learning 95(3):423–469.

Huang, Furong, UN Niranjan, M Hakeem and Animashree Anandkumar. 2013. “Fast Detection of

Overlapping Communities via Online Tensor Methods.”.

Jordan, Michael I, Zoubin Ghahramani, Tommi S Jaakkola and Lawrence K Saul. 1998. An introduction

to variational methods for graphical models. Springer.

Kalai, Adam Tauman, Ankur Moitra and Gregory Valiant. 2012. “Disentangling gaussians.” Communi-

cations of the ACM 55(2):113–120.

Kass, Robert E and Adrian E Raftery. 1995. “Bayes factors.” Journal of the american statistical associ-

ation 90(430):773–795.

King, Gary. 1989. “Unifying political methodology.” New York: Cambridge .

King, Gary, Jennifer Pan and Margaret E Roberts. 2013. “How censorship in China allows government

criticism but silences collective expression.” American Political Science Review 107(02):326–343.

40

Kivinen, Jyrki and Manfred K Warmuth. 1997. “Exponentiated gradient versus gradient descent for

linear predictors.” Information and Computation 132(1):1–63.

Koltcov, Sergei, Olessia Koltsova and Sergey Nikolenko. 2014. Latent dirichlet allocation: stability and
applications to studies of user-generated content. In Proceedings of the 2014 ACM conference on Web
science. ACM pp. 161–165.

Krippendorff, Klaus. 2012. Content analysis: An introduction to its methodology. Sage.

Kuhn, Harold W. 1955. “The Hungarian method for the assignment problem.” Naval research logistics

quarterly 2(1-2):83–97.

Kumar, Abhishek, Vikas Sindhwani and Prabhanjan Kambadur. 2012. “Fast conical hull algorithms for

near-separable non-negative matrix factorization.” arXiv preprint arXiv:1210.1190 .

Lancichinetti, Andrea, M Irmak Sirer, Jane X Wang, Daniel Acuna, Konrad K¨ording and Lu´ıs A Nunes
Amaral. 2014. “A high-reproducibility and high-accuracy method for automated topic classiﬁcation.”
arXiv preprint arXiv:1402.0422 .

Lau, Jey Han, David Newman and Timothy Baldwin. 2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality. In Proceedings of the European Chapter of the
Association for Computational Linguistics.

Lauderdale, Benjamin E and Tom S Clark. 2014. “Scaling politically meaningful dimensions using texts

and votes.” American Journal of Political Science .

Lazer, David, Alex Sandy Pentland, Lada Adamic, Sinan Aral, Albert Laszlo Barabasi, Devon Brewer,
Nicholas Christakis, Noshir Contractor, James Fowler, Myron Gutmann et al. 2009. “Life in the
network: the coming age of computational social science.” Science (New York, NY) 323(5915):721.

Lim, Yew Jin and Yee Whye Teh. 2007. Variational Bayesian approach to movie rating prediction. In

Proceedings of KDD Cup and Workshop. Vol. 7 Citeseer pp. 15–21.

Lloyd, Stuart. 1982. “Least squares quantization in PCM.” Information Theory, IEEE Transactions on

28(2):129–137.

Mahajan, Meena, Prajakta Nimbhorkar and Kasturi Varadarajan. 2009. The planar k-means problem is

NP-hard. In WALCOM: Algorithms and Computation. Springer pp. 274–285.

McLachlan, Geoffrey and David Peel. 2004. Finite mixture models. John Wiley & Sons.

Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality. In Advances in Neural Information Processing
Systems. pp. 3111–3119.

Mimno, David and David Blei. 2011. Bayesian checking for topic models. In Proceedings of the Con-
ference on Empirical Methods in Natural Language Processing. Association for Computational Lin-
guistics pp. 227–237.

Mimno, David, Hanna M Wallach, Edmund Talley, Miriam Leenders and Andrew McCallum. 2011. Op-
timizing semantic coherence in topic models. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing. Association for Computational Linguistics pp. 262–272.

41

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Even in traditional general linear models, there is also always the choice of model speciﬁcation in both

variables and functional form. Although multimodality brings new issues to the table, the responsibility

of the researcher to carefully validate the chosen model is fundamentally the same. This is true regardless

of whether the choice between competing models arises due to a non-convex latent variable model or

due to the selection of an important model tuning parameter in a globally convex problem. Thus even if

multimodality is an unfamiliar problem, social scientists can draw on the same set of best practices that

they employ throughout their research.

An important practical contribution of this chapter is that it extends the set of tools available to

scholars using topic models in applied research. While we have focused on STM, many of the procedures

we use will be helpful for a broader class of latent variable models. For instance, the approaches to

aligning topics and calculating stability across runs can all be applied directly to the broader class of

statistical topic models and with minor modiﬁcations to most latent variable models.

We see great potential for the analysis of “big” data in the social sciences, but rather than focus on

the data we have taken a more methodological focus. We think this has important implications for both

methodological development but also could structure the types of questions we ask, and the types of data

sets we seek to build. Methodologically, we think that there will be important advances in areas such

as optimal initialization strategies, which is especially important as our data sets grow in size. From an

applied perspective, users will be unlikely to want to wait for extended periods of time in order to get

even a single set of results. Advances in computational power needs to be matched with smart ways to

leverage that power. From a research design perspective, we think more focus should be put on bringing

greater structure to so called “unstructured” data. In the STM we focus on the inclusion of metadata

for modeling and hypothesis testing, but this is only one possible use. Can more direct supervision help

us with issues of multimodality? Of course, in the end, big data will be at its best when there is active

dialogue between those who pose the big question and those who might provide the big answers.

References
Airoldi, Edoardo M, David M Blei, Stephen E Fienberg and Eric P Xing. 2009. Mixed membership

stochastic blockmodels. In Advances in Neural Information Processing Systems. pp. 33–40.

Anandkumar, Anima, Rong Ge, Daniel Hsu and Sham M Kakade. 2013. “A tensor spectral approach to

learning mixed membership community models.” arXiv preprint arXiv:1302.2684 .

36

Anandkumar, Anima, Rong Ge, Daniel Hsu, Sham M Kakade and Matus Telgarsky. 2012. “Tensor

decompositions for learning latent variable models.” arXiv preprint arXiv:1210.7559 .

Anandkumar, Anima, Yi-kai Liu, Daniel J Hsu, Dean P Foster and Sham M Kakade. 2012. A spec-
tral algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems.
pp. 917–925.

Anandkumar, Animashree, Daniel Hsu, Adel Javanmard and Sham Kakade. 2013. Learning linear
bayesian networks with latent variables. In Proceedings of The 30th International Conference on Ma-
chine Learning. pp. 249–257.

Anandkumar, Animashree, Daniel Hsu and Sham M Kakade. 2012. “A method of moments for mixture

models and hidden Markov models.” arXiv preprint arXiv:1203.0683 .

Arora, Sanjeev, Aditya Bhaskara, Rong Ge and Tengyu Ma. 2013. “Provable bounds for learning some

deep representations.” arXiv preprint .

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2012. Learning topic models–going beyond svd. In Foun-

dations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE pp. 1–10.

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2013. “New algorithms for learning incoherent and over-

complete dictionaries.” arXiv preprint arXiv:1308.6273 .

Arora, Sanjeev, Rong Ge, Ankur Moitra and Sushant Sachdeva. 2012. Provable ICA with unknown
Gaussian noise, with implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems. pp. 2375–2383.

Arora, Sanjeev, Rong Ge, Ravindran Kannan and Ankur Moitra. 2012. Computing a nonnegative ma-
trix factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of
computing. ACM pp. 145–162.

Arora, Sanjeev, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu
and Michael Zhu. 2013. A Practical Algorithm for Topic Modeling with Provable Guarantees. In
Proceedings of The 30th International Conference on Machine Learning. pp. 280–288.

Arthur, David and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial
and Applied Mathematics pp. 1027–1035.

Asuncion Jr, Arthur Uy. 2011. Distributed and accelerated inference algorithms for probabilistic graph-

ical models. Technical report California State University at Long Beach.

Bahmani, Bahman, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei Vassilvitskii. 2012.

“Scalable k-means++.” Proceedings of the VLDB Endowment 5(7):622–633.

Beck, Nathaniel, Gary King and Langche Zeng. 2000. “Improving quantitative studies of international

conﬂict: A conjecture.” American Political Science Review pp. 21–35.

Belloni, Alexandre, Victor Chernozhukov and Christian Hansen. 2014. “High-Dimensional Methods and

Inference on Structural and Treatment Effects.” The Journal of Economic Perspectives 28(2):29–50.

37

Bishop, Christopher M et al. 2006. Pattern recognition and machine learning. Vol. 1 springer New York.

Blei, David M. 2012. “Probabilistic topic models.” Communications of the ACM 55(4):77–84.

Blei, David M. 2014. “Build, compute, critique, repeat: data analysis with latent variable models.”

Annual Review of Statistics and Its Application 1:203–232.

Blei, David M, Andrew Y Ng and Michael I Jordan. 2003. “Latent dirichlet allocation.” the Journal of

machine Learning research 3:993–1022.

Boyd, Stephen and Lieven Vandenberghe. 2009. Convex optimization. Cambridge university press.

Carpenter, Bob. 2010. Integrating out multinomial parameters in latent Dirichlet allocation and naive

Bayes for collapsed Gibbs sampling. Technical report Technical report, LingPipe.

Chaney, Allison June-Barlow and David M Blei. 2012. Visualizing Topic Models. In ICWSM.

Chang, Jonathan. 2012. lda: Collapsed Gibbs sampling methods for topic models. R package version

1.3.2.
URL: http://CRAN.R-project.org/package=lda

Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-graber and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In Advances in neural information processing systems.
pp. 288–296.

Chuang, Jason, Christopher D Manning and Jeffrey Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of the International Working Conference on Advanced
Visual Interfaces. ACM pp. 74–77.

Chuang, Jason, Sonal Gupta, Christopher Manning and Jeffrey Heer. 2013. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In Proceedings of the 30th International Conference
on Machine Learning (ICML-13). pp. 612–620.

Cochocki, A and Rolf Unbehauen. 1993. Neural networks for optimization and signal processing. John

Wiley & Sons, Inc.

Council, National Research. 2013. Frontiers in Massive Data Analysis. The National Academies Press.

de Klerk, Etienne and Dmitrii V Pasechnik. 2002. “Approximation of the stability number of a graph via

copositive programming.” SIAM Journal on Optimization 12(4):875–892.

De Marchi, Scott, Christopher Gelpi and Jeffrey D Grynaviski. 2004. “Untangling neural nets.” American

Political Science Review 98(02):371–378.

Dean, Jeffrey and Sanjay Ghemawat. 2008. “MapReduce: simpliﬁed data processing on large clusters.”

Communications of the ACM 51(1):107–113.

Deb, Partha and Pravin K Trivedi. 2002. “The structure of demand for health care: latent class versus

two-part models.” Journal of health economics 21(4):601–625.

Dempster, Arthur P, Nan M Laird and Donald B Rubin. 1977. “Maximum likelihood from incomplete
data via the EM algorithm.” Journal of the Royal Statistical Society. Series B (Methodological) pp. 1–
38.

38

Ding, Weicong, Mohammad H Rohban, Prakash Ishwar and Venkatesh Saligrama. 2013. “Topic discov-

ery through data dependent and random projections.” arXiv preprint arXiv:1303.3664 .

Ding, Weicong, Prakash Ishwar, Mohammad H Rohban and Venkatesh Saligrama. 2013. “Necessary
and Sufﬁcient Conditions for Novel Word Detection in Separable Topic Models.” arXiv preprint
arXiv:1310.7994 .

Donoho, David and Victoria Stodden. 2003. When does non-negative matrix factorization give a correct

decomposition into parts? In Advances in neural information processing systems. p. None.

Doshi-Velez, Finale, Yaorong Ge and Isaac Kohane. 2014. “Comorbidity clusters in autism spectrum

disorders: an electronic health record time-series analysis.” Pediatrics 133(1):e54–e63.

Drton, Mathias and Thomas S Richardson. 2004. “Multimodality of the likelihood in the bivariate

seemingly unrelated regressions model.” Biometrika 91(2):383–392.

DuMouchel, William. 1999. “Bayesian data mining in large frequency tables, with an application to the

FDA spontaneous reporting system.” The American Statistician 53(3):177–190.

Efron, Bradley et al. 1978. “The geometry of exponential families.” The Annals of Statistics 6(2):362–

376.

Eisenstein, Jacob and Eric Xing. 2010. “The CMU 2008 Political Blog Corpus.”.

Fan, Jianqing, Fang Han and Han Liu. 2014. “Challenges of Big Data analysis.” National Science Review

p. nwt032.

Foulds, J. R. and P. Smyth. 2014. Annealing Paths for the Evaluation of Topic Models. In Proceedings

of the Thirtieth Conference Conference on Uncertainty in Artiﬁcial Intelligence.

Gardner, Matthew J, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric Ringger and Kevin Seppi.
2010. The topic browser: An interactive tool for browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.

Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari and Donald B Rubin. 2013.

Bayesian data analysis. CRC press.

Gillis, Nicolas. 2014.

arXiv:1401.5226 .

“The why and how of nonnegative matrix factorization.” arXiv preprint

Gillis, Nicolas and Robert Luce. 2013. “Robust near-separable nonnegative matrix factorization using

linear optimization.” arXiv preprint arXiv:1302.4385 .

Goldstone, AndrewUnderwood et al. 2014. “The Quiet Transformations of Literary Studies: What

Thirteen Thousand Scholars Could Tell Us.”.

Grifﬁths, DMBTL and MIJJB Tenenbaum. 2004. “Hierarchical topic models and the nested Chinese

restaurant process.” Advances in neural information processing systems 16:17.

Grifﬁths, Thomas L and Mark Steyvers. 2004. “Finding scientiﬁc topics.” Proceedings of the National

academy of Sciences of the United States of America 101(Suppl 1):5228–5235.

39

Grimmer, Justin. 2010a. “A Bayesian hierarchical topic model for political texts: Measuring expressed

agendas in Senate press releases.” Political Analysis 18(1):1–35.

Grimmer, Justin. 2010b. “An introduction to Bayesian inference via variational approximations.” Politi-

cal Analysis p. mpq027.

Grimmer, Justin. 2013. Representational Style in Congress: What Legislators Say and why it Matters.

Cambridge University Press.

Grimmer, Justin and Brandon M Stewart. 2013. “Text as data: The promise and pitfalls of automatic

content analysis methods for political texts.” Political Analysis p. mps028.

Grimmer, Justin and Gary King. 2011. “General purpose computer-assisted clustering and conceptual-

ization.” Proceedings of the National Academy of Sciences 108(7):2643–2650.

Hambleton, Ronald K. 1991. Fundamentals of item response theory. Vol. 2 Sage publications.

Hamilton, James D. 1989. “A new approach to the economic analysis of nonstationary time series and

the business cycle.” Econometrica: Journal of the Econometric Society pp. 357–384.

Hoff, Peter D, Adrian E Raftery and Mark S Handcock. 2002. “Latent space approaches to social network

analysis.” Journal of the american Statistical association 97(460):1090–1098.

Hopkins, Daniel J and Gary King. 2010. “A method of automated nonparametric content analysis for

social science.” American Journal of Political Science 54(1):229–247.

Hornik, Kurt. 2005. “A clue for cluster ensembles.” Journal of Statistical Software 14(12).

Hsu, Daniel and Sham M Kakade. 2013. Learning mixtures of spherical gaussians: moment methods and
spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer
Science. ACM pp. 11–20.

Hu, Yuening, Jordan Boyd-Graber, Brianna Satinoff and Alison Smith. 2014. “Interactive topic model-

ing.” Machine Learning 95(3):423–469.

Huang, Furong, UN Niranjan, M Hakeem and Animashree Anandkumar. 2013. “Fast Detection of

Overlapping Communities via Online Tensor Methods.”.

Jordan, Michael I, Zoubin Ghahramani, Tommi S Jaakkola and Lawrence K Saul. 1998. An introduction

to variational methods for graphical models. Springer.

Kalai, Adam Tauman, Ankur Moitra and Gregory Valiant. 2012. “Disentangling gaussians.” Communi-

cations of the ACM 55(2):113–120.

Kass, Robert E and Adrian E Raftery. 1995. “Bayes factors.” Journal of the american statistical associ-

ation 90(430):773–795.

King, Gary. 1989. “Unifying political methodology.” New York: Cambridge .

King, Gary, Jennifer Pan and Margaret E Roberts. 2013. “How censorship in China allows government

criticism but silences collective expression.” American Political Science Review 107(02):326–343.

40

Kivinen, Jyrki and Manfred K Warmuth. 1997. “Exponentiated gradient versus gradient descent for

linear predictors.” Information and Computation 132(1):1–63.

Koltcov, Sergei, Olessia Koltsova and Sergey Nikolenko. 2014. Latent dirichlet allocation: stability and
applications to studies of user-generated content. In Proceedings of the 2014 ACM conference on Web
science. ACM pp. 161–165.

Krippendorff, Klaus. 2012. Content analysis: An introduction to its methodology. Sage.

Kuhn, Harold W. 1955. “The Hungarian method for the assignment problem.” Naval research logistics

quarterly 2(1-2):83–97.

Kumar, Abhishek, Vikas Sindhwani and Prabhanjan Kambadur. 2012. “Fast conical hull algorithms for

near-separable non-negative matrix factorization.” arXiv preprint arXiv:1210.1190 .

Lancichinetti, Andrea, M Irmak Sirer, Jane X Wang, Daniel Acuna, Konrad K¨ording and Lu´ıs A Nunes
Amaral. 2014. “A high-reproducibility and high-accuracy method for automated topic classiﬁcation.”
arXiv preprint arXiv:1402.0422 .

Lau, Jey Han, David Newman and Timothy Baldwin. 2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality. In Proceedings of the European Chapter of the
Association for Computational Linguistics.

Lauderdale, Benjamin E and Tom S Clark. 2014. “Scaling politically meaningful dimensions using texts

and votes.” American Journal of Political Science .

Lazer, David, Alex Sandy Pentland, Lada Adamic, Sinan Aral, Albert Laszlo Barabasi, Devon Brewer,
Nicholas Christakis, Noshir Contractor, James Fowler, Myron Gutmann et al. 2009. “Life in the
network: the coming age of computational social science.” Science (New York, NY) 323(5915):721.

Lim, Yew Jin and Yee Whye Teh. 2007. Variational Bayesian approach to movie rating prediction. In

Proceedings of KDD Cup and Workshop. Vol. 7 Citeseer pp. 15–21.

Lloyd, Stuart. 1982. “Least squares quantization in PCM.” Information Theory, IEEE Transactions on

28(2):129–137.

Mahajan, Meena, Prajakta Nimbhorkar and Kasturi Varadarajan. 2009. The planar k-means problem is

NP-hard. In WALCOM: Algorithms and Computation. Springer pp. 274–285.

McLachlan, Geoffrey and David Peel. 2004. Finite mixture models. John Wiley & Sons.

Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality. In Advances in Neural Information Processing
Systems. pp. 3111–3119.

Mimno, David and David Blei. 2011. Bayesian checking for topic models. In Proceedings of the Con-
ference on Empirical Methods in Natural Language Processing. Association for Computational Lin-
guistics pp. 227–237.

Mimno, David, Hanna M Wallach, Edmund Talley, Miriam Leenders and Andrew McCallum. 2011. Op-
timizing semantic coherence in topic models. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing. Association for Computational Linguistics pp. 262–272.

41

Mullainathan, Sendhil. 2014. “What Big Data Means for Social Science.” Behavioral and Experimental

Seminar.

Murphy, Kevin P. 2012. Machine learning: a probabilistic perspective. MIT press.

Nguyen, Thang, Yuening Hu and Jordan Boyd-Graber. 2014. Anchors Regularized: Adding Robustness
and Extensibility to Scalable Topic-Modeling Algorithms. In Association for Computational Linguis-
tics.

Nielsen, Frank and Richard Nock. 2014. “Further heuristics for k-means: The merge-and-split heuristic

and the (k, l)-means.” arXiv preprint arXiv:1406.6314 .

O’Connor, Brendan. 2014. MiTextExplorer: Linked brushing and mutual information for exploratory
text data analysis. In Proceedings of the Workshop on Interactive Language Learning, Visualization,
and Interfaces. Association for Computational Linguistics pp. 1–13.

OConnor, Brendan, Brandon M Stewart and Noah A Smith. 2013. Learning to Extract International

Relations from Political Context. In Proceedings of ACL.

Papadimitriou, Christos H and Kenneth Steiglitz. 1998. Combinatorial optimization: algorithms and

complexity. Courier Dover Publications.

Park, Jong Hee. 2012. “A Uniﬁed Method for Dynamic and Cross-Sectional Heterogeneity: Introducing

Hidden Markov Panel Models.” American Journal of Political Science 56(4):1040–1054.

Pearson, Karl. 1894. “Contributions to the mathematical theory of evolution.” Philosophical Transac-

tions of the Royal Society of London. A pp. 71–110.

Poole, Keith T and Howard Rosenthal. 1997. Congress: A political-economic history of roll call voting.

Oxford University Press.

Quinn, Kevin M, Burt L Monroe, Michael Colaresi, Michael H Crespin and Dragomir R Radev. 2010.
“How to analyze political attention with minimal assumptions and costs.” American Journal of Politi-
cal Science 54(1):209–228.

Rabiner, Lawrence and Biing-Hwang Juang. 1986. “An introduction to hidden Markov models.” ASSP

Magazine, IEEE 3(1):4–16.

Recht, Ben, Christopher Re, Joel Tropp and Victor Bittorf. 2012. Factoring nonnegative matrices with

linear programs. In Advances in Neural Information Processing Systems. pp. 1214–1222.

Reich, Justin, Dustin Tingley, Jetson Leder-Luis, Margaret E Roberts and Brandon M Stewart. N.d.
Computer Assisted Reading and Discovery for Student Generated Text. Technical report Working
Paper. Export.

Rivers, Douglas. 2003. “Identiﬁcation of multidimensional spatial voting models.” Typescript. Stanford

University .

Robert, Christian P and George Casella. 2004. Monte Carlo statistical methods. Vol. 319 Springer New

York.

42

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Even in traditional general linear models, there is also always the choice of model speciﬁcation in both

variables and functional form. Although multimodality brings new issues to the table, the responsibility

of the researcher to carefully validate the chosen model is fundamentally the same. This is true regardless

of whether the choice between competing models arises due to a non-convex latent variable model or

due to the selection of an important model tuning parameter in a globally convex problem. Thus even if

multimodality is an unfamiliar problem, social scientists can draw on the same set of best practices that

they employ throughout their research.

An important practical contribution of this chapter is that it extends the set of tools available to

scholars using topic models in applied research. While we have focused on STM, many of the procedures

we use will be helpful for a broader class of latent variable models. For instance, the approaches to

aligning topics and calculating stability across runs can all be applied directly to the broader class of

statistical topic models and with minor modiﬁcations to most latent variable models.

We see great potential for the analysis of “big” data in the social sciences, but rather than focus on

the data we have taken a more methodological focus. We think this has important implications for both

methodological development but also could structure the types of questions we ask, and the types of data

sets we seek to build. Methodologically, we think that there will be important advances in areas such

as optimal initialization strategies, which is especially important as our data sets grow in size. From an

applied perspective, users will be unlikely to want to wait for extended periods of time in order to get

even a single set of results. Advances in computational power needs to be matched with smart ways to

leverage that power. From a research design perspective, we think more focus should be put on bringing

greater structure to so called “unstructured” data. In the STM we focus on the inclusion of metadata

for modeling and hypothesis testing, but this is only one possible use. Can more direct supervision help

us with issues of multimodality? Of course, in the end, big data will be at its best when there is active

dialogue between those who pose the big question and those who might provide the big answers.

References
Airoldi, Edoardo M, David M Blei, Stephen E Fienberg and Eric P Xing. 2009. Mixed membership

stochastic blockmodels. In Advances in Neural Information Processing Systems. pp. 33–40.

Anandkumar, Anima, Rong Ge, Daniel Hsu and Sham M Kakade. 2013. “A tensor spectral approach to

learning mixed membership community models.” arXiv preprint arXiv:1302.2684 .

36

Anandkumar, Anima, Rong Ge, Daniel Hsu, Sham M Kakade and Matus Telgarsky. 2012. “Tensor

decompositions for learning latent variable models.” arXiv preprint arXiv:1210.7559 .

Anandkumar, Anima, Yi-kai Liu, Daniel J Hsu, Dean P Foster and Sham M Kakade. 2012. A spec-
tral algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems.
pp. 917–925.

Anandkumar, Animashree, Daniel Hsu, Adel Javanmard and Sham Kakade. 2013. Learning linear
bayesian networks with latent variables. In Proceedings of The 30th International Conference on Ma-
chine Learning. pp. 249–257.

Anandkumar, Animashree, Daniel Hsu and Sham M Kakade. 2012. “A method of moments for mixture

models and hidden Markov models.” arXiv preprint arXiv:1203.0683 .

Arora, Sanjeev, Aditya Bhaskara, Rong Ge and Tengyu Ma. 2013. “Provable bounds for learning some

deep representations.” arXiv preprint .

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2012. Learning topic models–going beyond svd. In Foun-

dations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE pp. 1–10.

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2013. “New algorithms for learning incoherent and over-

complete dictionaries.” arXiv preprint arXiv:1308.6273 .

Arora, Sanjeev, Rong Ge, Ankur Moitra and Sushant Sachdeva. 2012. Provable ICA with unknown
Gaussian noise, with implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems. pp. 2375–2383.

Arora, Sanjeev, Rong Ge, Ravindran Kannan and Ankur Moitra. 2012. Computing a nonnegative ma-
trix factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of
computing. ACM pp. 145–162.

Arora, Sanjeev, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu
and Michael Zhu. 2013. A Practical Algorithm for Topic Modeling with Provable Guarantees. In
Proceedings of The 30th International Conference on Machine Learning. pp. 280–288.

Arthur, David and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial
and Applied Mathematics pp. 1027–1035.

Asuncion Jr, Arthur Uy. 2011. Distributed and accelerated inference algorithms for probabilistic graph-

ical models. Technical report California State University at Long Beach.

Bahmani, Bahman, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei Vassilvitskii. 2012.

“Scalable k-means++.” Proceedings of the VLDB Endowment 5(7):622–633.

Beck, Nathaniel, Gary King and Langche Zeng. 2000. “Improving quantitative studies of international

conﬂict: A conjecture.” American Political Science Review pp. 21–35.

Belloni, Alexandre, Victor Chernozhukov and Christian Hansen. 2014. “High-Dimensional Methods and

Inference on Structural and Treatment Effects.” The Journal of Economic Perspectives 28(2):29–50.

37

Bishop, Christopher M et al. 2006. Pattern recognition and machine learning. Vol. 1 springer New York.

Blei, David M. 2012. “Probabilistic topic models.” Communications of the ACM 55(4):77–84.

Blei, David M. 2014. “Build, compute, critique, repeat: data analysis with latent variable models.”

Annual Review of Statistics and Its Application 1:203–232.

Blei, David M, Andrew Y Ng and Michael I Jordan. 2003. “Latent dirichlet allocation.” the Journal of

machine Learning research 3:993–1022.

Boyd, Stephen and Lieven Vandenberghe. 2009. Convex optimization. Cambridge university press.

Carpenter, Bob. 2010. Integrating out multinomial parameters in latent Dirichlet allocation and naive

Bayes for collapsed Gibbs sampling. Technical report Technical report, LingPipe.

Chaney, Allison June-Barlow and David M Blei. 2012. Visualizing Topic Models. In ICWSM.

Chang, Jonathan. 2012. lda: Collapsed Gibbs sampling methods for topic models. R package version

1.3.2.
URL: http://CRAN.R-project.org/package=lda

Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-graber and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In Advances in neural information processing systems.
pp. 288–296.

Chuang, Jason, Christopher D Manning and Jeffrey Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of the International Working Conference on Advanced
Visual Interfaces. ACM pp. 74–77.

Chuang, Jason, Sonal Gupta, Christopher Manning and Jeffrey Heer. 2013. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In Proceedings of the 30th International Conference
on Machine Learning (ICML-13). pp. 612–620.

Cochocki, A and Rolf Unbehauen. 1993. Neural networks for optimization and signal processing. John

Wiley & Sons, Inc.

Council, National Research. 2013. Frontiers in Massive Data Analysis. The National Academies Press.

de Klerk, Etienne and Dmitrii V Pasechnik. 2002. “Approximation of the stability number of a graph via

copositive programming.” SIAM Journal on Optimization 12(4):875–892.

De Marchi, Scott, Christopher Gelpi and Jeffrey D Grynaviski. 2004. “Untangling neural nets.” American

Political Science Review 98(02):371–378.

Dean, Jeffrey and Sanjay Ghemawat. 2008. “MapReduce: simpliﬁed data processing on large clusters.”

Communications of the ACM 51(1):107–113.

Deb, Partha and Pravin K Trivedi. 2002. “The structure of demand for health care: latent class versus

two-part models.” Journal of health economics 21(4):601–625.

Dempster, Arthur P, Nan M Laird and Donald B Rubin. 1977. “Maximum likelihood from incomplete
data via the EM algorithm.” Journal of the Royal Statistical Society. Series B (Methodological) pp. 1–
38.

38

Ding, Weicong, Mohammad H Rohban, Prakash Ishwar and Venkatesh Saligrama. 2013. “Topic discov-

ery through data dependent and random projections.” arXiv preprint arXiv:1303.3664 .

Ding, Weicong, Prakash Ishwar, Mohammad H Rohban and Venkatesh Saligrama. 2013. “Necessary
and Sufﬁcient Conditions for Novel Word Detection in Separable Topic Models.” arXiv preprint
arXiv:1310.7994 .

Donoho, David and Victoria Stodden. 2003. When does non-negative matrix factorization give a correct

decomposition into parts? In Advances in neural information processing systems. p. None.

Doshi-Velez, Finale, Yaorong Ge and Isaac Kohane. 2014. “Comorbidity clusters in autism spectrum

disorders: an electronic health record time-series analysis.” Pediatrics 133(1):e54–e63.

Drton, Mathias and Thomas S Richardson. 2004. “Multimodality of the likelihood in the bivariate

seemingly unrelated regressions model.” Biometrika 91(2):383–392.

DuMouchel, William. 1999. “Bayesian data mining in large frequency tables, with an application to the

FDA spontaneous reporting system.” The American Statistician 53(3):177–190.

Efron, Bradley et al. 1978. “The geometry of exponential families.” The Annals of Statistics 6(2):362–

376.

Eisenstein, Jacob and Eric Xing. 2010. “The CMU 2008 Political Blog Corpus.”.

Fan, Jianqing, Fang Han and Han Liu. 2014. “Challenges of Big Data analysis.” National Science Review

p. nwt032.

Foulds, J. R. and P. Smyth. 2014. Annealing Paths for the Evaluation of Topic Models. In Proceedings

of the Thirtieth Conference Conference on Uncertainty in Artiﬁcial Intelligence.

Gardner, Matthew J, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric Ringger and Kevin Seppi.
2010. The topic browser: An interactive tool for browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.

Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari and Donald B Rubin. 2013.

Bayesian data analysis. CRC press.

Gillis, Nicolas. 2014.

arXiv:1401.5226 .

“The why and how of nonnegative matrix factorization.” arXiv preprint

Gillis, Nicolas and Robert Luce. 2013. “Robust near-separable nonnegative matrix factorization using

linear optimization.” arXiv preprint arXiv:1302.4385 .

Goldstone, AndrewUnderwood et al. 2014. “The Quiet Transformations of Literary Studies: What

Thirteen Thousand Scholars Could Tell Us.”.

Grifﬁths, DMBTL and MIJJB Tenenbaum. 2004. “Hierarchical topic models and the nested Chinese

restaurant process.” Advances in neural information processing systems 16:17.

Grifﬁths, Thomas L and Mark Steyvers. 2004. “Finding scientiﬁc topics.” Proceedings of the National

academy of Sciences of the United States of America 101(Suppl 1):5228–5235.

39

Grimmer, Justin. 2010a. “A Bayesian hierarchical topic model for political texts: Measuring expressed

agendas in Senate press releases.” Political Analysis 18(1):1–35.

Grimmer, Justin. 2010b. “An introduction to Bayesian inference via variational approximations.” Politi-

cal Analysis p. mpq027.

Grimmer, Justin. 2013. Representational Style in Congress: What Legislators Say and why it Matters.

Cambridge University Press.

Grimmer, Justin and Brandon M Stewart. 2013. “Text as data: The promise and pitfalls of automatic

content analysis methods for political texts.” Political Analysis p. mps028.

Grimmer, Justin and Gary King. 2011. “General purpose computer-assisted clustering and conceptual-

ization.” Proceedings of the National Academy of Sciences 108(7):2643–2650.

Hambleton, Ronald K. 1991. Fundamentals of item response theory. Vol. 2 Sage publications.

Hamilton, James D. 1989. “A new approach to the economic analysis of nonstationary time series and

the business cycle.” Econometrica: Journal of the Econometric Society pp. 357–384.

Hoff, Peter D, Adrian E Raftery and Mark S Handcock. 2002. “Latent space approaches to social network

analysis.” Journal of the american Statistical association 97(460):1090–1098.

Hopkins, Daniel J and Gary King. 2010. “A method of automated nonparametric content analysis for

social science.” American Journal of Political Science 54(1):229–247.

Hornik, Kurt. 2005. “A clue for cluster ensembles.” Journal of Statistical Software 14(12).

Hsu, Daniel and Sham M Kakade. 2013. Learning mixtures of spherical gaussians: moment methods and
spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer
Science. ACM pp. 11–20.

Hu, Yuening, Jordan Boyd-Graber, Brianna Satinoff and Alison Smith. 2014. “Interactive topic model-

ing.” Machine Learning 95(3):423–469.

Huang, Furong, UN Niranjan, M Hakeem and Animashree Anandkumar. 2013. “Fast Detection of

Overlapping Communities via Online Tensor Methods.”.

Jordan, Michael I, Zoubin Ghahramani, Tommi S Jaakkola and Lawrence K Saul. 1998. An introduction

to variational methods for graphical models. Springer.

Kalai, Adam Tauman, Ankur Moitra and Gregory Valiant. 2012. “Disentangling gaussians.” Communi-

cations of the ACM 55(2):113–120.

Kass, Robert E and Adrian E Raftery. 1995. “Bayes factors.” Journal of the american statistical associ-

ation 90(430):773–795.

King, Gary. 1989. “Unifying political methodology.” New York: Cambridge .

King, Gary, Jennifer Pan and Margaret E Roberts. 2013. “How censorship in China allows government

criticism but silences collective expression.” American Political Science Review 107(02):326–343.

40

Kivinen, Jyrki and Manfred K Warmuth. 1997. “Exponentiated gradient versus gradient descent for

linear predictors.” Information and Computation 132(1):1–63.

Koltcov, Sergei, Olessia Koltsova and Sergey Nikolenko. 2014. Latent dirichlet allocation: stability and
applications to studies of user-generated content. In Proceedings of the 2014 ACM conference on Web
science. ACM pp. 161–165.

Krippendorff, Klaus. 2012. Content analysis: An introduction to its methodology. Sage.

Kuhn, Harold W. 1955. “The Hungarian method for the assignment problem.” Naval research logistics

quarterly 2(1-2):83–97.

Kumar, Abhishek, Vikas Sindhwani and Prabhanjan Kambadur. 2012. “Fast conical hull algorithms for

near-separable non-negative matrix factorization.” arXiv preprint arXiv:1210.1190 .

Lancichinetti, Andrea, M Irmak Sirer, Jane X Wang, Daniel Acuna, Konrad K¨ording and Lu´ıs A Nunes
Amaral. 2014. “A high-reproducibility and high-accuracy method for automated topic classiﬁcation.”
arXiv preprint arXiv:1402.0422 .

Lau, Jey Han, David Newman and Timothy Baldwin. 2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality. In Proceedings of the European Chapter of the
Association for Computational Linguistics.

Lauderdale, Benjamin E and Tom S Clark. 2014. “Scaling politically meaningful dimensions using texts

and votes.” American Journal of Political Science .

Lazer, David, Alex Sandy Pentland, Lada Adamic, Sinan Aral, Albert Laszlo Barabasi, Devon Brewer,
Nicholas Christakis, Noshir Contractor, James Fowler, Myron Gutmann et al. 2009. “Life in the
network: the coming age of computational social science.” Science (New York, NY) 323(5915):721.

Lim, Yew Jin and Yee Whye Teh. 2007. Variational Bayesian approach to movie rating prediction. In

Proceedings of KDD Cup and Workshop. Vol. 7 Citeseer pp. 15–21.

Lloyd, Stuart. 1982. “Least squares quantization in PCM.” Information Theory, IEEE Transactions on

28(2):129–137.

Mahajan, Meena, Prajakta Nimbhorkar and Kasturi Varadarajan. 2009. The planar k-means problem is

NP-hard. In WALCOM: Algorithms and Computation. Springer pp. 274–285.

McLachlan, Geoffrey and David Peel. 2004. Finite mixture models. John Wiley & Sons.

Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality. In Advances in Neural Information Processing
Systems. pp. 3111–3119.

Mimno, David and David Blei. 2011. Bayesian checking for topic models. In Proceedings of the Con-
ference on Empirical Methods in Natural Language Processing. Association for Computational Lin-
guistics pp. 227–237.

Mimno, David, Hanna M Wallach, Edmund Talley, Miriam Leenders and Andrew McCallum. 2011. Op-
timizing semantic coherence in topic models. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing. Association for Computational Linguistics pp. 262–272.

41

Mullainathan, Sendhil. 2014. “What Big Data Means for Social Science.” Behavioral and Experimental

Seminar.

Murphy, Kevin P. 2012. Machine learning: a probabilistic perspective. MIT press.

Nguyen, Thang, Yuening Hu and Jordan Boyd-Graber. 2014. Anchors Regularized: Adding Robustness
and Extensibility to Scalable Topic-Modeling Algorithms. In Association for Computational Linguis-
tics.

Nielsen, Frank and Richard Nock. 2014. “Further heuristics for k-means: The merge-and-split heuristic

and the (k, l)-means.” arXiv preprint arXiv:1406.6314 .

O’Connor, Brendan. 2014. MiTextExplorer: Linked brushing and mutual information for exploratory
text data analysis. In Proceedings of the Workshop on Interactive Language Learning, Visualization,
and Interfaces. Association for Computational Linguistics pp. 1–13.

OConnor, Brendan, Brandon M Stewart and Noah A Smith. 2013. Learning to Extract International

Relations from Political Context. In Proceedings of ACL.

Papadimitriou, Christos H and Kenneth Steiglitz. 1998. Combinatorial optimization: algorithms and

complexity. Courier Dover Publications.

Park, Jong Hee. 2012. “A Uniﬁed Method for Dynamic and Cross-Sectional Heterogeneity: Introducing

Hidden Markov Panel Models.” American Journal of Political Science 56(4):1040–1054.

Pearson, Karl. 1894. “Contributions to the mathematical theory of evolution.” Philosophical Transac-

tions of the Royal Society of London. A pp. 71–110.

Poole, Keith T and Howard Rosenthal. 1997. Congress: A political-economic history of roll call voting.

Oxford University Press.

Quinn, Kevin M, Burt L Monroe, Michael Colaresi, Michael H Crespin and Dragomir R Radev. 2010.
“How to analyze political attention with minimal assumptions and costs.” American Journal of Politi-
cal Science 54(1):209–228.

Rabiner, Lawrence and Biing-Hwang Juang. 1986. “An introduction to hidden Markov models.” ASSP

Magazine, IEEE 3(1):4–16.

Recht, Ben, Christopher Re, Joel Tropp and Victor Bittorf. 2012. Factoring nonnegative matrices with

linear programs. In Advances in Neural Information Processing Systems. pp. 1214–1222.

Reich, Justin, Dustin Tingley, Jetson Leder-Luis, Margaret E Roberts and Brandon M Stewart. N.d.
Computer Assisted Reading and Discovery for Student Generated Text. Technical report Working
Paper. Export.

Rivers, Douglas. 2003. “Identiﬁcation of multidimensional spatial voting models.” Typescript. Stanford

University .

Robert, Christian P and George Casella. 2004. Monte Carlo statistical methods. Vol. 319 Springer New

York.

42

Roberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis,
Shana Kushner Gadarian, Bethany Albertson and David G Rand. 2014. “Structural Topic Models
for Open-Ended Survey Responses.” American Journal of Political Science .

Ruiz, Francisco J. R., Isabel Valera, Carlos Blanco and Fernando Perez-Cruz. 2014. “Bayesian Non-
parametric Comorbidity Analysis of Psychiatric Disorders.” Journal of Machine Learning Research
15:1215–1247.
URL: http://jmlr.org/papers/v15/ruiz14a.html

Shneiderman, Ben. 1996. The eyes have it: A task by data type taxonomy for information visualizations.

In Visual Languages, 1996. Proceedings., IEEE Symposium on. IEEE pp. 336–343.

Song, Le, Eric P Xing and Ankur P Parikh. 2011. A spectral algorithm for latent tree graphical models. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11). pp. 1065–1072.

Sontag, David and Dan Roy. 2011. Complexity of inference in latent dirichlet allocation. In Advances in

Neural Information Processing Systems. pp. 1008–1016.

Srivastava, Virendera K and David EA Giles. 1987. Seemingly unrelated regression equations models:

estimation and inference. Vol. 80 CRC Press.

Teh, Yee Whye, Michael I Jordan, Matthew J Beal and David M Blei. 2006. “Hierarchical dirichlet

processes.” Journal of the american statistical association 101(476).

Tenenbaum, Joshua B, Charles Kemp, Thomas L Grifﬁths and Noah D Goodman. 2011. “How to grow

a mind: Statistics, structure, and abstraction.” science 331(6022):1279–1285.

Tibshirani, Robert. 1996. “Regression shrinkage and selection via the lasso.” Journal of the Royal Sta-

tistical Society. Series B (Methodological) pp. 267–288.

Vavasis, Stephen A. 2009. “On the complexity of nonnegative matrix factorization.” SIAM Journal on

Optimization 20(3):1364–1377.

Wallach, Hanna M, Iain Murray, Ruslan Salakhutdinov and David Mimno. 2009. Evaluation methods
for topic models. In Proceedings of the 26th Annual International Conference on Machine Learning.
ACM pp. 1105–1112.

Ward, Michael D, Brian D Greenhill and Kristin M Bakke. 2010. “The perils of policy by p-value:

Predicting civil conﬂicts.” Journal of Peace Research 47(4):363–375.

Wolpert, David H and William G Macready. 1997. “No free lunch theorems for optimization.” Evolu-

tionary Computation, IEEE Transactions on 1(1):67–82.

Yao, Limin, David Mimno and Andrew McCallum. 2009. Efﬁcient methods for topic model inference on
streaming document collections. In Proceedings of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining. ACM pp. 937–946.

Zhou, Tianyi, Jeff Bilmes and Carlos Guestrin. 2014. “Divide-and-Conquer Learning by Anchoring a

Conical Hull.” arXiv preprint arXiv:1406.5752 .

43

Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Even in traditional general linear models, there is also always the choice of model speciﬁcation in both

variables and functional form. Although multimodality brings new issues to the table, the responsibility

of the researcher to carefully validate the chosen model is fundamentally the same. This is true regardless

of whether the choice between competing models arises due to a non-convex latent variable model or

due to the selection of an important model tuning parameter in a globally convex problem. Thus even if

multimodality is an unfamiliar problem, social scientists can draw on the same set of best practices that

they employ throughout their research.

An important practical contribution of this chapter is that it extends the set of tools available to

scholars using topic models in applied research. While we have focused on STM, many of the procedures

we use will be helpful for a broader class of latent variable models. For instance, the approaches to

aligning topics and calculating stability across runs can all be applied directly to the broader class of

statistical topic models and with minor modiﬁcations to most latent variable models.

We see great potential for the analysis of “big” data in the social sciences, but rather than focus on

the data we have taken a more methodological focus. We think this has important implications for both

methodological development but also could structure the types of questions we ask, and the types of data

sets we seek to build. Methodologically, we think that there will be important advances in areas such

as optimal initialization strategies, which is especially important as our data sets grow in size. From an

applied perspective, users will be unlikely to want to wait for extended periods of time in order to get

even a single set of results. Advances in computational power needs to be matched with smart ways to

leverage that power. From a research design perspective, we think more focus should be put on bringing

greater structure to so called “unstructured” data. In the STM we focus on the inclusion of metadata

for modeling and hypothesis testing, but this is only one possible use. Can more direct supervision help

us with issues of multimodality? Of course, in the end, big data will be at its best when there is active

dialogue between those who pose the big question and those who might provide the big answers.

References
Airoldi, Edoardo M, David M Blei, Stephen E Fienberg and Eric P Xing. 2009. Mixed membership

stochastic blockmodels. In Advances in Neural Information Processing Systems. pp. 33–40.

Anandkumar, Anima, Rong Ge, Daniel Hsu and Sham M Kakade. 2013. “A tensor spectral approach to

learning mixed membership community models.” arXiv preprint arXiv:1302.2684 .

36

Anandkumar, Anima, Rong Ge, Daniel Hsu, Sham M Kakade and Matus Telgarsky. 2012. “Tensor

decompositions for learning latent variable models.” arXiv preprint arXiv:1210.7559 .

Anandkumar, Anima, Yi-kai Liu, Daniel J Hsu, Dean P Foster and Sham M Kakade. 2012. A spec-
tral algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems.
pp. 917–925.

Anandkumar, Animashree, Daniel Hsu, Adel Javanmard and Sham Kakade. 2013. Learning linear
bayesian networks with latent variables. In Proceedings of The 30th International Conference on Ma-
chine Learning. pp. 249–257.

Anandkumar, Animashree, Daniel Hsu and Sham M Kakade. 2012. “A method of moments for mixture

models and hidden Markov models.” arXiv preprint arXiv:1203.0683 .

Arora, Sanjeev, Aditya Bhaskara, Rong Ge and Tengyu Ma. 2013. “Provable bounds for learning some

deep representations.” arXiv preprint .

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2012. Learning topic models–going beyond svd. In Foun-

dations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE pp. 1–10.

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2013. “New algorithms for learning incoherent and over-

complete dictionaries.” arXiv preprint arXiv:1308.6273 .

Arora, Sanjeev, Rong Ge, Ankur Moitra and Sushant Sachdeva. 2012. Provable ICA with unknown
Gaussian noise, with implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems. pp. 2375–2383.

Arora, Sanjeev, Rong Ge, Ravindran Kannan and Ankur Moitra. 2012. Computing a nonnegative ma-
trix factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of
computing. ACM pp. 145–162.

Arora, Sanjeev, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu
and Michael Zhu. 2013. A Practical Algorithm for Topic Modeling with Provable Guarantees. In
Proceedings of The 30th International Conference on Machine Learning. pp. 280–288.

Arthur, David and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial
and Applied Mathematics pp. 1027–1035.

Asuncion Jr, Arthur Uy. 2011. Distributed and accelerated inference algorithms for probabilistic graph-

ical models. Technical report California State University at Long Beach.

Bahmani, Bahman, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei Vassilvitskii. 2012.

“Scalable k-means++.” Proceedings of the VLDB Endowment 5(7):622–633.

Beck, Nathaniel, Gary King and Langche Zeng. 2000. “Improving quantitative studies of international

conﬂict: A conjecture.” American Political Science Review pp. 21–35.

Belloni, Alexandre, Victor Chernozhukov and Christian Hansen. 2014. “High-Dimensional Methods and

Inference on Structural and Treatment Effects.” The Journal of Economic Perspectives 28(2):29–50.

37

Bishop, Christopher M et al. 2006. Pattern recognition and machine learning. Vol. 1 springer New York.

Blei, David M. 2012. “Probabilistic topic models.” Communications of the ACM 55(4):77–84.

Blei, David M. 2014. “Build, compute, critique, repeat: data analysis with latent variable models.”

Annual Review of Statistics and Its Application 1:203–232.

Blei, David M, Andrew Y Ng and Michael I Jordan. 2003. “Latent dirichlet allocation.” the Journal of

machine Learning research 3:993–1022.

Boyd, Stephen and Lieven Vandenberghe. 2009. Convex optimization. Cambridge university press.

Carpenter, Bob. 2010. Integrating out multinomial parameters in latent Dirichlet allocation and naive

Bayes for collapsed Gibbs sampling. Technical report Technical report, LingPipe.

Chaney, Allison June-Barlow and David M Blei. 2012. Visualizing Topic Models. In ICWSM.

Chang, Jonathan. 2012. lda: Collapsed Gibbs sampling methods for topic models. R package version

1.3.2.
URL: http://CRAN.R-project.org/package=lda

Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-graber and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In Advances in neural information processing systems.
pp. 288–296.

Chuang, Jason, Christopher D Manning and Jeffrey Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of the International Working Conference on Advanced
Visual Interfaces. ACM pp. 74–77.

Chuang, Jason, Sonal Gupta, Christopher Manning and Jeffrey Heer. 2013. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In Proceedings of the 30th International Conference
on Machine Learning (ICML-13). pp. 612–620.

Cochocki, A and Rolf Unbehauen. 1993. Neural networks for optimization and signal processing. John

Wiley & Sons, Inc.

Council, National Research. 2013. Frontiers in Massive Data Analysis. The National Academies Press.

de Klerk, Etienne and Dmitrii V Pasechnik. 2002. “Approximation of the stability number of a graph via

copositive programming.” SIAM Journal on Optimization 12(4):875–892.

De Marchi, Scott, Christopher Gelpi and Jeffrey D Grynaviski. 2004. “Untangling neural nets.” American

Political Science Review 98(02):371–378.

Dean, Jeffrey and Sanjay Ghemawat. 2008. “MapReduce: simpliﬁed data processing on large clusters.”

Communications of the ACM 51(1):107–113.

Deb, Partha and Pravin K Trivedi. 2002. “The structure of demand for health care: latent class versus

two-part models.” Journal of health economics 21(4):601–625.

Dempster, Arthur P, Nan M Laird and Donald B Rubin. 1977. “Maximum likelihood from incomplete
data via the EM algorithm.” Journal of the Royal Statistical Society. Series B (Methodological) pp. 1–
38.

38

Ding, Weicong, Mohammad H Rohban, Prakash Ishwar and Venkatesh Saligrama. 2013. “Topic discov-

ery through data dependent and random projections.” arXiv preprint arXiv:1303.3664 .

Ding, Weicong, Prakash Ishwar, Mohammad H Rohban and Venkatesh Saligrama. 2013. “Necessary
and Sufﬁcient Conditions for Novel Word Detection in Separable Topic Models.” arXiv preprint
arXiv:1310.7994 .

Donoho, David and Victoria Stodden. 2003. When does non-negative matrix factorization give a correct

decomposition into parts? In Advances in neural information processing systems. p. None.

Doshi-Velez, Finale, Yaorong Ge and Isaac Kohane. 2014. “Comorbidity clusters in autism spectrum

disorders: an electronic health record time-series analysis.” Pediatrics 133(1):e54–e63.

Drton, Mathias and Thomas S Richardson. 2004. “Multimodality of the likelihood in the bivariate

seemingly unrelated regressions model.” Biometrika 91(2):383–392.

DuMouchel, William. 1999. “Bayesian data mining in large frequency tables, with an application to the

FDA spontaneous reporting system.” The American Statistician 53(3):177–190.

Efron, Bradley et al. 1978. “The geometry of exponential families.” The Annals of Statistics 6(2):362–

376.

Eisenstein, Jacob and Eric Xing. 2010. “The CMU 2008 Political Blog Corpus.”.

Fan, Jianqing, Fang Han and Han Liu. 2014. “Challenges of Big Data analysis.” National Science Review

p. nwt032.

Foulds, J. R. and P. Smyth. 2014. Annealing Paths for the Evaluation of Topic Models. In Proceedings

of the Thirtieth Conference Conference on Uncertainty in Artiﬁcial Intelligence.

Gardner, Matthew J, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric Ringger and Kevin Seppi.
2010. The topic browser: An interactive tool for browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.

Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari and Donald B Rubin. 2013.

Bayesian data analysis. CRC press.

Gillis, Nicolas. 2014.

arXiv:1401.5226 .

“The why and how of nonnegative matrix factorization.” arXiv preprint

Gillis, Nicolas and Robert Luce. 2013. “Robust near-separable nonnegative matrix factorization using

linear optimization.” arXiv preprint arXiv:1302.4385 .

Goldstone, AndrewUnderwood et al. 2014. “The Quiet Transformations of Literary Studies: What

Thirteen Thousand Scholars Could Tell Us.”.

Grifﬁths, DMBTL and MIJJB Tenenbaum. 2004. “Hierarchical topic models and the nested Chinese

restaurant process.” Advances in neural information processing systems 16:17.

Grifﬁths, Thomas L and Mark Steyvers. 2004. “Finding scientiﬁc topics.” Proceedings of the National

academy of Sciences of the United States of America 101(Suppl 1):5228–5235.

39

Grimmer, Justin. 2010a. “A Bayesian hierarchical topic model for political texts: Measuring expressed

agendas in Senate press releases.” Political Analysis 18(1):1–35.

Grimmer, Justin. 2010b. “An introduction to Bayesian inference via variational approximations.” Politi-

cal Analysis p. mpq027.

Grimmer, Justin. 2013. Representational Style in Congress: What Legislators Say and why it Matters.

Cambridge University Press.

Grimmer, Justin and Brandon M Stewart. 2013. “Text as data: The promise and pitfalls of automatic

content analysis methods for political texts.” Political Analysis p. mps028.

Grimmer, Justin and Gary King. 2011. “General purpose computer-assisted clustering and conceptual-

ization.” Proceedings of the National Academy of Sciences 108(7):2643–2650.

Hambleton, Ronald K. 1991. Fundamentals of item response theory. Vol. 2 Sage publications.

Hamilton, James D. 1989. “A new approach to the economic analysis of nonstationary time series and

the business cycle.” Econometrica: Journal of the Econometric Society pp. 357–384.

Hoff, Peter D, Adrian E Raftery and Mark S Handcock. 2002. “Latent space approaches to social network

analysis.” Journal of the american Statistical association 97(460):1090–1098.

Hopkins, Daniel J and Gary King. 2010. “A method of automated nonparametric content analysis for

social science.” American Journal of Political Science 54(1):229–247.

Hornik, Kurt. 2005. “A clue for cluster ensembles.” Journal of Statistical Software 14(12).

Hsu, Daniel and Sham M Kakade. 2013. Learning mixtures of spherical gaussians: moment methods and
spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer
Science. ACM pp. 11–20.

Hu, Yuening, Jordan Boyd-Graber, Brianna Satinoff and Alison Smith. 2014. “Interactive topic model-

ing.” Machine Learning 95(3):423–469.

Huang, Furong, UN Niranjan, M Hakeem and Animashree Anandkumar. 2013. “Fast Detection of

Overlapping Communities via Online Tensor Methods.”.

Jordan, Michael I, Zoubin Ghahramani, Tommi S Jaakkola and Lawrence K Saul. 1998. An introduction

to variational methods for graphical models. Springer.

Kalai, Adam Tauman, Ankur Moitra and Gregory Valiant. 2012. “Disentangling gaussians.” Communi-

cations of the ACM 55(2):113–120.

Kass, Robert E and Adrian E Raftery. 1995. “Bayes factors.” Journal of the american statistical associ-

ation 90(430):773–795.

King, Gary. 1989. “Unifying political methodology.” New York: Cambridge .

King, Gary, Jennifer Pan and Margaret E Roberts. 2013. “How censorship in China allows government

criticism but silences collective expression.” American Political Science Review 107(02):326–343.

40

Kivinen, Jyrki and Manfred K Warmuth. 1997. “Exponentiated gradient versus gradient descent for

linear predictors.” Information and Computation 132(1):1–63.

Koltcov, Sergei, Olessia Koltsova and Sergey Nikolenko. 2014. Latent dirichlet allocation: stability and
applications to studies of user-generated content. In Proceedings of the 2014 ACM conference on Web
science. ACM pp. 161–165.

Krippendorff, Klaus. 2012. Content analysis: An introduction to its methodology. Sage.

Kuhn, Harold W. 1955. “The Hungarian method for the assignment problem.” Naval research logistics

quarterly 2(1-2):83–97.

Kumar, Abhishek, Vikas Sindhwani and Prabhanjan Kambadur. 2012. “Fast conical hull algorithms for

near-separable non-negative matrix factorization.” arXiv preprint arXiv:1210.1190 .

Lancichinetti, Andrea, M Irmak Sirer, Jane X Wang, Daniel Acuna, Konrad K¨ording and Lu´ıs A Nunes
Amaral. 2014. “A high-reproducibility and high-accuracy method for automated topic classiﬁcation.”
arXiv preprint arXiv:1402.0422 .

Lau, Jey Han, David Newman and Timothy Baldwin. 2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality. In Proceedings of the European Chapter of the
Association for Computational Linguistics.

Lauderdale, Benjamin E and Tom S Clark. 2014. “Scaling politically meaningful dimensions using texts

and votes.” American Journal of Political Science .

Lazer, David, Alex Sandy Pentland, Lada Adamic, Sinan Aral, Albert Laszlo Barabasi, Devon Brewer,
Nicholas Christakis, Noshir Contractor, James Fowler, Myron Gutmann et al. 2009. “Life in the
network: the coming age of computational social science.” Science (New York, NY) 323(5915):721.

Lim, Yew Jin and Yee Whye Teh. 2007. Variational Bayesian approach to movie rating prediction. In

Proceedings of KDD Cup and Workshop. Vol. 7 Citeseer pp. 15–21.

Lloyd, Stuart. 1982. “Least squares quantization in PCM.” Information Theory, IEEE Transactions on

28(2):129–137.

Mahajan, Meena, Prajakta Nimbhorkar and Kasturi Varadarajan. 2009. The planar k-means problem is

NP-hard. In WALCOM: Algorithms and Computation. Springer pp. 274–285.

McLachlan, Geoffrey and David Peel. 2004. Finite mixture models. John Wiley & Sons.

Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality. In Advances in Neural Information Processing
Systems. pp. 3111–3119.

Mimno, David and David Blei. 2011. Bayesian checking for topic models. In Proceedings of the Con-
ference on Empirical Methods in Natural Language Processing. Association for Computational Lin-
guistics pp. 227–237.

Mimno, David, Hanna M Wallach, Edmund Talley, Miriam Leenders and Andrew McCallum. 2011. Op-
timizing semantic coherence in topic models. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing. Association for Computational Linguistics pp. 262–272.

41

Mullainathan, Sendhil. 2014. “What Big Data Means for Social Science.” Behavioral and Experimental

Seminar.

Murphy, Kevin P. 2012. Machine learning: a probabilistic perspective. MIT press.

Nguyen, Thang, Yuening Hu and Jordan Boyd-Graber. 2014. Anchors Regularized: Adding Robustness
and Extensibility to Scalable Topic-Modeling Algorithms. In Association for Computational Linguis-
tics.

Nielsen, Frank and Richard Nock. 2014. “Further heuristics for k-means: The merge-and-split heuristic

and the (k, l)-means.” arXiv preprint arXiv:1406.6314 .

O’Connor, Brendan. 2014. MiTextExplorer: Linked brushing and mutual information for exploratory
text data analysis. In Proceedings of the Workshop on Interactive Language Learning, Visualization,
and Interfaces. Association for Computational Linguistics pp. 1–13.

OConnor, Brendan, Brandon M Stewart and Noah A Smith. 2013. Learning to Extract International

Relations from Political Context. In Proceedings of ACL.

Papadimitriou, Christos H and Kenneth Steiglitz. 1998. Combinatorial optimization: algorithms and

complexity. Courier Dover Publications.

Park, Jong Hee. 2012. “A Uniﬁed Method for Dynamic and Cross-Sectional Heterogeneity: Introducing

Hidden Markov Panel Models.” American Journal of Political Science 56(4):1040–1054.

Pearson, Karl. 1894. “Contributions to the mathematical theory of evolution.” Philosophical Transac-

tions of the Royal Society of London. A pp. 71–110.

Poole, Keith T and Howard Rosenthal. 1997. Congress: A political-economic history of roll call voting.

Oxford University Press.

Quinn, Kevin M, Burt L Monroe, Michael Colaresi, Michael H Crespin and Dragomir R Radev. 2010.
“How to analyze political attention with minimal assumptions and costs.” American Journal of Politi-
cal Science 54(1):209–228.

Rabiner, Lawrence and Biing-Hwang Juang. 1986. “An introduction to hidden Markov models.” ASSP

Magazine, IEEE 3(1):4–16.

Recht, Ben, Christopher Re, Joel Tropp and Victor Bittorf. 2012. Factoring nonnegative matrices with

linear programs. In Advances in Neural Information Processing Systems. pp. 1214–1222.

Reich, Justin, Dustin Tingley, Jetson Leder-Luis, Margaret E Roberts and Brandon M Stewart. N.d.
Computer Assisted Reading and Discovery for Student Generated Text. Technical report Working
Paper. Export.

Rivers, Douglas. 2003. “Identiﬁcation of multidimensional spatial voting models.” Typescript. Stanford

University .

Robert, Christian P and George Casella. 2004. Monte Carlo statistical methods. Vol. 319 Springer New

York.

42

Roberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis,
Shana Kushner Gadarian, Bethany Albertson and David G Rand. 2014. “Structural Topic Models
for Open-Ended Survey Responses.” American Journal of Political Science .

Ruiz, Francisco J. R., Isabel Valera, Carlos Blanco and Fernando Perez-Cruz. 2014. “Bayesian Non-
parametric Comorbidity Analysis of Psychiatric Disorders.” Journal of Machine Learning Research
15:1215–1247.
URL: http://jmlr.org/papers/v15/ruiz14a.html

Shneiderman, Ben. 1996. The eyes have it: A task by data type taxonomy for information visualizations.

In Visual Languages, 1996. Proceedings., IEEE Symposium on. IEEE pp. 336–343.

Song, Le, Eric P Xing and Ankur P Parikh. 2011. A spectral algorithm for latent tree graphical models. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11). pp. 1065–1072.

Sontag, David and Dan Roy. 2011. Complexity of inference in latent dirichlet allocation. In Advances in

Neural Information Processing Systems. pp. 1008–1016.

Srivastava, Virendera K and David EA Giles. 1987. Seemingly unrelated regression equations models:

estimation and inference. Vol. 80 CRC Press.

Teh, Yee Whye, Michael I Jordan, Matthew J Beal and David M Blei. 2006. “Hierarchical dirichlet

processes.” Journal of the american statistical association 101(476).

Tenenbaum, Joshua B, Charles Kemp, Thomas L Grifﬁths and Noah D Goodman. 2011. “How to grow

a mind: Statistics, structure, and abstraction.” science 331(6022):1279–1285.

Tibshirani, Robert. 1996. “Regression shrinkage and selection via the lasso.” Journal of the Royal Sta-

tistical Society. Series B (Methodological) pp. 267–288.

Vavasis, Stephen A. 2009. “On the complexity of nonnegative matrix factorization.” SIAM Journal on

Optimization 20(3):1364–1377.

Wallach, Hanna M, Iain Murray, Ruslan Salakhutdinov and David Mimno. 2009. Evaluation methods
for topic models. In Proceedings of the 26th Annual International Conference on Machine Learning.
ACM pp. 1105–1112.

Ward, Michael D, Brian D Greenhill and Kristin M Bakke. 2010. “The perils of policy by p-value:

Predicting civil conﬂicts.” Journal of Peace Research 47(4):363–375.

Wolpert, David H and William G Macready. 1997. “No free lunch theorems for optimization.” Evolu-

tionary Computation, IEEE Transactions on 1(1):67–82.

Yao, Limin, David Mimno and Andrew McCallum. 2009. Efﬁcient methods for topic model inference on
streaming document collections. In Proceedings of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining. ACM pp. 937–946.

Zhou, Tianyi, Jeff Bilmes and Carlos Guestrin. 2014. “Divide-and-Conquer Learning by Anchoring a

Conical Hull.” arXiv preprint arXiv:1406.5752 .

43

Figure 1: Five example topics from the reference model. These are given the labels Supreme Court,
Cheney, Global Warming, Iran/N.K. Nukes, and Wright respectively.

44

Topic 18:  law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,feder, requir, amend, protect, gun, govern, allow, appeal, citizenTopic 30:  presid, vice, cheney, offic, presidenti, first, execut, dick, decis, leader,role, histori, nation, branch, power, part, govern, order, idea, washingtonTopic 48:  global, warm, research, climat, studi, chang, scienc, scientist, gore, caus,human, scientif, earth, emiss, planet, cell, environment, report, water, greenTopic 60:  iran, nuclear, threat, weapon, iranian, program, missil, north, bomb, defens,korea, strike, sanction, intern, build, militari, intellig, capabl, pose,developTopic 71:  black, wright, white, race, church, racial, racist, pastor, jeremiah,africanamerican, racism, african, comment, reverend, king, controversi, rev,view, communiti, southNavigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Even in traditional general linear models, there is also always the choice of model speciﬁcation in both

variables and functional form. Although multimodality brings new issues to the table, the responsibility

of the researcher to carefully validate the chosen model is fundamentally the same. This is true regardless

of whether the choice between competing models arises due to a non-convex latent variable model or

due to the selection of an important model tuning parameter in a globally convex problem. Thus even if

multimodality is an unfamiliar problem, social scientists can draw on the same set of best practices that

they employ throughout their research.

An important practical contribution of this chapter is that it extends the set of tools available to

scholars using topic models in applied research. While we have focused on STM, many of the procedures

we use will be helpful for a broader class of latent variable models. For instance, the approaches to

aligning topics and calculating stability across runs can all be applied directly to the broader class of

statistical topic models and with minor modiﬁcations to most latent variable models.

We see great potential for the analysis of “big” data in the social sciences, but rather than focus on

the data we have taken a more methodological focus. We think this has important implications for both

methodological development but also could structure the types of questions we ask, and the types of data

sets we seek to build. Methodologically, we think that there will be important advances in areas such

as optimal initialization strategies, which is especially important as our data sets grow in size. From an

applied perspective, users will be unlikely to want to wait for extended periods of time in order to get

even a single set of results. Advances in computational power needs to be matched with smart ways to

leverage that power. From a research design perspective, we think more focus should be put on bringing

greater structure to so called “unstructured” data. In the STM we focus on the inclusion of metadata

for modeling and hypothesis testing, but this is only one possible use. Can more direct supervision help

us with issues of multimodality? Of course, in the end, big data will be at its best when there is active

dialogue between those who pose the big question and those who might provide the big answers.

References
Airoldi, Edoardo M, David M Blei, Stephen E Fienberg and Eric P Xing. 2009. Mixed membership

stochastic blockmodels. In Advances in Neural Information Processing Systems. pp. 33–40.

Anandkumar, Anima, Rong Ge, Daniel Hsu and Sham M Kakade. 2013. “A tensor spectral approach to

learning mixed membership community models.” arXiv preprint arXiv:1302.2684 .

36

Anandkumar, Anima, Rong Ge, Daniel Hsu, Sham M Kakade and Matus Telgarsky. 2012. “Tensor

decompositions for learning latent variable models.” arXiv preprint arXiv:1210.7559 .

Anandkumar, Anima, Yi-kai Liu, Daniel J Hsu, Dean P Foster and Sham M Kakade. 2012. A spec-
tral algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems.
pp. 917–925.

Anandkumar, Animashree, Daniel Hsu, Adel Javanmard and Sham Kakade. 2013. Learning linear
bayesian networks with latent variables. In Proceedings of The 30th International Conference on Ma-
chine Learning. pp. 249–257.

Anandkumar, Animashree, Daniel Hsu and Sham M Kakade. 2012. “A method of moments for mixture

models and hidden Markov models.” arXiv preprint arXiv:1203.0683 .

Arora, Sanjeev, Aditya Bhaskara, Rong Ge and Tengyu Ma. 2013. “Provable bounds for learning some

deep representations.” arXiv preprint .

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2012. Learning topic models–going beyond svd. In Foun-

dations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE pp. 1–10.

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2013. “New algorithms for learning incoherent and over-

complete dictionaries.” arXiv preprint arXiv:1308.6273 .

Arora, Sanjeev, Rong Ge, Ankur Moitra and Sushant Sachdeva. 2012. Provable ICA with unknown
Gaussian noise, with implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems. pp. 2375–2383.

Arora, Sanjeev, Rong Ge, Ravindran Kannan and Ankur Moitra. 2012. Computing a nonnegative ma-
trix factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of
computing. ACM pp. 145–162.

Arora, Sanjeev, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu
and Michael Zhu. 2013. A Practical Algorithm for Topic Modeling with Provable Guarantees. In
Proceedings of The 30th International Conference on Machine Learning. pp. 280–288.

Arthur, David and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial
and Applied Mathematics pp. 1027–1035.

Asuncion Jr, Arthur Uy. 2011. Distributed and accelerated inference algorithms for probabilistic graph-

ical models. Technical report California State University at Long Beach.

Bahmani, Bahman, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei Vassilvitskii. 2012.

“Scalable k-means++.” Proceedings of the VLDB Endowment 5(7):622–633.

Beck, Nathaniel, Gary King and Langche Zeng. 2000. “Improving quantitative studies of international

conﬂict: A conjecture.” American Political Science Review pp. 21–35.

Belloni, Alexandre, Victor Chernozhukov and Christian Hansen. 2014. “High-Dimensional Methods and

Inference on Structural and Treatment Effects.” The Journal of Economic Perspectives 28(2):29–50.

37

Bishop, Christopher M et al. 2006. Pattern recognition and machine learning. Vol. 1 springer New York.

Blei, David M. 2012. “Probabilistic topic models.” Communications of the ACM 55(4):77–84.

Blei, David M. 2014. “Build, compute, critique, repeat: data analysis with latent variable models.”

Annual Review of Statistics and Its Application 1:203–232.

Blei, David M, Andrew Y Ng and Michael I Jordan. 2003. “Latent dirichlet allocation.” the Journal of

machine Learning research 3:993–1022.

Boyd, Stephen and Lieven Vandenberghe. 2009. Convex optimization. Cambridge university press.

Carpenter, Bob. 2010. Integrating out multinomial parameters in latent Dirichlet allocation and naive

Bayes for collapsed Gibbs sampling. Technical report Technical report, LingPipe.

Chaney, Allison June-Barlow and David M Blei. 2012. Visualizing Topic Models. In ICWSM.

Chang, Jonathan. 2012. lda: Collapsed Gibbs sampling methods for topic models. R package version

1.3.2.
URL: http://CRAN.R-project.org/package=lda

Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-graber and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In Advances in neural information processing systems.
pp. 288–296.

Chuang, Jason, Christopher D Manning and Jeffrey Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of the International Working Conference on Advanced
Visual Interfaces. ACM pp. 74–77.

Chuang, Jason, Sonal Gupta, Christopher Manning and Jeffrey Heer. 2013. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In Proceedings of the 30th International Conference
on Machine Learning (ICML-13). pp. 612–620.

Cochocki, A and Rolf Unbehauen. 1993. Neural networks for optimization and signal processing. John

Wiley & Sons, Inc.

Council, National Research. 2013. Frontiers in Massive Data Analysis. The National Academies Press.

de Klerk, Etienne and Dmitrii V Pasechnik. 2002. “Approximation of the stability number of a graph via

copositive programming.” SIAM Journal on Optimization 12(4):875–892.

De Marchi, Scott, Christopher Gelpi and Jeffrey D Grynaviski. 2004. “Untangling neural nets.” American

Political Science Review 98(02):371–378.

Dean, Jeffrey and Sanjay Ghemawat. 2008. “MapReduce: simpliﬁed data processing on large clusters.”

Communications of the ACM 51(1):107–113.

Deb, Partha and Pravin K Trivedi. 2002. “The structure of demand for health care: latent class versus

two-part models.” Journal of health economics 21(4):601–625.

Dempster, Arthur P, Nan M Laird and Donald B Rubin. 1977. “Maximum likelihood from incomplete
data via the EM algorithm.” Journal of the Royal Statistical Society. Series B (Methodological) pp. 1–
38.

38

Ding, Weicong, Mohammad H Rohban, Prakash Ishwar and Venkatesh Saligrama. 2013. “Topic discov-

ery through data dependent and random projections.” arXiv preprint arXiv:1303.3664 .

Ding, Weicong, Prakash Ishwar, Mohammad H Rohban and Venkatesh Saligrama. 2013. “Necessary
and Sufﬁcient Conditions for Novel Word Detection in Separable Topic Models.” arXiv preprint
arXiv:1310.7994 .

Donoho, David and Victoria Stodden. 2003. When does non-negative matrix factorization give a correct

decomposition into parts? In Advances in neural information processing systems. p. None.

Doshi-Velez, Finale, Yaorong Ge and Isaac Kohane. 2014. “Comorbidity clusters in autism spectrum

disorders: an electronic health record time-series analysis.” Pediatrics 133(1):e54–e63.

Drton, Mathias and Thomas S Richardson. 2004. “Multimodality of the likelihood in the bivariate

seemingly unrelated regressions model.” Biometrika 91(2):383–392.

DuMouchel, William. 1999. “Bayesian data mining in large frequency tables, with an application to the

FDA spontaneous reporting system.” The American Statistician 53(3):177–190.

Efron, Bradley et al. 1978. “The geometry of exponential families.” The Annals of Statistics 6(2):362–

376.

Eisenstein, Jacob and Eric Xing. 2010. “The CMU 2008 Political Blog Corpus.”.

Fan, Jianqing, Fang Han and Han Liu. 2014. “Challenges of Big Data analysis.” National Science Review

p. nwt032.

Foulds, J. R. and P. Smyth. 2014. Annealing Paths for the Evaluation of Topic Models. In Proceedings

of the Thirtieth Conference Conference on Uncertainty in Artiﬁcial Intelligence.

Gardner, Matthew J, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric Ringger and Kevin Seppi.
2010. The topic browser: An interactive tool for browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.

Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari and Donald B Rubin. 2013.

Bayesian data analysis. CRC press.

Gillis, Nicolas. 2014.

arXiv:1401.5226 .

“The why and how of nonnegative matrix factorization.” arXiv preprint

Gillis, Nicolas and Robert Luce. 2013. “Robust near-separable nonnegative matrix factorization using

linear optimization.” arXiv preprint arXiv:1302.4385 .

Goldstone, AndrewUnderwood et al. 2014. “The Quiet Transformations of Literary Studies: What

Thirteen Thousand Scholars Could Tell Us.”.

Grifﬁths, DMBTL and MIJJB Tenenbaum. 2004. “Hierarchical topic models and the nested Chinese

restaurant process.” Advances in neural information processing systems 16:17.

Grifﬁths, Thomas L and Mark Steyvers. 2004. “Finding scientiﬁc topics.” Proceedings of the National

academy of Sciences of the United States of America 101(Suppl 1):5228–5235.

39

Grimmer, Justin. 2010a. “A Bayesian hierarchical topic model for political texts: Measuring expressed

agendas in Senate press releases.” Political Analysis 18(1):1–35.

Grimmer, Justin. 2010b. “An introduction to Bayesian inference via variational approximations.” Politi-

cal Analysis p. mpq027.

Grimmer, Justin. 2013. Representational Style in Congress: What Legislators Say and why it Matters.

Cambridge University Press.

Grimmer, Justin and Brandon M Stewart. 2013. “Text as data: The promise and pitfalls of automatic

content analysis methods for political texts.” Political Analysis p. mps028.

Grimmer, Justin and Gary King. 2011. “General purpose computer-assisted clustering and conceptual-

ization.” Proceedings of the National Academy of Sciences 108(7):2643–2650.

Hambleton, Ronald K. 1991. Fundamentals of item response theory. Vol. 2 Sage publications.

Hamilton, James D. 1989. “A new approach to the economic analysis of nonstationary time series and

the business cycle.” Econometrica: Journal of the Econometric Society pp. 357–384.

Hoff, Peter D, Adrian E Raftery and Mark S Handcock. 2002. “Latent space approaches to social network

analysis.” Journal of the american Statistical association 97(460):1090–1098.

Hopkins, Daniel J and Gary King. 2010. “A method of automated nonparametric content analysis for

social science.” American Journal of Political Science 54(1):229–247.

Hornik, Kurt. 2005. “A clue for cluster ensembles.” Journal of Statistical Software 14(12).

Hsu, Daniel and Sham M Kakade. 2013. Learning mixtures of spherical gaussians: moment methods and
spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer
Science. ACM pp. 11–20.

Hu, Yuening, Jordan Boyd-Graber, Brianna Satinoff and Alison Smith. 2014. “Interactive topic model-

ing.” Machine Learning 95(3):423–469.

Huang, Furong, UN Niranjan, M Hakeem and Animashree Anandkumar. 2013. “Fast Detection of

Overlapping Communities via Online Tensor Methods.”.

Jordan, Michael I, Zoubin Ghahramani, Tommi S Jaakkola and Lawrence K Saul. 1998. An introduction

to variational methods for graphical models. Springer.

Kalai, Adam Tauman, Ankur Moitra and Gregory Valiant. 2012. “Disentangling gaussians.” Communi-

cations of the ACM 55(2):113–120.

Kass, Robert E and Adrian E Raftery. 1995. “Bayes factors.” Journal of the american statistical associ-

ation 90(430):773–795.

King, Gary. 1989. “Unifying political methodology.” New York: Cambridge .

King, Gary, Jennifer Pan and Margaret E Roberts. 2013. “How censorship in China allows government

criticism but silences collective expression.” American Political Science Review 107(02):326–343.

40

Kivinen, Jyrki and Manfred K Warmuth. 1997. “Exponentiated gradient versus gradient descent for

linear predictors.” Information and Computation 132(1):1–63.

Koltcov, Sergei, Olessia Koltsova and Sergey Nikolenko. 2014. Latent dirichlet allocation: stability and
applications to studies of user-generated content. In Proceedings of the 2014 ACM conference on Web
science. ACM pp. 161–165.

Krippendorff, Klaus. 2012. Content analysis: An introduction to its methodology. Sage.

Kuhn, Harold W. 1955. “The Hungarian method for the assignment problem.” Naval research logistics

quarterly 2(1-2):83–97.

Kumar, Abhishek, Vikas Sindhwani and Prabhanjan Kambadur. 2012. “Fast conical hull algorithms for

near-separable non-negative matrix factorization.” arXiv preprint arXiv:1210.1190 .

Lancichinetti, Andrea, M Irmak Sirer, Jane X Wang, Daniel Acuna, Konrad K¨ording and Lu´ıs A Nunes
Amaral. 2014. “A high-reproducibility and high-accuracy method for automated topic classiﬁcation.”
arXiv preprint arXiv:1402.0422 .

Lau, Jey Han, David Newman and Timothy Baldwin. 2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality. In Proceedings of the European Chapter of the
Association for Computational Linguistics.

Lauderdale, Benjamin E and Tom S Clark. 2014. “Scaling politically meaningful dimensions using texts

and votes.” American Journal of Political Science .

Lazer, David, Alex Sandy Pentland, Lada Adamic, Sinan Aral, Albert Laszlo Barabasi, Devon Brewer,
Nicholas Christakis, Noshir Contractor, James Fowler, Myron Gutmann et al. 2009. “Life in the
network: the coming age of computational social science.” Science (New York, NY) 323(5915):721.

Lim, Yew Jin and Yee Whye Teh. 2007. Variational Bayesian approach to movie rating prediction. In

Proceedings of KDD Cup and Workshop. Vol. 7 Citeseer pp. 15–21.

Lloyd, Stuart. 1982. “Least squares quantization in PCM.” Information Theory, IEEE Transactions on

28(2):129–137.

Mahajan, Meena, Prajakta Nimbhorkar and Kasturi Varadarajan. 2009. The planar k-means problem is

NP-hard. In WALCOM: Algorithms and Computation. Springer pp. 274–285.

McLachlan, Geoffrey and David Peel. 2004. Finite mixture models. John Wiley & Sons.

Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality. In Advances in Neural Information Processing
Systems. pp. 3111–3119.

Mimno, David and David Blei. 2011. Bayesian checking for topic models. In Proceedings of the Con-
ference on Empirical Methods in Natural Language Processing. Association for Computational Lin-
guistics pp. 227–237.

Mimno, David, Hanna M Wallach, Edmund Talley, Miriam Leenders and Andrew McCallum. 2011. Op-
timizing semantic coherence in topic models. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing. Association for Computational Linguistics pp. 262–272.

41

Mullainathan, Sendhil. 2014. “What Big Data Means for Social Science.” Behavioral and Experimental

Seminar.

Murphy, Kevin P. 2012. Machine learning: a probabilistic perspective. MIT press.

Nguyen, Thang, Yuening Hu and Jordan Boyd-Graber. 2014. Anchors Regularized: Adding Robustness
and Extensibility to Scalable Topic-Modeling Algorithms. In Association for Computational Linguis-
tics.

Nielsen, Frank and Richard Nock. 2014. “Further heuristics for k-means: The merge-and-split heuristic

and the (k, l)-means.” arXiv preprint arXiv:1406.6314 .

O’Connor, Brendan. 2014. MiTextExplorer: Linked brushing and mutual information for exploratory
text data analysis. In Proceedings of the Workshop on Interactive Language Learning, Visualization,
and Interfaces. Association for Computational Linguistics pp. 1–13.

OConnor, Brendan, Brandon M Stewart and Noah A Smith. 2013. Learning to Extract International

Relations from Political Context. In Proceedings of ACL.

Papadimitriou, Christos H and Kenneth Steiglitz. 1998. Combinatorial optimization: algorithms and

complexity. Courier Dover Publications.

Park, Jong Hee. 2012. “A Uniﬁed Method for Dynamic and Cross-Sectional Heterogeneity: Introducing

Hidden Markov Panel Models.” American Journal of Political Science 56(4):1040–1054.

Pearson, Karl. 1894. “Contributions to the mathematical theory of evolution.” Philosophical Transac-

tions of the Royal Society of London. A pp. 71–110.

Poole, Keith T and Howard Rosenthal. 1997. Congress: A political-economic history of roll call voting.

Oxford University Press.

Quinn, Kevin M, Burt L Monroe, Michael Colaresi, Michael H Crespin and Dragomir R Radev. 2010.
“How to analyze political attention with minimal assumptions and costs.” American Journal of Politi-
cal Science 54(1):209–228.

Rabiner, Lawrence and Biing-Hwang Juang. 1986. “An introduction to hidden Markov models.” ASSP

Magazine, IEEE 3(1):4–16.

Recht, Ben, Christopher Re, Joel Tropp and Victor Bittorf. 2012. Factoring nonnegative matrices with

linear programs. In Advances in Neural Information Processing Systems. pp. 1214–1222.

Reich, Justin, Dustin Tingley, Jetson Leder-Luis, Margaret E Roberts and Brandon M Stewart. N.d.
Computer Assisted Reading and Discovery for Student Generated Text. Technical report Working
Paper. Export.

Rivers, Douglas. 2003. “Identiﬁcation of multidimensional spatial voting models.” Typescript. Stanford

University .

Robert, Christian P and George Casella. 2004. Monte Carlo statistical methods. Vol. 319 Springer New

York.

42

Roberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis,
Shana Kushner Gadarian, Bethany Albertson and David G Rand. 2014. “Structural Topic Models
for Open-Ended Survey Responses.” American Journal of Political Science .

Ruiz, Francisco J. R., Isabel Valera, Carlos Blanco and Fernando Perez-Cruz. 2014. “Bayesian Non-
parametric Comorbidity Analysis of Psychiatric Disorders.” Journal of Machine Learning Research
15:1215–1247.
URL: http://jmlr.org/papers/v15/ruiz14a.html

Shneiderman, Ben. 1996. The eyes have it: A task by data type taxonomy for information visualizations.

In Visual Languages, 1996. Proceedings., IEEE Symposium on. IEEE pp. 336–343.

Song, Le, Eric P Xing and Ankur P Parikh. 2011. A spectral algorithm for latent tree graphical models. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11). pp. 1065–1072.

Sontag, David and Dan Roy. 2011. Complexity of inference in latent dirichlet allocation. In Advances in

Neural Information Processing Systems. pp. 1008–1016.

Srivastava, Virendera K and David EA Giles. 1987. Seemingly unrelated regression equations models:

estimation and inference. Vol. 80 CRC Press.

Teh, Yee Whye, Michael I Jordan, Matthew J Beal and David M Blei. 2006. “Hierarchical dirichlet

processes.” Journal of the american statistical association 101(476).

Tenenbaum, Joshua B, Charles Kemp, Thomas L Grifﬁths and Noah D Goodman. 2011. “How to grow

a mind: Statistics, structure, and abstraction.” science 331(6022):1279–1285.

Tibshirani, Robert. 1996. “Regression shrinkage and selection via the lasso.” Journal of the Royal Sta-

tistical Society. Series B (Methodological) pp. 267–288.

Vavasis, Stephen A. 2009. “On the complexity of nonnegative matrix factorization.” SIAM Journal on

Optimization 20(3):1364–1377.

Wallach, Hanna M, Iain Murray, Ruslan Salakhutdinov and David Mimno. 2009. Evaluation methods
for topic models. In Proceedings of the 26th Annual International Conference on Machine Learning.
ACM pp. 1105–1112.

Ward, Michael D, Brian D Greenhill and Kristin M Bakke. 2010. “The perils of policy by p-value:

Predicting civil conﬂicts.” Journal of Peace Research 47(4):363–375.

Wolpert, David H and William G Macready. 1997. “No free lunch theorems for optimization.” Evolu-

tionary Computation, IEEE Transactions on 1(1):67–82.

Yao, Limin, David Mimno and Andrew McCallum. 2009. Efﬁcient methods for topic model inference on
streaming document collections. In Proceedings of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining. ACM pp. 937–946.

Zhou, Tianyi, Jeff Bilmes and Carlos Guestrin. 2014. “Divide-and-Conquer Learning by Anchoring a

Conical Hull.” arXiv preprint arXiv:1406.5752 .

43

Figure 1: Five example topics from the reference model. These are given the labels Supreme Court,
Cheney, Global Warming, Iran/N.K. Nukes, and Wright respectively.

44

Topic 18:  law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,feder, requir, amend, protect, gun, govern, allow, appeal, citizenTopic 30:  presid, vice, cheney, offic, presidenti, first, execut, dick, decis, leader,role, histori, nation, branch, power, part, govern, order, idea, washingtonTopic 48:  global, warm, research, climat, studi, chang, scienc, scientist, gore, caus,human, scientif, earth, emiss, planet, cell, environment, report, water, greenTopic 60:  iran, nuclear, threat, weapon, iranian, program, missil, north, bomb, defens,korea, strike, sanction, intern, build, militari, intellig, capabl, pose,developTopic 71:  black, wright, white, race, church, racial, racist, pastor, jeremiah,africanamerican, racism, african, comment, reverend, king, controversi, rev,view, communiti, southFigure 2: Relation between three measures of topic similarity across all topics and modes. Plotted
surface is a kernel smoothed density estimate.

45

02468100.00.51.01.52.0L1 and Top WordsTop 10 Words in CommonL1 Distance02468100.00.51.01.52.0L1 and Top DocsTop 10 Docs in CommonL1 Distance02468100246810Words and DocsTop 10 Docs in CommonTop 10 Words in CommonNavigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Even in traditional general linear models, there is also always the choice of model speciﬁcation in both

variables and functional form. Although multimodality brings new issues to the table, the responsibility

of the researcher to carefully validate the chosen model is fundamentally the same. This is true regardless

of whether the choice between competing models arises due to a non-convex latent variable model or

due to the selection of an important model tuning parameter in a globally convex problem. Thus even if

multimodality is an unfamiliar problem, social scientists can draw on the same set of best practices that

they employ throughout their research.

An important practical contribution of this chapter is that it extends the set of tools available to

scholars using topic models in applied research. While we have focused on STM, many of the procedures

we use will be helpful for a broader class of latent variable models. For instance, the approaches to

aligning topics and calculating stability across runs can all be applied directly to the broader class of

statistical topic models and with minor modiﬁcations to most latent variable models.

We see great potential for the analysis of “big” data in the social sciences, but rather than focus on

the data we have taken a more methodological focus. We think this has important implications for both

methodological development but also could structure the types of questions we ask, and the types of data

sets we seek to build. Methodologically, we think that there will be important advances in areas such

as optimal initialization strategies, which is especially important as our data sets grow in size. From an

applied perspective, users will be unlikely to want to wait for extended periods of time in order to get

even a single set of results. Advances in computational power needs to be matched with smart ways to

leverage that power. From a research design perspective, we think more focus should be put on bringing

greater structure to so called “unstructured” data. In the STM we focus on the inclusion of metadata

for modeling and hypothesis testing, but this is only one possible use. Can more direct supervision help

us with issues of multimodality? Of course, in the end, big data will be at its best when there is active

dialogue between those who pose the big question and those who might provide the big answers.

References
Airoldi, Edoardo M, David M Blei, Stephen E Fienberg and Eric P Xing. 2009. Mixed membership

stochastic blockmodels. In Advances in Neural Information Processing Systems. pp. 33–40.

Anandkumar, Anima, Rong Ge, Daniel Hsu and Sham M Kakade. 2013. “A tensor spectral approach to

learning mixed membership community models.” arXiv preprint arXiv:1302.2684 .

36

Anandkumar, Anima, Rong Ge, Daniel Hsu, Sham M Kakade and Matus Telgarsky. 2012. “Tensor

decompositions for learning latent variable models.” arXiv preprint arXiv:1210.7559 .

Anandkumar, Anima, Yi-kai Liu, Daniel J Hsu, Dean P Foster and Sham M Kakade. 2012. A spec-
tral algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems.
pp. 917–925.

Anandkumar, Animashree, Daniel Hsu, Adel Javanmard and Sham Kakade. 2013. Learning linear
bayesian networks with latent variables. In Proceedings of The 30th International Conference on Ma-
chine Learning. pp. 249–257.

Anandkumar, Animashree, Daniel Hsu and Sham M Kakade. 2012. “A method of moments for mixture

models and hidden Markov models.” arXiv preprint arXiv:1203.0683 .

Arora, Sanjeev, Aditya Bhaskara, Rong Ge and Tengyu Ma. 2013. “Provable bounds for learning some

deep representations.” arXiv preprint .

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2012. Learning topic models–going beyond svd. In Foun-

dations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE pp. 1–10.

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2013. “New algorithms for learning incoherent and over-

complete dictionaries.” arXiv preprint arXiv:1308.6273 .

Arora, Sanjeev, Rong Ge, Ankur Moitra and Sushant Sachdeva. 2012. Provable ICA with unknown
Gaussian noise, with implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems. pp. 2375–2383.

Arora, Sanjeev, Rong Ge, Ravindran Kannan and Ankur Moitra. 2012. Computing a nonnegative ma-
trix factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of
computing. ACM pp. 145–162.

Arora, Sanjeev, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu
and Michael Zhu. 2013. A Practical Algorithm for Topic Modeling with Provable Guarantees. In
Proceedings of The 30th International Conference on Machine Learning. pp. 280–288.

Arthur, David and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial
and Applied Mathematics pp. 1027–1035.

Asuncion Jr, Arthur Uy. 2011. Distributed and accelerated inference algorithms for probabilistic graph-

ical models. Technical report California State University at Long Beach.

Bahmani, Bahman, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei Vassilvitskii. 2012.

“Scalable k-means++.” Proceedings of the VLDB Endowment 5(7):622–633.

Beck, Nathaniel, Gary King and Langche Zeng. 2000. “Improving quantitative studies of international

conﬂict: A conjecture.” American Political Science Review pp. 21–35.

Belloni, Alexandre, Victor Chernozhukov and Christian Hansen. 2014. “High-Dimensional Methods and

Inference on Structural and Treatment Effects.” The Journal of Economic Perspectives 28(2):29–50.

37

Bishop, Christopher M et al. 2006. Pattern recognition and machine learning. Vol. 1 springer New York.

Blei, David M. 2012. “Probabilistic topic models.” Communications of the ACM 55(4):77–84.

Blei, David M. 2014. “Build, compute, critique, repeat: data analysis with latent variable models.”

Annual Review of Statistics and Its Application 1:203–232.

Blei, David M, Andrew Y Ng and Michael I Jordan. 2003. “Latent dirichlet allocation.” the Journal of

machine Learning research 3:993–1022.

Boyd, Stephen and Lieven Vandenberghe. 2009. Convex optimization. Cambridge university press.

Carpenter, Bob. 2010. Integrating out multinomial parameters in latent Dirichlet allocation and naive

Bayes for collapsed Gibbs sampling. Technical report Technical report, LingPipe.

Chaney, Allison June-Barlow and David M Blei. 2012. Visualizing Topic Models. In ICWSM.

Chang, Jonathan. 2012. lda: Collapsed Gibbs sampling methods for topic models. R package version

1.3.2.
URL: http://CRAN.R-project.org/package=lda

Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-graber and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In Advances in neural information processing systems.
pp. 288–296.

Chuang, Jason, Christopher D Manning and Jeffrey Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of the International Working Conference on Advanced
Visual Interfaces. ACM pp. 74–77.

Chuang, Jason, Sonal Gupta, Christopher Manning and Jeffrey Heer. 2013. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In Proceedings of the 30th International Conference
on Machine Learning (ICML-13). pp. 612–620.

Cochocki, A and Rolf Unbehauen. 1993. Neural networks for optimization and signal processing. John

Wiley & Sons, Inc.

Council, National Research. 2013. Frontiers in Massive Data Analysis. The National Academies Press.

de Klerk, Etienne and Dmitrii V Pasechnik. 2002. “Approximation of the stability number of a graph via

copositive programming.” SIAM Journal on Optimization 12(4):875–892.

De Marchi, Scott, Christopher Gelpi and Jeffrey D Grynaviski. 2004. “Untangling neural nets.” American

Political Science Review 98(02):371–378.

Dean, Jeffrey and Sanjay Ghemawat. 2008. “MapReduce: simpliﬁed data processing on large clusters.”

Communications of the ACM 51(1):107–113.

Deb, Partha and Pravin K Trivedi. 2002. “The structure of demand for health care: latent class versus

two-part models.” Journal of health economics 21(4):601–625.

Dempster, Arthur P, Nan M Laird and Donald B Rubin. 1977. “Maximum likelihood from incomplete
data via the EM algorithm.” Journal of the Royal Statistical Society. Series B (Methodological) pp. 1–
38.

38

Ding, Weicong, Mohammad H Rohban, Prakash Ishwar and Venkatesh Saligrama. 2013. “Topic discov-

ery through data dependent and random projections.” arXiv preprint arXiv:1303.3664 .

Ding, Weicong, Prakash Ishwar, Mohammad H Rohban and Venkatesh Saligrama. 2013. “Necessary
and Sufﬁcient Conditions for Novel Word Detection in Separable Topic Models.” arXiv preprint
arXiv:1310.7994 .

Donoho, David and Victoria Stodden. 2003. When does non-negative matrix factorization give a correct

decomposition into parts? In Advances in neural information processing systems. p. None.

Doshi-Velez, Finale, Yaorong Ge and Isaac Kohane. 2014. “Comorbidity clusters in autism spectrum

disorders: an electronic health record time-series analysis.” Pediatrics 133(1):e54–e63.

Drton, Mathias and Thomas S Richardson. 2004. “Multimodality of the likelihood in the bivariate

seemingly unrelated regressions model.” Biometrika 91(2):383–392.

DuMouchel, William. 1999. “Bayesian data mining in large frequency tables, with an application to the

FDA spontaneous reporting system.” The American Statistician 53(3):177–190.

Efron, Bradley et al. 1978. “The geometry of exponential families.” The Annals of Statistics 6(2):362–

376.

Eisenstein, Jacob and Eric Xing. 2010. “The CMU 2008 Political Blog Corpus.”.

Fan, Jianqing, Fang Han and Han Liu. 2014. “Challenges of Big Data analysis.” National Science Review

p. nwt032.

Foulds, J. R. and P. Smyth. 2014. Annealing Paths for the Evaluation of Topic Models. In Proceedings

of the Thirtieth Conference Conference on Uncertainty in Artiﬁcial Intelligence.

Gardner, Matthew J, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric Ringger and Kevin Seppi.
2010. The topic browser: An interactive tool for browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.

Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari and Donald B Rubin. 2013.

Bayesian data analysis. CRC press.

Gillis, Nicolas. 2014.

arXiv:1401.5226 .

“The why and how of nonnegative matrix factorization.” arXiv preprint

Gillis, Nicolas and Robert Luce. 2013. “Robust near-separable nonnegative matrix factorization using

linear optimization.” arXiv preprint arXiv:1302.4385 .

Goldstone, AndrewUnderwood et al. 2014. “The Quiet Transformations of Literary Studies: What

Thirteen Thousand Scholars Could Tell Us.”.

Grifﬁths, DMBTL and MIJJB Tenenbaum. 2004. “Hierarchical topic models and the nested Chinese

restaurant process.” Advances in neural information processing systems 16:17.

Grifﬁths, Thomas L and Mark Steyvers. 2004. “Finding scientiﬁc topics.” Proceedings of the National

academy of Sciences of the United States of America 101(Suppl 1):5228–5235.

39

Grimmer, Justin. 2010a. “A Bayesian hierarchical topic model for political texts: Measuring expressed

agendas in Senate press releases.” Political Analysis 18(1):1–35.

Grimmer, Justin. 2010b. “An introduction to Bayesian inference via variational approximations.” Politi-

cal Analysis p. mpq027.

Grimmer, Justin. 2013. Representational Style in Congress: What Legislators Say and why it Matters.

Cambridge University Press.

Grimmer, Justin and Brandon M Stewart. 2013. “Text as data: The promise and pitfalls of automatic

content analysis methods for political texts.” Political Analysis p. mps028.

Grimmer, Justin and Gary King. 2011. “General purpose computer-assisted clustering and conceptual-

ization.” Proceedings of the National Academy of Sciences 108(7):2643–2650.

Hambleton, Ronald K. 1991. Fundamentals of item response theory. Vol. 2 Sage publications.

Hamilton, James D. 1989. “A new approach to the economic analysis of nonstationary time series and

the business cycle.” Econometrica: Journal of the Econometric Society pp. 357–384.

Hoff, Peter D, Adrian E Raftery and Mark S Handcock. 2002. “Latent space approaches to social network

analysis.” Journal of the american Statistical association 97(460):1090–1098.

Hopkins, Daniel J and Gary King. 2010. “A method of automated nonparametric content analysis for

social science.” American Journal of Political Science 54(1):229–247.

Hornik, Kurt. 2005. “A clue for cluster ensembles.” Journal of Statistical Software 14(12).

Hsu, Daniel and Sham M Kakade. 2013. Learning mixtures of spherical gaussians: moment methods and
spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer
Science. ACM pp. 11–20.

Hu, Yuening, Jordan Boyd-Graber, Brianna Satinoff and Alison Smith. 2014. “Interactive topic model-

ing.” Machine Learning 95(3):423–469.

Huang, Furong, UN Niranjan, M Hakeem and Animashree Anandkumar. 2013. “Fast Detection of

Overlapping Communities via Online Tensor Methods.”.

Jordan, Michael I, Zoubin Ghahramani, Tommi S Jaakkola and Lawrence K Saul. 1998. An introduction

to variational methods for graphical models. Springer.

Kalai, Adam Tauman, Ankur Moitra and Gregory Valiant. 2012. “Disentangling gaussians.” Communi-

cations of the ACM 55(2):113–120.

Kass, Robert E and Adrian E Raftery. 1995. “Bayes factors.” Journal of the american statistical associ-

ation 90(430):773–795.

King, Gary. 1989. “Unifying political methodology.” New York: Cambridge .

King, Gary, Jennifer Pan and Margaret E Roberts. 2013. “How censorship in China allows government

criticism but silences collective expression.” American Political Science Review 107(02):326–343.

40

Kivinen, Jyrki and Manfred K Warmuth. 1997. “Exponentiated gradient versus gradient descent for

linear predictors.” Information and Computation 132(1):1–63.

Koltcov, Sergei, Olessia Koltsova and Sergey Nikolenko. 2014. Latent dirichlet allocation: stability and
applications to studies of user-generated content. In Proceedings of the 2014 ACM conference on Web
science. ACM pp. 161–165.

Krippendorff, Klaus. 2012. Content analysis: An introduction to its methodology. Sage.

Kuhn, Harold W. 1955. “The Hungarian method for the assignment problem.” Naval research logistics

quarterly 2(1-2):83–97.

Kumar, Abhishek, Vikas Sindhwani and Prabhanjan Kambadur. 2012. “Fast conical hull algorithms for

near-separable non-negative matrix factorization.” arXiv preprint arXiv:1210.1190 .

Lancichinetti, Andrea, M Irmak Sirer, Jane X Wang, Daniel Acuna, Konrad K¨ording and Lu´ıs A Nunes
Amaral. 2014. “A high-reproducibility and high-accuracy method for automated topic classiﬁcation.”
arXiv preprint arXiv:1402.0422 .

Lau, Jey Han, David Newman and Timothy Baldwin. 2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality. In Proceedings of the European Chapter of the
Association for Computational Linguistics.

Lauderdale, Benjamin E and Tom S Clark. 2014. “Scaling politically meaningful dimensions using texts

and votes.” American Journal of Political Science .

Lazer, David, Alex Sandy Pentland, Lada Adamic, Sinan Aral, Albert Laszlo Barabasi, Devon Brewer,
Nicholas Christakis, Noshir Contractor, James Fowler, Myron Gutmann et al. 2009. “Life in the
network: the coming age of computational social science.” Science (New York, NY) 323(5915):721.

Lim, Yew Jin and Yee Whye Teh. 2007. Variational Bayesian approach to movie rating prediction. In

Proceedings of KDD Cup and Workshop. Vol. 7 Citeseer pp. 15–21.

Lloyd, Stuart. 1982. “Least squares quantization in PCM.” Information Theory, IEEE Transactions on

28(2):129–137.

Mahajan, Meena, Prajakta Nimbhorkar and Kasturi Varadarajan. 2009. The planar k-means problem is

NP-hard. In WALCOM: Algorithms and Computation. Springer pp. 274–285.

McLachlan, Geoffrey and David Peel. 2004. Finite mixture models. John Wiley & Sons.

Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality. In Advances in Neural Information Processing
Systems. pp. 3111–3119.

Mimno, David and David Blei. 2011. Bayesian checking for topic models. In Proceedings of the Con-
ference on Empirical Methods in Natural Language Processing. Association for Computational Lin-
guistics pp. 227–237.

Mimno, David, Hanna M Wallach, Edmund Talley, Miriam Leenders and Andrew McCallum. 2011. Op-
timizing semantic coherence in topic models. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing. Association for Computational Linguistics pp. 262–272.

41

Mullainathan, Sendhil. 2014. “What Big Data Means for Social Science.” Behavioral and Experimental

Seminar.

Murphy, Kevin P. 2012. Machine learning: a probabilistic perspective. MIT press.

Nguyen, Thang, Yuening Hu and Jordan Boyd-Graber. 2014. Anchors Regularized: Adding Robustness
and Extensibility to Scalable Topic-Modeling Algorithms. In Association for Computational Linguis-
tics.

Nielsen, Frank and Richard Nock. 2014. “Further heuristics for k-means: The merge-and-split heuristic

and the (k, l)-means.” arXiv preprint arXiv:1406.6314 .

O’Connor, Brendan. 2014. MiTextExplorer: Linked brushing and mutual information for exploratory
text data analysis. In Proceedings of the Workshop on Interactive Language Learning, Visualization,
and Interfaces. Association for Computational Linguistics pp. 1–13.

OConnor, Brendan, Brandon M Stewart and Noah A Smith. 2013. Learning to Extract International

Relations from Political Context. In Proceedings of ACL.

Papadimitriou, Christos H and Kenneth Steiglitz. 1998. Combinatorial optimization: algorithms and

complexity. Courier Dover Publications.

Park, Jong Hee. 2012. “A Uniﬁed Method for Dynamic and Cross-Sectional Heterogeneity: Introducing

Hidden Markov Panel Models.” American Journal of Political Science 56(4):1040–1054.

Pearson, Karl. 1894. “Contributions to the mathematical theory of evolution.” Philosophical Transac-

tions of the Royal Society of London. A pp. 71–110.

Poole, Keith T and Howard Rosenthal. 1997. Congress: A political-economic history of roll call voting.

Oxford University Press.

Quinn, Kevin M, Burt L Monroe, Michael Colaresi, Michael H Crespin and Dragomir R Radev. 2010.
“How to analyze political attention with minimal assumptions and costs.” American Journal of Politi-
cal Science 54(1):209–228.

Rabiner, Lawrence and Biing-Hwang Juang. 1986. “An introduction to hidden Markov models.” ASSP

Magazine, IEEE 3(1):4–16.

Recht, Ben, Christopher Re, Joel Tropp and Victor Bittorf. 2012. Factoring nonnegative matrices with

linear programs. In Advances in Neural Information Processing Systems. pp. 1214–1222.

Reich, Justin, Dustin Tingley, Jetson Leder-Luis, Margaret E Roberts and Brandon M Stewart. N.d.
Computer Assisted Reading and Discovery for Student Generated Text. Technical report Working
Paper. Export.

Rivers, Douglas. 2003. “Identiﬁcation of multidimensional spatial voting models.” Typescript. Stanford

University .

Robert, Christian P and George Casella. 2004. Monte Carlo statistical methods. Vol. 319 Springer New

York.

42

Roberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis,
Shana Kushner Gadarian, Bethany Albertson and David G Rand. 2014. “Structural Topic Models
for Open-Ended Survey Responses.” American Journal of Political Science .

Ruiz, Francisco J. R., Isabel Valera, Carlos Blanco and Fernando Perez-Cruz. 2014. “Bayesian Non-
parametric Comorbidity Analysis of Psychiatric Disorders.” Journal of Machine Learning Research
15:1215–1247.
URL: http://jmlr.org/papers/v15/ruiz14a.html

Shneiderman, Ben. 1996. The eyes have it: A task by data type taxonomy for information visualizations.

In Visual Languages, 1996. Proceedings., IEEE Symposium on. IEEE pp. 336–343.

Song, Le, Eric P Xing and Ankur P Parikh. 2011. A spectral algorithm for latent tree graphical models. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11). pp. 1065–1072.

Sontag, David and Dan Roy. 2011. Complexity of inference in latent dirichlet allocation. In Advances in

Neural Information Processing Systems. pp. 1008–1016.

Srivastava, Virendera K and David EA Giles. 1987. Seemingly unrelated regression equations models:

estimation and inference. Vol. 80 CRC Press.

Teh, Yee Whye, Michael I Jordan, Matthew J Beal and David M Blei. 2006. “Hierarchical dirichlet

processes.” Journal of the american statistical association 101(476).

Tenenbaum, Joshua B, Charles Kemp, Thomas L Grifﬁths and Noah D Goodman. 2011. “How to grow

a mind: Statistics, structure, and abstraction.” science 331(6022):1279–1285.

Tibshirani, Robert. 1996. “Regression shrinkage and selection via the lasso.” Journal of the Royal Sta-

tistical Society. Series B (Methodological) pp. 267–288.

Vavasis, Stephen A. 2009. “On the complexity of nonnegative matrix factorization.” SIAM Journal on

Optimization 20(3):1364–1377.

Wallach, Hanna M, Iain Murray, Ruslan Salakhutdinov and David Mimno. 2009. Evaluation methods
for topic models. In Proceedings of the 26th Annual International Conference on Machine Learning.
ACM pp. 1105–1112.

Ward, Michael D, Brian D Greenhill and Kristin M Bakke. 2010. “The perils of policy by p-value:

Predicting civil conﬂicts.” Journal of Peace Research 47(4):363–375.

Wolpert, David H and William G Macready. 1997. “No free lunch theorems for optimization.” Evolu-

tionary Computation, IEEE Transactions on 1(1):67–82.

Yao, Limin, David Mimno and Andrew McCallum. 2009. Efﬁcient methods for topic model inference on
streaming document collections. In Proceedings of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining. ACM pp. 937–946.

Zhou, Tianyi, Jeff Bilmes and Carlos Guestrin. 2014. “Divide-and-Conquer Learning by Anchoring a

Conical Hull.” arXiv preprint arXiv:1406.5752 .

43

Figure 1: Five example topics from the reference model. These are given the labels Supreme Court,
Cheney, Global Warming, Iran/N.K. Nukes, and Wright respectively.

44

Topic 18:  law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,feder, requir, amend, protect, gun, govern, allow, appeal, citizenTopic 30:  presid, vice, cheney, offic, presidenti, first, execut, dick, decis, leader,role, histori, nation, branch, power, part, govern, order, idea, washingtonTopic 48:  global, warm, research, climat, studi, chang, scienc, scientist, gore, caus,human, scientif, earth, emiss, planet, cell, environment, report, water, greenTopic 60:  iran, nuclear, threat, weapon, iranian, program, missil, north, bomb, defens,korea, strike, sanction, intern, build, militari, intellig, capabl, pose,developTopic 71:  black, wright, white, race, church, racial, racist, pastor, jeremiah,africanamerican, racism, african, comment, reverend, king, controversi, rev,view, communiti, southFigure 2: Relation between three measures of topic similarity across all topics and modes. Plotted
surface is a kernel smoothed density estimate.

45

02468100.00.51.01.52.0L1 and Top WordsTop 10 Words in CommonL1 Distance02468100.00.51.01.52.0L1 and Top DocsTop 10 Docs in CommonL1 Distance02468100246810Words and DocsTop 10 Docs in CommonTop 10 Words in CommonFigure 3: Comparison between the approximation to the bound on the marginal likelihood (the objective
function) with similarity metrics aggregated to the model level.

46

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.40.50.60.70.80.91.0Bound and WordsBoundProportion of 10 Words in Commonlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.40.60.81.0Bound and DocsBoundProportion of 10 Docs in Commonlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.00.20.40.60.81.01.2Bound and L1BoundExpected L1 DistNavigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Even in traditional general linear models, there is also always the choice of model speciﬁcation in both

variables and functional form. Although multimodality brings new issues to the table, the responsibility

of the researcher to carefully validate the chosen model is fundamentally the same. This is true regardless

of whether the choice between competing models arises due to a non-convex latent variable model or

due to the selection of an important model tuning parameter in a globally convex problem. Thus even if

multimodality is an unfamiliar problem, social scientists can draw on the same set of best practices that

they employ throughout their research.

An important practical contribution of this chapter is that it extends the set of tools available to

scholars using topic models in applied research. While we have focused on STM, many of the procedures

we use will be helpful for a broader class of latent variable models. For instance, the approaches to

aligning topics and calculating stability across runs can all be applied directly to the broader class of

statistical topic models and with minor modiﬁcations to most latent variable models.

We see great potential for the analysis of “big” data in the social sciences, but rather than focus on

the data we have taken a more methodological focus. We think this has important implications for both

methodological development but also could structure the types of questions we ask, and the types of data

sets we seek to build. Methodologically, we think that there will be important advances in areas such

as optimal initialization strategies, which is especially important as our data sets grow in size. From an

applied perspective, users will be unlikely to want to wait for extended periods of time in order to get

even a single set of results. Advances in computational power needs to be matched with smart ways to

leverage that power. From a research design perspective, we think more focus should be put on bringing

greater structure to so called “unstructured” data. In the STM we focus on the inclusion of metadata

for modeling and hypothesis testing, but this is only one possible use. Can more direct supervision help

us with issues of multimodality? Of course, in the end, big data will be at its best when there is active

dialogue between those who pose the big question and those who might provide the big answers.

References
Airoldi, Edoardo M, David M Blei, Stephen E Fienberg and Eric P Xing. 2009. Mixed membership

stochastic blockmodels. In Advances in Neural Information Processing Systems. pp. 33–40.

Anandkumar, Anima, Rong Ge, Daniel Hsu and Sham M Kakade. 2013. “A tensor spectral approach to

learning mixed membership community models.” arXiv preprint arXiv:1302.2684 .

36

Anandkumar, Anima, Rong Ge, Daniel Hsu, Sham M Kakade and Matus Telgarsky. 2012. “Tensor

decompositions for learning latent variable models.” arXiv preprint arXiv:1210.7559 .

Anandkumar, Anima, Yi-kai Liu, Daniel J Hsu, Dean P Foster and Sham M Kakade. 2012. A spec-
tral algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems.
pp. 917–925.

Anandkumar, Animashree, Daniel Hsu, Adel Javanmard and Sham Kakade. 2013. Learning linear
bayesian networks with latent variables. In Proceedings of The 30th International Conference on Ma-
chine Learning. pp. 249–257.

Anandkumar, Animashree, Daniel Hsu and Sham M Kakade. 2012. “A method of moments for mixture

models and hidden Markov models.” arXiv preprint arXiv:1203.0683 .

Arora, Sanjeev, Aditya Bhaskara, Rong Ge and Tengyu Ma. 2013. “Provable bounds for learning some

deep representations.” arXiv preprint .

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2012. Learning topic models–going beyond svd. In Foun-

dations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE pp. 1–10.

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2013. “New algorithms for learning incoherent and over-

complete dictionaries.” arXiv preprint arXiv:1308.6273 .

Arora, Sanjeev, Rong Ge, Ankur Moitra and Sushant Sachdeva. 2012. Provable ICA with unknown
Gaussian noise, with implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems. pp. 2375–2383.

Arora, Sanjeev, Rong Ge, Ravindran Kannan and Ankur Moitra. 2012. Computing a nonnegative ma-
trix factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of
computing. ACM pp. 145–162.

Arora, Sanjeev, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu
and Michael Zhu. 2013. A Practical Algorithm for Topic Modeling with Provable Guarantees. In
Proceedings of The 30th International Conference on Machine Learning. pp. 280–288.

Arthur, David and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial
and Applied Mathematics pp. 1027–1035.

Asuncion Jr, Arthur Uy. 2011. Distributed and accelerated inference algorithms for probabilistic graph-

ical models. Technical report California State University at Long Beach.

Bahmani, Bahman, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei Vassilvitskii. 2012.

“Scalable k-means++.” Proceedings of the VLDB Endowment 5(7):622–633.

Beck, Nathaniel, Gary King and Langche Zeng. 2000. “Improving quantitative studies of international

conﬂict: A conjecture.” American Political Science Review pp. 21–35.

Belloni, Alexandre, Victor Chernozhukov and Christian Hansen. 2014. “High-Dimensional Methods and

Inference on Structural and Treatment Effects.” The Journal of Economic Perspectives 28(2):29–50.

37

Bishop, Christopher M et al. 2006. Pattern recognition and machine learning. Vol. 1 springer New York.

Blei, David M. 2012. “Probabilistic topic models.” Communications of the ACM 55(4):77–84.

Blei, David M. 2014. “Build, compute, critique, repeat: data analysis with latent variable models.”

Annual Review of Statistics and Its Application 1:203–232.

Blei, David M, Andrew Y Ng and Michael I Jordan. 2003. “Latent dirichlet allocation.” the Journal of

machine Learning research 3:993–1022.

Boyd, Stephen and Lieven Vandenberghe. 2009. Convex optimization. Cambridge university press.

Carpenter, Bob. 2010. Integrating out multinomial parameters in latent Dirichlet allocation and naive

Bayes for collapsed Gibbs sampling. Technical report Technical report, LingPipe.

Chaney, Allison June-Barlow and David M Blei. 2012. Visualizing Topic Models. In ICWSM.

Chang, Jonathan. 2012. lda: Collapsed Gibbs sampling methods for topic models. R package version

1.3.2.
URL: http://CRAN.R-project.org/package=lda

Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-graber and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In Advances in neural information processing systems.
pp. 288–296.

Chuang, Jason, Christopher D Manning and Jeffrey Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of the International Working Conference on Advanced
Visual Interfaces. ACM pp. 74–77.

Chuang, Jason, Sonal Gupta, Christopher Manning and Jeffrey Heer. 2013. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In Proceedings of the 30th International Conference
on Machine Learning (ICML-13). pp. 612–620.

Cochocki, A and Rolf Unbehauen. 1993. Neural networks for optimization and signal processing. John

Wiley & Sons, Inc.

Council, National Research. 2013. Frontiers in Massive Data Analysis. The National Academies Press.

de Klerk, Etienne and Dmitrii V Pasechnik. 2002. “Approximation of the stability number of a graph via

copositive programming.” SIAM Journal on Optimization 12(4):875–892.

De Marchi, Scott, Christopher Gelpi and Jeffrey D Grynaviski. 2004. “Untangling neural nets.” American

Political Science Review 98(02):371–378.

Dean, Jeffrey and Sanjay Ghemawat. 2008. “MapReduce: simpliﬁed data processing on large clusters.”

Communications of the ACM 51(1):107–113.

Deb, Partha and Pravin K Trivedi. 2002. “The structure of demand for health care: latent class versus

two-part models.” Journal of health economics 21(4):601–625.

Dempster, Arthur P, Nan M Laird and Donald B Rubin. 1977. “Maximum likelihood from incomplete
data via the EM algorithm.” Journal of the Royal Statistical Society. Series B (Methodological) pp. 1–
38.

38

Ding, Weicong, Mohammad H Rohban, Prakash Ishwar and Venkatesh Saligrama. 2013. “Topic discov-

ery through data dependent and random projections.” arXiv preprint arXiv:1303.3664 .

Ding, Weicong, Prakash Ishwar, Mohammad H Rohban and Venkatesh Saligrama. 2013. “Necessary
and Sufﬁcient Conditions for Novel Word Detection in Separable Topic Models.” arXiv preprint
arXiv:1310.7994 .

Donoho, David and Victoria Stodden. 2003. When does non-negative matrix factorization give a correct

decomposition into parts? In Advances in neural information processing systems. p. None.

Doshi-Velez, Finale, Yaorong Ge and Isaac Kohane. 2014. “Comorbidity clusters in autism spectrum

disorders: an electronic health record time-series analysis.” Pediatrics 133(1):e54–e63.

Drton, Mathias and Thomas S Richardson. 2004. “Multimodality of the likelihood in the bivariate

seemingly unrelated regressions model.” Biometrika 91(2):383–392.

DuMouchel, William. 1999. “Bayesian data mining in large frequency tables, with an application to the

FDA spontaneous reporting system.” The American Statistician 53(3):177–190.

Efron, Bradley et al. 1978. “The geometry of exponential families.” The Annals of Statistics 6(2):362–

376.

Eisenstein, Jacob and Eric Xing. 2010. “The CMU 2008 Political Blog Corpus.”.

Fan, Jianqing, Fang Han and Han Liu. 2014. “Challenges of Big Data analysis.” National Science Review

p. nwt032.

Foulds, J. R. and P. Smyth. 2014. Annealing Paths for the Evaluation of Topic Models. In Proceedings

of the Thirtieth Conference Conference on Uncertainty in Artiﬁcial Intelligence.

Gardner, Matthew J, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric Ringger and Kevin Seppi.
2010. The topic browser: An interactive tool for browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.

Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari and Donald B Rubin. 2013.

Bayesian data analysis. CRC press.

Gillis, Nicolas. 2014.

arXiv:1401.5226 .

“The why and how of nonnegative matrix factorization.” arXiv preprint

Gillis, Nicolas and Robert Luce. 2013. “Robust near-separable nonnegative matrix factorization using

linear optimization.” arXiv preprint arXiv:1302.4385 .

Goldstone, AndrewUnderwood et al. 2014. “The Quiet Transformations of Literary Studies: What

Thirteen Thousand Scholars Could Tell Us.”.

Grifﬁths, DMBTL and MIJJB Tenenbaum. 2004. “Hierarchical topic models and the nested Chinese

restaurant process.” Advances in neural information processing systems 16:17.

Grifﬁths, Thomas L and Mark Steyvers. 2004. “Finding scientiﬁc topics.” Proceedings of the National

academy of Sciences of the United States of America 101(Suppl 1):5228–5235.

39

Grimmer, Justin. 2010a. “A Bayesian hierarchical topic model for political texts: Measuring expressed

agendas in Senate press releases.” Political Analysis 18(1):1–35.

Grimmer, Justin. 2010b. “An introduction to Bayesian inference via variational approximations.” Politi-

cal Analysis p. mpq027.

Grimmer, Justin. 2013. Representational Style in Congress: What Legislators Say and why it Matters.

Cambridge University Press.

Grimmer, Justin and Brandon M Stewart. 2013. “Text as data: The promise and pitfalls of automatic

content analysis methods for political texts.” Political Analysis p. mps028.

Grimmer, Justin and Gary King. 2011. “General purpose computer-assisted clustering and conceptual-

ization.” Proceedings of the National Academy of Sciences 108(7):2643–2650.

Hambleton, Ronald K. 1991. Fundamentals of item response theory. Vol. 2 Sage publications.

Hamilton, James D. 1989. “A new approach to the economic analysis of nonstationary time series and

the business cycle.” Econometrica: Journal of the Econometric Society pp. 357–384.

Hoff, Peter D, Adrian E Raftery and Mark S Handcock. 2002. “Latent space approaches to social network

analysis.” Journal of the american Statistical association 97(460):1090–1098.

Hopkins, Daniel J and Gary King. 2010. “A method of automated nonparametric content analysis for

social science.” American Journal of Political Science 54(1):229–247.

Hornik, Kurt. 2005. “A clue for cluster ensembles.” Journal of Statistical Software 14(12).

Hsu, Daniel and Sham M Kakade. 2013. Learning mixtures of spherical gaussians: moment methods and
spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer
Science. ACM pp. 11–20.

Hu, Yuening, Jordan Boyd-Graber, Brianna Satinoff and Alison Smith. 2014. “Interactive topic model-

ing.” Machine Learning 95(3):423–469.

Huang, Furong, UN Niranjan, M Hakeem and Animashree Anandkumar. 2013. “Fast Detection of

Overlapping Communities via Online Tensor Methods.”.

Jordan, Michael I, Zoubin Ghahramani, Tommi S Jaakkola and Lawrence K Saul. 1998. An introduction

to variational methods for graphical models. Springer.

Kalai, Adam Tauman, Ankur Moitra and Gregory Valiant. 2012. “Disentangling gaussians.” Communi-

cations of the ACM 55(2):113–120.

Kass, Robert E and Adrian E Raftery. 1995. “Bayes factors.” Journal of the american statistical associ-

ation 90(430):773–795.

King, Gary. 1989. “Unifying political methodology.” New York: Cambridge .

King, Gary, Jennifer Pan and Margaret E Roberts. 2013. “How censorship in China allows government

criticism but silences collective expression.” American Political Science Review 107(02):326–343.

40

Kivinen, Jyrki and Manfred K Warmuth. 1997. “Exponentiated gradient versus gradient descent for

linear predictors.” Information and Computation 132(1):1–63.

Koltcov, Sergei, Olessia Koltsova and Sergey Nikolenko. 2014. Latent dirichlet allocation: stability and
applications to studies of user-generated content. In Proceedings of the 2014 ACM conference on Web
science. ACM pp. 161–165.

Krippendorff, Klaus. 2012. Content analysis: An introduction to its methodology. Sage.

Kuhn, Harold W. 1955. “The Hungarian method for the assignment problem.” Naval research logistics

quarterly 2(1-2):83–97.

Kumar, Abhishek, Vikas Sindhwani and Prabhanjan Kambadur. 2012. “Fast conical hull algorithms for

near-separable non-negative matrix factorization.” arXiv preprint arXiv:1210.1190 .

Lancichinetti, Andrea, M Irmak Sirer, Jane X Wang, Daniel Acuna, Konrad K¨ording and Lu´ıs A Nunes
Amaral. 2014. “A high-reproducibility and high-accuracy method for automated topic classiﬁcation.”
arXiv preprint arXiv:1402.0422 .

Lau, Jey Han, David Newman and Timothy Baldwin. 2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality. In Proceedings of the European Chapter of the
Association for Computational Linguistics.

Lauderdale, Benjamin E and Tom S Clark. 2014. “Scaling politically meaningful dimensions using texts

and votes.” American Journal of Political Science .

Lazer, David, Alex Sandy Pentland, Lada Adamic, Sinan Aral, Albert Laszlo Barabasi, Devon Brewer,
Nicholas Christakis, Noshir Contractor, James Fowler, Myron Gutmann et al. 2009. “Life in the
network: the coming age of computational social science.” Science (New York, NY) 323(5915):721.

Lim, Yew Jin and Yee Whye Teh. 2007. Variational Bayesian approach to movie rating prediction. In

Proceedings of KDD Cup and Workshop. Vol. 7 Citeseer pp. 15–21.

Lloyd, Stuart. 1982. “Least squares quantization in PCM.” Information Theory, IEEE Transactions on

28(2):129–137.

Mahajan, Meena, Prajakta Nimbhorkar and Kasturi Varadarajan. 2009. The planar k-means problem is

NP-hard. In WALCOM: Algorithms and Computation. Springer pp. 274–285.

McLachlan, Geoffrey and David Peel. 2004. Finite mixture models. John Wiley & Sons.

Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality. In Advances in Neural Information Processing
Systems. pp. 3111–3119.

Mimno, David and David Blei. 2011. Bayesian checking for topic models. In Proceedings of the Con-
ference on Empirical Methods in Natural Language Processing. Association for Computational Lin-
guistics pp. 227–237.

Mimno, David, Hanna M Wallach, Edmund Talley, Miriam Leenders and Andrew McCallum. 2011. Op-
timizing semantic coherence in topic models. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing. Association for Computational Linguistics pp. 262–272.

41

Mullainathan, Sendhil. 2014. “What Big Data Means for Social Science.” Behavioral and Experimental

Seminar.

Murphy, Kevin P. 2012. Machine learning: a probabilistic perspective. MIT press.

Nguyen, Thang, Yuening Hu and Jordan Boyd-Graber. 2014. Anchors Regularized: Adding Robustness
and Extensibility to Scalable Topic-Modeling Algorithms. In Association for Computational Linguis-
tics.

Nielsen, Frank and Richard Nock. 2014. “Further heuristics for k-means: The merge-and-split heuristic

and the (k, l)-means.” arXiv preprint arXiv:1406.6314 .

O’Connor, Brendan. 2014. MiTextExplorer: Linked brushing and mutual information for exploratory
text data analysis. In Proceedings of the Workshop on Interactive Language Learning, Visualization,
and Interfaces. Association for Computational Linguistics pp. 1–13.

OConnor, Brendan, Brandon M Stewart and Noah A Smith. 2013. Learning to Extract International

Relations from Political Context. In Proceedings of ACL.

Papadimitriou, Christos H and Kenneth Steiglitz. 1998. Combinatorial optimization: algorithms and

complexity. Courier Dover Publications.

Park, Jong Hee. 2012. “A Uniﬁed Method for Dynamic and Cross-Sectional Heterogeneity: Introducing

Hidden Markov Panel Models.” American Journal of Political Science 56(4):1040–1054.

Pearson, Karl. 1894. “Contributions to the mathematical theory of evolution.” Philosophical Transac-

tions of the Royal Society of London. A pp. 71–110.

Poole, Keith T and Howard Rosenthal. 1997. Congress: A political-economic history of roll call voting.

Oxford University Press.

Quinn, Kevin M, Burt L Monroe, Michael Colaresi, Michael H Crespin and Dragomir R Radev. 2010.
“How to analyze political attention with minimal assumptions and costs.” American Journal of Politi-
cal Science 54(1):209–228.

Rabiner, Lawrence and Biing-Hwang Juang. 1986. “An introduction to hidden Markov models.” ASSP

Magazine, IEEE 3(1):4–16.

Recht, Ben, Christopher Re, Joel Tropp and Victor Bittorf. 2012. Factoring nonnegative matrices with

linear programs. In Advances in Neural Information Processing Systems. pp. 1214–1222.

Reich, Justin, Dustin Tingley, Jetson Leder-Luis, Margaret E Roberts and Brandon M Stewart. N.d.
Computer Assisted Reading and Discovery for Student Generated Text. Technical report Working
Paper. Export.

Rivers, Douglas. 2003. “Identiﬁcation of multidimensional spatial voting models.” Typescript. Stanford

University .

Robert, Christian P and George Casella. 2004. Monte Carlo statistical methods. Vol. 319 Springer New

York.

42

Roberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis,
Shana Kushner Gadarian, Bethany Albertson and David G Rand. 2014. “Structural Topic Models
for Open-Ended Survey Responses.” American Journal of Political Science .

Ruiz, Francisco J. R., Isabel Valera, Carlos Blanco and Fernando Perez-Cruz. 2014. “Bayesian Non-
parametric Comorbidity Analysis of Psychiatric Disorders.” Journal of Machine Learning Research
15:1215–1247.
URL: http://jmlr.org/papers/v15/ruiz14a.html

Shneiderman, Ben. 1996. The eyes have it: A task by data type taxonomy for information visualizations.

In Visual Languages, 1996. Proceedings., IEEE Symposium on. IEEE pp. 336–343.

Song, Le, Eric P Xing and Ankur P Parikh. 2011. A spectral algorithm for latent tree graphical models. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11). pp. 1065–1072.

Sontag, David and Dan Roy. 2011. Complexity of inference in latent dirichlet allocation. In Advances in

Neural Information Processing Systems. pp. 1008–1016.

Srivastava, Virendera K and David EA Giles. 1987. Seemingly unrelated regression equations models:

estimation and inference. Vol. 80 CRC Press.

Teh, Yee Whye, Michael I Jordan, Matthew J Beal and David M Blei. 2006. “Hierarchical dirichlet

processes.” Journal of the american statistical association 101(476).

Tenenbaum, Joshua B, Charles Kemp, Thomas L Grifﬁths and Noah D Goodman. 2011. “How to grow

a mind: Statistics, structure, and abstraction.” science 331(6022):1279–1285.

Tibshirani, Robert. 1996. “Regression shrinkage and selection via the lasso.” Journal of the Royal Sta-

tistical Society. Series B (Methodological) pp. 267–288.

Vavasis, Stephen A. 2009. “On the complexity of nonnegative matrix factorization.” SIAM Journal on

Optimization 20(3):1364–1377.

Wallach, Hanna M, Iain Murray, Ruslan Salakhutdinov and David Mimno. 2009. Evaluation methods
for topic models. In Proceedings of the 26th Annual International Conference on Machine Learning.
ACM pp. 1105–1112.

Ward, Michael D, Brian D Greenhill and Kristin M Bakke. 2010. “The perils of policy by p-value:

Predicting civil conﬂicts.” Journal of Peace Research 47(4):363–375.

Wolpert, David H and William G Macready. 1997. “No free lunch theorems for optimization.” Evolu-

tionary Computation, IEEE Transactions on 1(1):67–82.

Yao, Limin, David Mimno and Andrew McCallum. 2009. Efﬁcient methods for topic model inference on
streaming document collections. In Proceedings of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining. ACM pp. 937–946.

Zhou, Tianyi, Jeff Bilmes and Carlos Guestrin. 2014. “Divide-and-Conquer Learning by Anchoring a

Conical Hull.” arXiv preprint arXiv:1406.5752 .

43

Figure 1: Five example topics from the reference model. These are given the labels Supreme Court,
Cheney, Global Warming, Iran/N.K. Nukes, and Wright respectively.

44

Topic 18:  law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,feder, requir, amend, protect, gun, govern, allow, appeal, citizenTopic 30:  presid, vice, cheney, offic, presidenti, first, execut, dick, decis, leader,role, histori, nation, branch, power, part, govern, order, idea, washingtonTopic 48:  global, warm, research, climat, studi, chang, scienc, scientist, gore, caus,human, scientif, earth, emiss, planet, cell, environment, report, water, greenTopic 60:  iran, nuclear, threat, weapon, iranian, program, missil, north, bomb, defens,korea, strike, sanction, intern, build, militari, intellig, capabl, pose,developTopic 71:  black, wright, white, race, church, racial, racist, pastor, jeremiah,africanamerican, racism, african, comment, reverend, king, controversi, rev,view, communiti, southFigure 2: Relation between three measures of topic similarity across all topics and modes. Plotted
surface is a kernel smoothed density estimate.

45

02468100.00.51.01.52.0L1 and Top WordsTop 10 Words in CommonL1 Distance02468100.00.51.01.52.0L1 and Top DocsTop 10 Docs in CommonL1 Distance02468100246810Words and DocsTop 10 Docs in CommonTop 10 Words in CommonFigure 3: Comparison between the approximation to the bound on the marginal likelihood (the objective
function) with similarity metrics aggregated to the model level.

46

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.40.50.60.70.80.91.0Bound and WordsBoundProportion of 10 Words in Commonlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.40.60.81.0Bound and DocsBoundProportion of 10 Docs in Commonlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.00.20.40.60.81.01.2Bound and L1BoundExpected L1 DistFigure 4: Aggregation of similarity metrics to the topic level.

47

Word Expectations over RunsExpected Common WordsFrequency2468051015CourtCheneyClimateIran/N.K.WrightDoc Expectations over RunsExpected Common DocsFrequency0246802468101214CourtCheneyClimateIran/N.K.WrightL1 Expectations over RunsExpected DistFrequency0.51.01.50246810CourtCheneyClimateIran/N.K.WrightNavigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Even in traditional general linear models, there is also always the choice of model speciﬁcation in both

variables and functional form. Although multimodality brings new issues to the table, the responsibility

of the researcher to carefully validate the chosen model is fundamentally the same. This is true regardless

of whether the choice between competing models arises due to a non-convex latent variable model or

due to the selection of an important model tuning parameter in a globally convex problem. Thus even if

multimodality is an unfamiliar problem, social scientists can draw on the same set of best practices that

they employ throughout their research.

An important practical contribution of this chapter is that it extends the set of tools available to

scholars using topic models in applied research. While we have focused on STM, many of the procedures

we use will be helpful for a broader class of latent variable models. For instance, the approaches to

aligning topics and calculating stability across runs can all be applied directly to the broader class of

statistical topic models and with minor modiﬁcations to most latent variable models.

We see great potential for the analysis of “big” data in the social sciences, but rather than focus on

the data we have taken a more methodological focus. We think this has important implications for both

methodological development but also could structure the types of questions we ask, and the types of data

sets we seek to build. Methodologically, we think that there will be important advances in areas such

as optimal initialization strategies, which is especially important as our data sets grow in size. From an

applied perspective, users will be unlikely to want to wait for extended periods of time in order to get

even a single set of results. Advances in computational power needs to be matched with smart ways to

leverage that power. From a research design perspective, we think more focus should be put on bringing

greater structure to so called “unstructured” data. In the STM we focus on the inclusion of metadata

for modeling and hypothesis testing, but this is only one possible use. Can more direct supervision help

us with issues of multimodality? Of course, in the end, big data will be at its best when there is active

dialogue between those who pose the big question and those who might provide the big answers.

References
Airoldi, Edoardo M, David M Blei, Stephen E Fienberg and Eric P Xing. 2009. Mixed membership

stochastic blockmodels. In Advances in Neural Information Processing Systems. pp. 33–40.

Anandkumar, Anima, Rong Ge, Daniel Hsu and Sham M Kakade. 2013. “A tensor spectral approach to

learning mixed membership community models.” arXiv preprint arXiv:1302.2684 .

36

Anandkumar, Anima, Rong Ge, Daniel Hsu, Sham M Kakade and Matus Telgarsky. 2012. “Tensor

decompositions for learning latent variable models.” arXiv preprint arXiv:1210.7559 .

Anandkumar, Anima, Yi-kai Liu, Daniel J Hsu, Dean P Foster and Sham M Kakade. 2012. A spec-
tral algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems.
pp. 917–925.

Anandkumar, Animashree, Daniel Hsu, Adel Javanmard and Sham Kakade. 2013. Learning linear
bayesian networks with latent variables. In Proceedings of The 30th International Conference on Ma-
chine Learning. pp. 249–257.

Anandkumar, Animashree, Daniel Hsu and Sham M Kakade. 2012. “A method of moments for mixture

models and hidden Markov models.” arXiv preprint arXiv:1203.0683 .

Arora, Sanjeev, Aditya Bhaskara, Rong Ge and Tengyu Ma. 2013. “Provable bounds for learning some

deep representations.” arXiv preprint .

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2012. Learning topic models–going beyond svd. In Foun-

dations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE pp. 1–10.

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2013. “New algorithms for learning incoherent and over-

complete dictionaries.” arXiv preprint arXiv:1308.6273 .

Arora, Sanjeev, Rong Ge, Ankur Moitra and Sushant Sachdeva. 2012. Provable ICA with unknown
Gaussian noise, with implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems. pp. 2375–2383.

Arora, Sanjeev, Rong Ge, Ravindran Kannan and Ankur Moitra. 2012. Computing a nonnegative ma-
trix factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of
computing. ACM pp. 145–162.

Arora, Sanjeev, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu
and Michael Zhu. 2013. A Practical Algorithm for Topic Modeling with Provable Guarantees. In
Proceedings of The 30th International Conference on Machine Learning. pp. 280–288.

Arthur, David and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial
and Applied Mathematics pp. 1027–1035.

Asuncion Jr, Arthur Uy. 2011. Distributed and accelerated inference algorithms for probabilistic graph-

ical models. Technical report California State University at Long Beach.

Bahmani, Bahman, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei Vassilvitskii. 2012.

“Scalable k-means++.” Proceedings of the VLDB Endowment 5(7):622–633.

Beck, Nathaniel, Gary King and Langche Zeng. 2000. “Improving quantitative studies of international

conﬂict: A conjecture.” American Political Science Review pp. 21–35.

Belloni, Alexandre, Victor Chernozhukov and Christian Hansen. 2014. “High-Dimensional Methods and

Inference on Structural and Treatment Effects.” The Journal of Economic Perspectives 28(2):29–50.

37

Bishop, Christopher M et al. 2006. Pattern recognition and machine learning. Vol. 1 springer New York.

Blei, David M. 2012. “Probabilistic topic models.” Communications of the ACM 55(4):77–84.

Blei, David M. 2014. “Build, compute, critique, repeat: data analysis with latent variable models.”

Annual Review of Statistics and Its Application 1:203–232.

Blei, David M, Andrew Y Ng and Michael I Jordan. 2003. “Latent dirichlet allocation.” the Journal of

machine Learning research 3:993–1022.

Boyd, Stephen and Lieven Vandenberghe. 2009. Convex optimization. Cambridge university press.

Carpenter, Bob. 2010. Integrating out multinomial parameters in latent Dirichlet allocation and naive

Bayes for collapsed Gibbs sampling. Technical report Technical report, LingPipe.

Chaney, Allison June-Barlow and David M Blei. 2012. Visualizing Topic Models. In ICWSM.

Chang, Jonathan. 2012. lda: Collapsed Gibbs sampling methods for topic models. R package version

1.3.2.
URL: http://CRAN.R-project.org/package=lda

Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-graber and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In Advances in neural information processing systems.
pp. 288–296.

Chuang, Jason, Christopher D Manning and Jeffrey Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of the International Working Conference on Advanced
Visual Interfaces. ACM pp. 74–77.

Chuang, Jason, Sonal Gupta, Christopher Manning and Jeffrey Heer. 2013. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In Proceedings of the 30th International Conference
on Machine Learning (ICML-13). pp. 612–620.

Cochocki, A and Rolf Unbehauen. 1993. Neural networks for optimization and signal processing. John

Wiley & Sons, Inc.

Council, National Research. 2013. Frontiers in Massive Data Analysis. The National Academies Press.

de Klerk, Etienne and Dmitrii V Pasechnik. 2002. “Approximation of the stability number of a graph via

copositive programming.” SIAM Journal on Optimization 12(4):875–892.

De Marchi, Scott, Christopher Gelpi and Jeffrey D Grynaviski. 2004. “Untangling neural nets.” American

Political Science Review 98(02):371–378.

Dean, Jeffrey and Sanjay Ghemawat. 2008. “MapReduce: simpliﬁed data processing on large clusters.”

Communications of the ACM 51(1):107–113.

Deb, Partha and Pravin K Trivedi. 2002. “The structure of demand for health care: latent class versus

two-part models.” Journal of health economics 21(4):601–625.

Dempster, Arthur P, Nan M Laird and Donald B Rubin. 1977. “Maximum likelihood from incomplete
data via the EM algorithm.” Journal of the Royal Statistical Society. Series B (Methodological) pp. 1–
38.

38

Ding, Weicong, Mohammad H Rohban, Prakash Ishwar and Venkatesh Saligrama. 2013. “Topic discov-

ery through data dependent and random projections.” arXiv preprint arXiv:1303.3664 .

Ding, Weicong, Prakash Ishwar, Mohammad H Rohban and Venkatesh Saligrama. 2013. “Necessary
and Sufﬁcient Conditions for Novel Word Detection in Separable Topic Models.” arXiv preprint
arXiv:1310.7994 .

Donoho, David and Victoria Stodden. 2003. When does non-negative matrix factorization give a correct

decomposition into parts? In Advances in neural information processing systems. p. None.

Doshi-Velez, Finale, Yaorong Ge and Isaac Kohane. 2014. “Comorbidity clusters in autism spectrum

disorders: an electronic health record time-series analysis.” Pediatrics 133(1):e54–e63.

Drton, Mathias and Thomas S Richardson. 2004. “Multimodality of the likelihood in the bivariate

seemingly unrelated regressions model.” Biometrika 91(2):383–392.

DuMouchel, William. 1999. “Bayesian data mining in large frequency tables, with an application to the

FDA spontaneous reporting system.” The American Statistician 53(3):177–190.

Efron, Bradley et al. 1978. “The geometry of exponential families.” The Annals of Statistics 6(2):362–

376.

Eisenstein, Jacob and Eric Xing. 2010. “The CMU 2008 Political Blog Corpus.”.

Fan, Jianqing, Fang Han and Han Liu. 2014. “Challenges of Big Data analysis.” National Science Review

p. nwt032.

Foulds, J. R. and P. Smyth. 2014. Annealing Paths for the Evaluation of Topic Models. In Proceedings

of the Thirtieth Conference Conference on Uncertainty in Artiﬁcial Intelligence.

Gardner, Matthew J, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric Ringger and Kevin Seppi.
2010. The topic browser: An interactive tool for browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.

Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari and Donald B Rubin. 2013.

Bayesian data analysis. CRC press.

Gillis, Nicolas. 2014.

arXiv:1401.5226 .

“The why and how of nonnegative matrix factorization.” arXiv preprint

Gillis, Nicolas and Robert Luce. 2013. “Robust near-separable nonnegative matrix factorization using

linear optimization.” arXiv preprint arXiv:1302.4385 .

Goldstone, AndrewUnderwood et al. 2014. “The Quiet Transformations of Literary Studies: What

Thirteen Thousand Scholars Could Tell Us.”.

Grifﬁths, DMBTL and MIJJB Tenenbaum. 2004. “Hierarchical topic models and the nested Chinese

restaurant process.” Advances in neural information processing systems 16:17.

Grifﬁths, Thomas L and Mark Steyvers. 2004. “Finding scientiﬁc topics.” Proceedings of the National

academy of Sciences of the United States of America 101(Suppl 1):5228–5235.

39

Grimmer, Justin. 2010a. “A Bayesian hierarchical topic model for political texts: Measuring expressed

agendas in Senate press releases.” Political Analysis 18(1):1–35.

Grimmer, Justin. 2010b. “An introduction to Bayesian inference via variational approximations.” Politi-

cal Analysis p. mpq027.

Grimmer, Justin. 2013. Representational Style in Congress: What Legislators Say and why it Matters.

Cambridge University Press.

Grimmer, Justin and Brandon M Stewart. 2013. “Text as data: The promise and pitfalls of automatic

content analysis methods for political texts.” Political Analysis p. mps028.

Grimmer, Justin and Gary King. 2011. “General purpose computer-assisted clustering and conceptual-

ization.” Proceedings of the National Academy of Sciences 108(7):2643–2650.

Hambleton, Ronald K. 1991. Fundamentals of item response theory. Vol. 2 Sage publications.

Hamilton, James D. 1989. “A new approach to the economic analysis of nonstationary time series and

the business cycle.” Econometrica: Journal of the Econometric Society pp. 357–384.

Hoff, Peter D, Adrian E Raftery and Mark S Handcock. 2002. “Latent space approaches to social network

analysis.” Journal of the american Statistical association 97(460):1090–1098.

Hopkins, Daniel J and Gary King. 2010. “A method of automated nonparametric content analysis for

social science.” American Journal of Political Science 54(1):229–247.

Hornik, Kurt. 2005. “A clue for cluster ensembles.” Journal of Statistical Software 14(12).

Hsu, Daniel and Sham M Kakade. 2013. Learning mixtures of spherical gaussians: moment methods and
spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer
Science. ACM pp. 11–20.

Hu, Yuening, Jordan Boyd-Graber, Brianna Satinoff and Alison Smith. 2014. “Interactive topic model-

ing.” Machine Learning 95(3):423–469.

Huang, Furong, UN Niranjan, M Hakeem and Animashree Anandkumar. 2013. “Fast Detection of

Overlapping Communities via Online Tensor Methods.”.

Jordan, Michael I, Zoubin Ghahramani, Tommi S Jaakkola and Lawrence K Saul. 1998. An introduction

to variational methods for graphical models. Springer.

Kalai, Adam Tauman, Ankur Moitra and Gregory Valiant. 2012. “Disentangling gaussians.” Communi-

cations of the ACM 55(2):113–120.

Kass, Robert E and Adrian E Raftery. 1995. “Bayes factors.” Journal of the american statistical associ-

ation 90(430):773–795.

King, Gary. 1989. “Unifying political methodology.” New York: Cambridge .

King, Gary, Jennifer Pan and Margaret E Roberts. 2013. “How censorship in China allows government

criticism but silences collective expression.” American Political Science Review 107(02):326–343.

40

Kivinen, Jyrki and Manfred K Warmuth. 1997. “Exponentiated gradient versus gradient descent for

linear predictors.” Information and Computation 132(1):1–63.

Koltcov, Sergei, Olessia Koltsova and Sergey Nikolenko. 2014. Latent dirichlet allocation: stability and
applications to studies of user-generated content. In Proceedings of the 2014 ACM conference on Web
science. ACM pp. 161–165.

Krippendorff, Klaus. 2012. Content analysis: An introduction to its methodology. Sage.

Kuhn, Harold W. 1955. “The Hungarian method for the assignment problem.” Naval research logistics

quarterly 2(1-2):83–97.

Kumar, Abhishek, Vikas Sindhwani and Prabhanjan Kambadur. 2012. “Fast conical hull algorithms for

near-separable non-negative matrix factorization.” arXiv preprint arXiv:1210.1190 .

Lancichinetti, Andrea, M Irmak Sirer, Jane X Wang, Daniel Acuna, Konrad K¨ording and Lu´ıs A Nunes
Amaral. 2014. “A high-reproducibility and high-accuracy method for automated topic classiﬁcation.”
arXiv preprint arXiv:1402.0422 .

Lau, Jey Han, David Newman and Timothy Baldwin. 2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality. In Proceedings of the European Chapter of the
Association for Computational Linguistics.

Lauderdale, Benjamin E and Tom S Clark. 2014. “Scaling politically meaningful dimensions using texts

and votes.” American Journal of Political Science .

Lazer, David, Alex Sandy Pentland, Lada Adamic, Sinan Aral, Albert Laszlo Barabasi, Devon Brewer,
Nicholas Christakis, Noshir Contractor, James Fowler, Myron Gutmann et al. 2009. “Life in the
network: the coming age of computational social science.” Science (New York, NY) 323(5915):721.

Lim, Yew Jin and Yee Whye Teh. 2007. Variational Bayesian approach to movie rating prediction. In

Proceedings of KDD Cup and Workshop. Vol. 7 Citeseer pp. 15–21.

Lloyd, Stuart. 1982. “Least squares quantization in PCM.” Information Theory, IEEE Transactions on

28(2):129–137.

Mahajan, Meena, Prajakta Nimbhorkar and Kasturi Varadarajan. 2009. The planar k-means problem is

NP-hard. In WALCOM: Algorithms and Computation. Springer pp. 274–285.

McLachlan, Geoffrey and David Peel. 2004. Finite mixture models. John Wiley & Sons.

Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality. In Advances in Neural Information Processing
Systems. pp. 3111–3119.

Mimno, David and David Blei. 2011. Bayesian checking for topic models. In Proceedings of the Con-
ference on Empirical Methods in Natural Language Processing. Association for Computational Lin-
guistics pp. 227–237.

Mimno, David, Hanna M Wallach, Edmund Talley, Miriam Leenders and Andrew McCallum. 2011. Op-
timizing semantic coherence in topic models. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing. Association for Computational Linguistics pp. 262–272.

41

Mullainathan, Sendhil. 2014. “What Big Data Means for Social Science.” Behavioral and Experimental

Seminar.

Murphy, Kevin P. 2012. Machine learning: a probabilistic perspective. MIT press.

Nguyen, Thang, Yuening Hu and Jordan Boyd-Graber. 2014. Anchors Regularized: Adding Robustness
and Extensibility to Scalable Topic-Modeling Algorithms. In Association for Computational Linguis-
tics.

Nielsen, Frank and Richard Nock. 2014. “Further heuristics for k-means: The merge-and-split heuristic

and the (k, l)-means.” arXiv preprint arXiv:1406.6314 .

O’Connor, Brendan. 2014. MiTextExplorer: Linked brushing and mutual information for exploratory
text data analysis. In Proceedings of the Workshop on Interactive Language Learning, Visualization,
and Interfaces. Association for Computational Linguistics pp. 1–13.

OConnor, Brendan, Brandon M Stewart and Noah A Smith. 2013. Learning to Extract International

Relations from Political Context. In Proceedings of ACL.

Papadimitriou, Christos H and Kenneth Steiglitz. 1998. Combinatorial optimization: algorithms and

complexity. Courier Dover Publications.

Park, Jong Hee. 2012. “A Uniﬁed Method for Dynamic and Cross-Sectional Heterogeneity: Introducing

Hidden Markov Panel Models.” American Journal of Political Science 56(4):1040–1054.

Pearson, Karl. 1894. “Contributions to the mathematical theory of evolution.” Philosophical Transac-

tions of the Royal Society of London. A pp. 71–110.

Poole, Keith T and Howard Rosenthal. 1997. Congress: A political-economic history of roll call voting.

Oxford University Press.

Quinn, Kevin M, Burt L Monroe, Michael Colaresi, Michael H Crespin and Dragomir R Radev. 2010.
“How to analyze political attention with minimal assumptions and costs.” American Journal of Politi-
cal Science 54(1):209–228.

Rabiner, Lawrence and Biing-Hwang Juang. 1986. “An introduction to hidden Markov models.” ASSP

Magazine, IEEE 3(1):4–16.

Recht, Ben, Christopher Re, Joel Tropp and Victor Bittorf. 2012. Factoring nonnegative matrices with

linear programs. In Advances in Neural Information Processing Systems. pp. 1214–1222.

Reich, Justin, Dustin Tingley, Jetson Leder-Luis, Margaret E Roberts and Brandon M Stewart. N.d.
Computer Assisted Reading and Discovery for Student Generated Text. Technical report Working
Paper. Export.

Rivers, Douglas. 2003. “Identiﬁcation of multidimensional spatial voting models.” Typescript. Stanford

University .

Robert, Christian P and George Casella. 2004. Monte Carlo statistical methods. Vol. 319 Springer New

York.

42

Roberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis,
Shana Kushner Gadarian, Bethany Albertson and David G Rand. 2014. “Structural Topic Models
for Open-Ended Survey Responses.” American Journal of Political Science .

Ruiz, Francisco J. R., Isabel Valera, Carlos Blanco and Fernando Perez-Cruz. 2014. “Bayesian Non-
parametric Comorbidity Analysis of Psychiatric Disorders.” Journal of Machine Learning Research
15:1215–1247.
URL: http://jmlr.org/papers/v15/ruiz14a.html

Shneiderman, Ben. 1996. The eyes have it: A task by data type taxonomy for information visualizations.

In Visual Languages, 1996. Proceedings., IEEE Symposium on. IEEE pp. 336–343.

Song, Le, Eric P Xing and Ankur P Parikh. 2011. A spectral algorithm for latent tree graphical models. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11). pp. 1065–1072.

Sontag, David and Dan Roy. 2011. Complexity of inference in latent dirichlet allocation. In Advances in

Neural Information Processing Systems. pp. 1008–1016.

Srivastava, Virendera K and David EA Giles. 1987. Seemingly unrelated regression equations models:

estimation and inference. Vol. 80 CRC Press.

Teh, Yee Whye, Michael I Jordan, Matthew J Beal and David M Blei. 2006. “Hierarchical dirichlet

processes.” Journal of the american statistical association 101(476).

Tenenbaum, Joshua B, Charles Kemp, Thomas L Grifﬁths and Noah D Goodman. 2011. “How to grow

a mind: Statistics, structure, and abstraction.” science 331(6022):1279–1285.

Tibshirani, Robert. 1996. “Regression shrinkage and selection via the lasso.” Journal of the Royal Sta-

tistical Society. Series B (Methodological) pp. 267–288.

Vavasis, Stephen A. 2009. “On the complexity of nonnegative matrix factorization.” SIAM Journal on

Optimization 20(3):1364–1377.

Wallach, Hanna M, Iain Murray, Ruslan Salakhutdinov and David Mimno. 2009. Evaluation methods
for topic models. In Proceedings of the 26th Annual International Conference on Machine Learning.
ACM pp. 1105–1112.

Ward, Michael D, Brian D Greenhill and Kristin M Bakke. 2010. “The perils of policy by p-value:

Predicting civil conﬂicts.” Journal of Peace Research 47(4):363–375.

Wolpert, David H and William G Macready. 1997. “No free lunch theorems for optimization.” Evolu-

tionary Computation, IEEE Transactions on 1(1):67–82.

Yao, Limin, David Mimno and Andrew McCallum. 2009. Efﬁcient methods for topic model inference on
streaming document collections. In Proceedings of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining. ACM pp. 937–946.

Zhou, Tianyi, Jeff Bilmes and Carlos Guestrin. 2014. “Divide-and-Conquer Learning by Anchoring a

Conical Hull.” arXiv preprint arXiv:1406.5752 .

43

Figure 1: Five example topics from the reference model. These are given the labels Supreme Court,
Cheney, Global Warming, Iran/N.K. Nukes, and Wright respectively.

44

Topic 18:  law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,feder, requir, amend, protect, gun, govern, allow, appeal, citizenTopic 30:  presid, vice, cheney, offic, presidenti, first, execut, dick, decis, leader,role, histori, nation, branch, power, part, govern, order, idea, washingtonTopic 48:  global, warm, research, climat, studi, chang, scienc, scientist, gore, caus,human, scientif, earth, emiss, planet, cell, environment, report, water, greenTopic 60:  iran, nuclear, threat, weapon, iranian, program, missil, north, bomb, defens,korea, strike, sanction, intern, build, militari, intellig, capabl, pose,developTopic 71:  black, wright, white, race, church, racial, racist, pastor, jeremiah,africanamerican, racism, african, comment, reverend, king, controversi, rev,view, communiti, southFigure 2: Relation between three measures of topic similarity across all topics and modes. Plotted
surface is a kernel smoothed density estimate.

45

02468100.00.51.01.52.0L1 and Top WordsTop 10 Words in CommonL1 Distance02468100.00.51.01.52.0L1 and Top DocsTop 10 Docs in CommonL1 Distance02468100246810Words and DocsTop 10 Docs in CommonTop 10 Words in CommonFigure 3: Comparison between the approximation to the bound on the marginal likelihood (the objective
function) with similarity metrics aggregated to the model level.

46

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.40.50.60.70.80.91.0Bound and WordsBoundProportion of 10 Words in Commonlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.40.60.81.0Bound and DocsBoundProportion of 10 Docs in Commonlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.00.20.40.60.81.01.2Bound and L1BoundExpected L1 DistFigure 4: Aggregation of similarity metrics to the topic level.

47

Word Expectations over RunsExpected Common WordsFrequency2468051015CourtCheneyClimateIran/N.K.WrightDoc Expectations over RunsExpected Common DocsFrequency0246802468101214CourtCheneyClimateIran/N.K.WrightL1 Expectations over RunsExpected DistFrequency0.51.01.50246810CourtCheneyClimateIran/N.K.WrightFigure 5: Differences in topical coverage by rating (controlling for time). Effects to the right of 0
indicate a topic more heavily used by Conservatives. Lines indicate 95% conﬁdence intervals using the
“global” approximation to measurement uncertainty (Roberts et al., 2014).

48

l−0.006−0.004−0.0020.0000.0020.0040.006Partisan Rating Effects by TopicExpected Difference in Topic Proportion (Conservative minus Liberal)lWrightlIran/North Korea NukelGlobal WarminglCheneylSupreme CourtNavigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Even in traditional general linear models, there is also always the choice of model speciﬁcation in both

variables and functional form. Although multimodality brings new issues to the table, the responsibility

of the researcher to carefully validate the chosen model is fundamentally the same. This is true regardless

of whether the choice between competing models arises due to a non-convex latent variable model or

due to the selection of an important model tuning parameter in a globally convex problem. Thus even if

multimodality is an unfamiliar problem, social scientists can draw on the same set of best practices that

they employ throughout their research.

An important practical contribution of this chapter is that it extends the set of tools available to

scholars using topic models in applied research. While we have focused on STM, many of the procedures

we use will be helpful for a broader class of latent variable models. For instance, the approaches to

aligning topics and calculating stability across runs can all be applied directly to the broader class of

statistical topic models and with minor modiﬁcations to most latent variable models.

We see great potential for the analysis of “big” data in the social sciences, but rather than focus on

the data we have taken a more methodological focus. We think this has important implications for both

methodological development but also could structure the types of questions we ask, and the types of data

sets we seek to build. Methodologically, we think that there will be important advances in areas such

as optimal initialization strategies, which is especially important as our data sets grow in size. From an

applied perspective, users will be unlikely to want to wait for extended periods of time in order to get

even a single set of results. Advances in computational power needs to be matched with smart ways to

leverage that power. From a research design perspective, we think more focus should be put on bringing

greater structure to so called “unstructured” data. In the STM we focus on the inclusion of metadata

for modeling and hypothesis testing, but this is only one possible use. Can more direct supervision help

us with issues of multimodality? Of course, in the end, big data will be at its best when there is active

dialogue between those who pose the big question and those who might provide the big answers.

References
Airoldi, Edoardo M, David M Blei, Stephen E Fienberg and Eric P Xing. 2009. Mixed membership

stochastic blockmodels. In Advances in Neural Information Processing Systems. pp. 33–40.

Anandkumar, Anima, Rong Ge, Daniel Hsu and Sham M Kakade. 2013. “A tensor spectral approach to

learning mixed membership community models.” arXiv preprint arXiv:1302.2684 .

36

Anandkumar, Anima, Rong Ge, Daniel Hsu, Sham M Kakade and Matus Telgarsky. 2012. “Tensor

decompositions for learning latent variable models.” arXiv preprint arXiv:1210.7559 .

Anandkumar, Anima, Yi-kai Liu, Daniel J Hsu, Dean P Foster and Sham M Kakade. 2012. A spec-
tral algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems.
pp. 917–925.

Anandkumar, Animashree, Daniel Hsu, Adel Javanmard and Sham Kakade. 2013. Learning linear
bayesian networks with latent variables. In Proceedings of The 30th International Conference on Ma-
chine Learning. pp. 249–257.

Anandkumar, Animashree, Daniel Hsu and Sham M Kakade. 2012. “A method of moments for mixture

models and hidden Markov models.” arXiv preprint arXiv:1203.0683 .

Arora, Sanjeev, Aditya Bhaskara, Rong Ge and Tengyu Ma. 2013. “Provable bounds for learning some

deep representations.” arXiv preprint .

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2012. Learning topic models–going beyond svd. In Foun-

dations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE pp. 1–10.

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2013. “New algorithms for learning incoherent and over-

complete dictionaries.” arXiv preprint arXiv:1308.6273 .

Arora, Sanjeev, Rong Ge, Ankur Moitra and Sushant Sachdeva. 2012. Provable ICA with unknown
Gaussian noise, with implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems. pp. 2375–2383.

Arora, Sanjeev, Rong Ge, Ravindran Kannan and Ankur Moitra. 2012. Computing a nonnegative ma-
trix factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of
computing. ACM pp. 145–162.

Arora, Sanjeev, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu
and Michael Zhu. 2013. A Practical Algorithm for Topic Modeling with Provable Guarantees. In
Proceedings of The 30th International Conference on Machine Learning. pp. 280–288.

Arthur, David and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial
and Applied Mathematics pp. 1027–1035.

Asuncion Jr, Arthur Uy. 2011. Distributed and accelerated inference algorithms for probabilistic graph-

ical models. Technical report California State University at Long Beach.

Bahmani, Bahman, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei Vassilvitskii. 2012.

“Scalable k-means++.” Proceedings of the VLDB Endowment 5(7):622–633.

Beck, Nathaniel, Gary King and Langche Zeng. 2000. “Improving quantitative studies of international

conﬂict: A conjecture.” American Political Science Review pp. 21–35.

Belloni, Alexandre, Victor Chernozhukov and Christian Hansen. 2014. “High-Dimensional Methods and

Inference on Structural and Treatment Effects.” The Journal of Economic Perspectives 28(2):29–50.

37

Bishop, Christopher M et al. 2006. Pattern recognition and machine learning. Vol. 1 springer New York.

Blei, David M. 2012. “Probabilistic topic models.” Communications of the ACM 55(4):77–84.

Blei, David M. 2014. “Build, compute, critique, repeat: data analysis with latent variable models.”

Annual Review of Statistics and Its Application 1:203–232.

Blei, David M, Andrew Y Ng and Michael I Jordan. 2003. “Latent dirichlet allocation.” the Journal of

machine Learning research 3:993–1022.

Boyd, Stephen and Lieven Vandenberghe. 2009. Convex optimization. Cambridge university press.

Carpenter, Bob. 2010. Integrating out multinomial parameters in latent Dirichlet allocation and naive

Bayes for collapsed Gibbs sampling. Technical report Technical report, LingPipe.

Chaney, Allison June-Barlow and David M Blei. 2012. Visualizing Topic Models. In ICWSM.

Chang, Jonathan. 2012. lda: Collapsed Gibbs sampling methods for topic models. R package version

1.3.2.
URL: http://CRAN.R-project.org/package=lda

Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-graber and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In Advances in neural information processing systems.
pp. 288–296.

Chuang, Jason, Christopher D Manning and Jeffrey Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of the International Working Conference on Advanced
Visual Interfaces. ACM pp. 74–77.

Chuang, Jason, Sonal Gupta, Christopher Manning and Jeffrey Heer. 2013. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In Proceedings of the 30th International Conference
on Machine Learning (ICML-13). pp. 612–620.

Cochocki, A and Rolf Unbehauen. 1993. Neural networks for optimization and signal processing. John

Wiley & Sons, Inc.

Council, National Research. 2013. Frontiers in Massive Data Analysis. The National Academies Press.

de Klerk, Etienne and Dmitrii V Pasechnik. 2002. “Approximation of the stability number of a graph via

copositive programming.” SIAM Journal on Optimization 12(4):875–892.

De Marchi, Scott, Christopher Gelpi and Jeffrey D Grynaviski. 2004. “Untangling neural nets.” American

Political Science Review 98(02):371–378.

Dean, Jeffrey and Sanjay Ghemawat. 2008. “MapReduce: simpliﬁed data processing on large clusters.”

Communications of the ACM 51(1):107–113.

Deb, Partha and Pravin K Trivedi. 2002. “The structure of demand for health care: latent class versus

two-part models.” Journal of health economics 21(4):601–625.

Dempster, Arthur P, Nan M Laird and Donald B Rubin. 1977. “Maximum likelihood from incomplete
data via the EM algorithm.” Journal of the Royal Statistical Society. Series B (Methodological) pp. 1–
38.

38

Ding, Weicong, Mohammad H Rohban, Prakash Ishwar and Venkatesh Saligrama. 2013. “Topic discov-

ery through data dependent and random projections.” arXiv preprint arXiv:1303.3664 .

Ding, Weicong, Prakash Ishwar, Mohammad H Rohban and Venkatesh Saligrama. 2013. “Necessary
and Sufﬁcient Conditions for Novel Word Detection in Separable Topic Models.” arXiv preprint
arXiv:1310.7994 .

Donoho, David and Victoria Stodden. 2003. When does non-negative matrix factorization give a correct

decomposition into parts? In Advances in neural information processing systems. p. None.

Doshi-Velez, Finale, Yaorong Ge and Isaac Kohane. 2014. “Comorbidity clusters in autism spectrum

disorders: an electronic health record time-series analysis.” Pediatrics 133(1):e54–e63.

Drton, Mathias and Thomas S Richardson. 2004. “Multimodality of the likelihood in the bivariate

seemingly unrelated regressions model.” Biometrika 91(2):383–392.

DuMouchel, William. 1999. “Bayesian data mining in large frequency tables, with an application to the

FDA spontaneous reporting system.” The American Statistician 53(3):177–190.

Efron, Bradley et al. 1978. “The geometry of exponential families.” The Annals of Statistics 6(2):362–

376.

Eisenstein, Jacob and Eric Xing. 2010. “The CMU 2008 Political Blog Corpus.”.

Fan, Jianqing, Fang Han and Han Liu. 2014. “Challenges of Big Data analysis.” National Science Review

p. nwt032.

Foulds, J. R. and P. Smyth. 2014. Annealing Paths for the Evaluation of Topic Models. In Proceedings

of the Thirtieth Conference Conference on Uncertainty in Artiﬁcial Intelligence.

Gardner, Matthew J, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric Ringger and Kevin Seppi.
2010. The topic browser: An interactive tool for browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.

Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari and Donald B Rubin. 2013.

Bayesian data analysis. CRC press.

Gillis, Nicolas. 2014.

arXiv:1401.5226 .

“The why and how of nonnegative matrix factorization.” arXiv preprint

Gillis, Nicolas and Robert Luce. 2013. “Robust near-separable nonnegative matrix factorization using

linear optimization.” arXiv preprint arXiv:1302.4385 .

Goldstone, AndrewUnderwood et al. 2014. “The Quiet Transformations of Literary Studies: What

Thirteen Thousand Scholars Could Tell Us.”.

Grifﬁths, DMBTL and MIJJB Tenenbaum. 2004. “Hierarchical topic models and the nested Chinese

restaurant process.” Advances in neural information processing systems 16:17.

Grifﬁths, Thomas L and Mark Steyvers. 2004. “Finding scientiﬁc topics.” Proceedings of the National

academy of Sciences of the United States of America 101(Suppl 1):5228–5235.

39

Grimmer, Justin. 2010a. “A Bayesian hierarchical topic model for political texts: Measuring expressed

agendas in Senate press releases.” Political Analysis 18(1):1–35.

Grimmer, Justin. 2010b. “An introduction to Bayesian inference via variational approximations.” Politi-

cal Analysis p. mpq027.

Grimmer, Justin. 2013. Representational Style in Congress: What Legislators Say and why it Matters.

Cambridge University Press.

Grimmer, Justin and Brandon M Stewart. 2013. “Text as data: The promise and pitfalls of automatic

content analysis methods for political texts.” Political Analysis p. mps028.

Grimmer, Justin and Gary King. 2011. “General purpose computer-assisted clustering and conceptual-

ization.” Proceedings of the National Academy of Sciences 108(7):2643–2650.

Hambleton, Ronald K. 1991. Fundamentals of item response theory. Vol. 2 Sage publications.

Hamilton, James D. 1989. “A new approach to the economic analysis of nonstationary time series and

the business cycle.” Econometrica: Journal of the Econometric Society pp. 357–384.

Hoff, Peter D, Adrian E Raftery and Mark S Handcock. 2002. “Latent space approaches to social network

analysis.” Journal of the american Statistical association 97(460):1090–1098.

Hopkins, Daniel J and Gary King. 2010. “A method of automated nonparametric content analysis for

social science.” American Journal of Political Science 54(1):229–247.

Hornik, Kurt. 2005. “A clue for cluster ensembles.” Journal of Statistical Software 14(12).

Hsu, Daniel and Sham M Kakade. 2013. Learning mixtures of spherical gaussians: moment methods and
spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer
Science. ACM pp. 11–20.

Hu, Yuening, Jordan Boyd-Graber, Brianna Satinoff and Alison Smith. 2014. “Interactive topic model-

ing.” Machine Learning 95(3):423–469.

Huang, Furong, UN Niranjan, M Hakeem and Animashree Anandkumar. 2013. “Fast Detection of

Overlapping Communities via Online Tensor Methods.”.

Jordan, Michael I, Zoubin Ghahramani, Tommi S Jaakkola and Lawrence K Saul. 1998. An introduction

to variational methods for graphical models. Springer.

Kalai, Adam Tauman, Ankur Moitra and Gregory Valiant. 2012. “Disentangling gaussians.” Communi-

cations of the ACM 55(2):113–120.

Kass, Robert E and Adrian E Raftery. 1995. “Bayes factors.” Journal of the american statistical associ-

ation 90(430):773–795.

King, Gary. 1989. “Unifying political methodology.” New York: Cambridge .

King, Gary, Jennifer Pan and Margaret E Roberts. 2013. “How censorship in China allows government

criticism but silences collective expression.” American Political Science Review 107(02):326–343.

40

Kivinen, Jyrki and Manfred K Warmuth. 1997. “Exponentiated gradient versus gradient descent for

linear predictors.” Information and Computation 132(1):1–63.

Koltcov, Sergei, Olessia Koltsova and Sergey Nikolenko. 2014. Latent dirichlet allocation: stability and
applications to studies of user-generated content. In Proceedings of the 2014 ACM conference on Web
science. ACM pp. 161–165.

Krippendorff, Klaus. 2012. Content analysis: An introduction to its methodology. Sage.

Kuhn, Harold W. 1955. “The Hungarian method for the assignment problem.” Naval research logistics

quarterly 2(1-2):83–97.

Kumar, Abhishek, Vikas Sindhwani and Prabhanjan Kambadur. 2012. “Fast conical hull algorithms for

near-separable non-negative matrix factorization.” arXiv preprint arXiv:1210.1190 .

Lancichinetti, Andrea, M Irmak Sirer, Jane X Wang, Daniel Acuna, Konrad K¨ording and Lu´ıs A Nunes
Amaral. 2014. “A high-reproducibility and high-accuracy method for automated topic classiﬁcation.”
arXiv preprint arXiv:1402.0422 .

Lau, Jey Han, David Newman and Timothy Baldwin. 2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality. In Proceedings of the European Chapter of the
Association for Computational Linguistics.

Lauderdale, Benjamin E and Tom S Clark. 2014. “Scaling politically meaningful dimensions using texts

and votes.” American Journal of Political Science .

Lazer, David, Alex Sandy Pentland, Lada Adamic, Sinan Aral, Albert Laszlo Barabasi, Devon Brewer,
Nicholas Christakis, Noshir Contractor, James Fowler, Myron Gutmann et al. 2009. “Life in the
network: the coming age of computational social science.” Science (New York, NY) 323(5915):721.

Lim, Yew Jin and Yee Whye Teh. 2007. Variational Bayesian approach to movie rating prediction. In

Proceedings of KDD Cup and Workshop. Vol. 7 Citeseer pp. 15–21.

Lloyd, Stuart. 1982. “Least squares quantization in PCM.” Information Theory, IEEE Transactions on

28(2):129–137.

Mahajan, Meena, Prajakta Nimbhorkar and Kasturi Varadarajan. 2009. The planar k-means problem is

NP-hard. In WALCOM: Algorithms and Computation. Springer pp. 274–285.

McLachlan, Geoffrey and David Peel. 2004. Finite mixture models. John Wiley & Sons.

Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality. In Advances in Neural Information Processing
Systems. pp. 3111–3119.

Mimno, David and David Blei. 2011. Bayesian checking for topic models. In Proceedings of the Con-
ference on Empirical Methods in Natural Language Processing. Association for Computational Lin-
guistics pp. 227–237.

Mimno, David, Hanna M Wallach, Edmund Talley, Miriam Leenders and Andrew McCallum. 2011. Op-
timizing semantic coherence in topic models. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing. Association for Computational Linguistics pp. 262–272.

41

Mullainathan, Sendhil. 2014. “What Big Data Means for Social Science.” Behavioral and Experimental

Seminar.

Murphy, Kevin P. 2012. Machine learning: a probabilistic perspective. MIT press.

Nguyen, Thang, Yuening Hu and Jordan Boyd-Graber. 2014. Anchors Regularized: Adding Robustness
and Extensibility to Scalable Topic-Modeling Algorithms. In Association for Computational Linguis-
tics.

Nielsen, Frank and Richard Nock. 2014. “Further heuristics for k-means: The merge-and-split heuristic

and the (k, l)-means.” arXiv preprint arXiv:1406.6314 .

O’Connor, Brendan. 2014. MiTextExplorer: Linked brushing and mutual information for exploratory
text data analysis. In Proceedings of the Workshop on Interactive Language Learning, Visualization,
and Interfaces. Association for Computational Linguistics pp. 1–13.

OConnor, Brendan, Brandon M Stewart and Noah A Smith. 2013. Learning to Extract International

Relations from Political Context. In Proceedings of ACL.

Papadimitriou, Christos H and Kenneth Steiglitz. 1998. Combinatorial optimization: algorithms and

complexity. Courier Dover Publications.

Park, Jong Hee. 2012. “A Uniﬁed Method for Dynamic and Cross-Sectional Heterogeneity: Introducing

Hidden Markov Panel Models.” American Journal of Political Science 56(4):1040–1054.

Pearson, Karl. 1894. “Contributions to the mathematical theory of evolution.” Philosophical Transac-

tions of the Royal Society of London. A pp. 71–110.

Poole, Keith T and Howard Rosenthal. 1997. Congress: A political-economic history of roll call voting.

Oxford University Press.

Quinn, Kevin M, Burt L Monroe, Michael Colaresi, Michael H Crespin and Dragomir R Radev. 2010.
“How to analyze political attention with minimal assumptions and costs.” American Journal of Politi-
cal Science 54(1):209–228.

Rabiner, Lawrence and Biing-Hwang Juang. 1986. “An introduction to hidden Markov models.” ASSP

Magazine, IEEE 3(1):4–16.

Recht, Ben, Christopher Re, Joel Tropp and Victor Bittorf. 2012. Factoring nonnegative matrices with

linear programs. In Advances in Neural Information Processing Systems. pp. 1214–1222.

Reich, Justin, Dustin Tingley, Jetson Leder-Luis, Margaret E Roberts and Brandon M Stewart. N.d.
Computer Assisted Reading and Discovery for Student Generated Text. Technical report Working
Paper. Export.

Rivers, Douglas. 2003. “Identiﬁcation of multidimensional spatial voting models.” Typescript. Stanford

University .

Robert, Christian P and George Casella. 2004. Monte Carlo statistical methods. Vol. 319 Springer New

York.

42

Roberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis,
Shana Kushner Gadarian, Bethany Albertson and David G Rand. 2014. “Structural Topic Models
for Open-Ended Survey Responses.” American Journal of Political Science .

Ruiz, Francisco J. R., Isabel Valera, Carlos Blanco and Fernando Perez-Cruz. 2014. “Bayesian Non-
parametric Comorbidity Analysis of Psychiatric Disorders.” Journal of Machine Learning Research
15:1215–1247.
URL: http://jmlr.org/papers/v15/ruiz14a.html

Shneiderman, Ben. 1996. The eyes have it: A task by data type taxonomy for information visualizations.

In Visual Languages, 1996. Proceedings., IEEE Symposium on. IEEE pp. 336–343.

Song, Le, Eric P Xing and Ankur P Parikh. 2011. A spectral algorithm for latent tree graphical models. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11). pp. 1065–1072.

Sontag, David and Dan Roy. 2011. Complexity of inference in latent dirichlet allocation. In Advances in

Neural Information Processing Systems. pp. 1008–1016.

Srivastava, Virendera K and David EA Giles. 1987. Seemingly unrelated regression equations models:

estimation and inference. Vol. 80 CRC Press.

Teh, Yee Whye, Michael I Jordan, Matthew J Beal and David M Blei. 2006. “Hierarchical dirichlet

processes.” Journal of the american statistical association 101(476).

Tenenbaum, Joshua B, Charles Kemp, Thomas L Grifﬁths and Noah D Goodman. 2011. “How to grow

a mind: Statistics, structure, and abstraction.” science 331(6022):1279–1285.

Tibshirani, Robert. 1996. “Regression shrinkage and selection via the lasso.” Journal of the Royal Sta-

tistical Society. Series B (Methodological) pp. 267–288.

Vavasis, Stephen A. 2009. “On the complexity of nonnegative matrix factorization.” SIAM Journal on

Optimization 20(3):1364–1377.

Wallach, Hanna M, Iain Murray, Ruslan Salakhutdinov and David Mimno. 2009. Evaluation methods
for topic models. In Proceedings of the 26th Annual International Conference on Machine Learning.
ACM pp. 1105–1112.

Ward, Michael D, Brian D Greenhill and Kristin M Bakke. 2010. “The perils of policy by p-value:

Predicting civil conﬂicts.” Journal of Peace Research 47(4):363–375.

Wolpert, David H and William G Macready. 1997. “No free lunch theorems for optimization.” Evolu-

tionary Computation, IEEE Transactions on 1(1):67–82.

Yao, Limin, David Mimno and Andrew McCallum. 2009. Efﬁcient methods for topic model inference on
streaming document collections. In Proceedings of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining. ACM pp. 937–946.

Zhou, Tianyi, Jeff Bilmes and Carlos Guestrin. 2014. “Divide-and-Conquer Learning by Anchoring a

Conical Hull.” arXiv preprint arXiv:1406.5752 .

43

Figure 1: Five example topics from the reference model. These are given the labels Supreme Court,
Cheney, Global Warming, Iran/N.K. Nukes, and Wright respectively.

44

Topic 18:  law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,feder, requir, amend, protect, gun, govern, allow, appeal, citizenTopic 30:  presid, vice, cheney, offic, presidenti, first, execut, dick, decis, leader,role, histori, nation, branch, power, part, govern, order, idea, washingtonTopic 48:  global, warm, research, climat, studi, chang, scienc, scientist, gore, caus,human, scientif, earth, emiss, planet, cell, environment, report, water, greenTopic 60:  iran, nuclear, threat, weapon, iranian, program, missil, north, bomb, defens,korea, strike, sanction, intern, build, militari, intellig, capabl, pose,developTopic 71:  black, wright, white, race, church, racial, racist, pastor, jeremiah,africanamerican, racism, african, comment, reverend, king, controversi, rev,view, communiti, southFigure 2: Relation between three measures of topic similarity across all topics and modes. Plotted
surface is a kernel smoothed density estimate.

45

02468100.00.51.01.52.0L1 and Top WordsTop 10 Words in CommonL1 Distance02468100.00.51.01.52.0L1 and Top DocsTop 10 Docs in CommonL1 Distance02468100246810Words and DocsTop 10 Docs in CommonTop 10 Words in CommonFigure 3: Comparison between the approximation to the bound on the marginal likelihood (the objective
function) with similarity metrics aggregated to the model level.

46

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.40.50.60.70.80.91.0Bound and WordsBoundProportion of 10 Words in Commonlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.40.60.81.0Bound and DocsBoundProportion of 10 Docs in Commonlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.00.20.40.60.81.01.2Bound and L1BoundExpected L1 DistFigure 4: Aggregation of similarity metrics to the topic level.

47

Word Expectations over RunsExpected Common WordsFrequency2468051015CourtCheneyClimateIran/N.K.WrightDoc Expectations over RunsExpected Common DocsFrequency0246802468101214CourtCheneyClimateIran/N.K.WrightL1 Expectations over RunsExpected DistFrequency0.51.01.50246810CourtCheneyClimateIran/N.K.WrightFigure 5: Differences in topical coverage by rating (controlling for time). Effects to the right of 0
indicate a topic more heavily used by Conservatives. Lines indicate 95% conﬁdence intervals using the
“global” approximation to measurement uncertainty (Roberts et al., 2014).

48

l−0.006−0.004−0.0020.0000.0020.0040.006Partisan Rating Effects by TopicExpected Difference in Topic Proportion (Conservative minus Liberal)lWrightlIran/North Korea NukelGlobal WarminglCheneylSupreme CourtFigure 6: The ﬁrst 300 characters of the three posts most associated with the global warming topic.
Posts 1 and 3 come from American Thinker and post 2 comes from Think Progress.

49

Deathly news for the religion of Global Warming. Looks like at least oneprominent scientific group has changed its mind about the irrefutability ofevidence regarding man made climate change.The American Physical Societyrepresenting nearly 50,000 physicists "has reversed its stance on climateClimate change report forecasts global sea levels to rise up to 4 feet by 2100.According to a new report led by the U.S. Geological Survey, the U.S. â€œfacesthe possibility of much more rapid climate change by the end of the centurythan previous studies have suggested.â€(cid:157) The report,NASA has confirmed that a developing natural climate pattern will likely resultin much colder temperatures.Â(cid:160) Of course, the climate alarmists' favoritedubious data source was also quick to point out that such natural phenomenashould not confuse the issue of manmade greenhouse gas induced globalNavigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Even in traditional general linear models, there is also always the choice of model speciﬁcation in both

variables and functional form. Although multimodality brings new issues to the table, the responsibility

of the researcher to carefully validate the chosen model is fundamentally the same. This is true regardless

of whether the choice between competing models arises due to a non-convex latent variable model or

due to the selection of an important model tuning parameter in a globally convex problem. Thus even if

multimodality is an unfamiliar problem, social scientists can draw on the same set of best practices that

they employ throughout their research.

An important practical contribution of this chapter is that it extends the set of tools available to

scholars using topic models in applied research. While we have focused on STM, many of the procedures

we use will be helpful for a broader class of latent variable models. For instance, the approaches to

aligning topics and calculating stability across runs can all be applied directly to the broader class of

statistical topic models and with minor modiﬁcations to most latent variable models.

We see great potential for the analysis of “big” data in the social sciences, but rather than focus on

the data we have taken a more methodological focus. We think this has important implications for both

methodological development but also could structure the types of questions we ask, and the types of data

sets we seek to build. Methodologically, we think that there will be important advances in areas such

as optimal initialization strategies, which is especially important as our data sets grow in size. From an

applied perspective, users will be unlikely to want to wait for extended periods of time in order to get

even a single set of results. Advances in computational power needs to be matched with smart ways to

leverage that power. From a research design perspective, we think more focus should be put on bringing

greater structure to so called “unstructured” data. In the STM we focus on the inclusion of metadata

for modeling and hypothesis testing, but this is only one possible use. Can more direct supervision help

us with issues of multimodality? Of course, in the end, big data will be at its best when there is active

dialogue between those who pose the big question and those who might provide the big answers.

References
Airoldi, Edoardo M, David M Blei, Stephen E Fienberg and Eric P Xing. 2009. Mixed membership

stochastic blockmodels. In Advances in Neural Information Processing Systems. pp. 33–40.

Anandkumar, Anima, Rong Ge, Daniel Hsu and Sham M Kakade. 2013. “A tensor spectral approach to

learning mixed membership community models.” arXiv preprint arXiv:1302.2684 .

36

Anandkumar, Anima, Rong Ge, Daniel Hsu, Sham M Kakade and Matus Telgarsky. 2012. “Tensor

decompositions for learning latent variable models.” arXiv preprint arXiv:1210.7559 .

Anandkumar, Anima, Yi-kai Liu, Daniel J Hsu, Dean P Foster and Sham M Kakade. 2012. A spec-
tral algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems.
pp. 917–925.

Anandkumar, Animashree, Daniel Hsu, Adel Javanmard and Sham Kakade. 2013. Learning linear
bayesian networks with latent variables. In Proceedings of The 30th International Conference on Ma-
chine Learning. pp. 249–257.

Anandkumar, Animashree, Daniel Hsu and Sham M Kakade. 2012. “A method of moments for mixture

models and hidden Markov models.” arXiv preprint arXiv:1203.0683 .

Arora, Sanjeev, Aditya Bhaskara, Rong Ge and Tengyu Ma. 2013. “Provable bounds for learning some

deep representations.” arXiv preprint .

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2012. Learning topic models–going beyond svd. In Foun-

dations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE pp. 1–10.

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2013. “New algorithms for learning incoherent and over-

complete dictionaries.” arXiv preprint arXiv:1308.6273 .

Arora, Sanjeev, Rong Ge, Ankur Moitra and Sushant Sachdeva. 2012. Provable ICA with unknown
Gaussian noise, with implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems. pp. 2375–2383.

Arora, Sanjeev, Rong Ge, Ravindran Kannan and Ankur Moitra. 2012. Computing a nonnegative ma-
trix factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of
computing. ACM pp. 145–162.

Arora, Sanjeev, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu
and Michael Zhu. 2013. A Practical Algorithm for Topic Modeling with Provable Guarantees. In
Proceedings of The 30th International Conference on Machine Learning. pp. 280–288.

Arthur, David and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial
and Applied Mathematics pp. 1027–1035.

Asuncion Jr, Arthur Uy. 2011. Distributed and accelerated inference algorithms for probabilistic graph-

ical models. Technical report California State University at Long Beach.

Bahmani, Bahman, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei Vassilvitskii. 2012.

“Scalable k-means++.” Proceedings of the VLDB Endowment 5(7):622–633.

Beck, Nathaniel, Gary King and Langche Zeng. 2000. “Improving quantitative studies of international

conﬂict: A conjecture.” American Political Science Review pp. 21–35.

Belloni, Alexandre, Victor Chernozhukov and Christian Hansen. 2014. “High-Dimensional Methods and

Inference on Structural and Treatment Effects.” The Journal of Economic Perspectives 28(2):29–50.

37

Bishop, Christopher M et al. 2006. Pattern recognition and machine learning. Vol. 1 springer New York.

Blei, David M. 2012. “Probabilistic topic models.” Communications of the ACM 55(4):77–84.

Blei, David M. 2014. “Build, compute, critique, repeat: data analysis with latent variable models.”

Annual Review of Statistics and Its Application 1:203–232.

Blei, David M, Andrew Y Ng and Michael I Jordan. 2003. “Latent dirichlet allocation.” the Journal of

machine Learning research 3:993–1022.

Boyd, Stephen and Lieven Vandenberghe. 2009. Convex optimization. Cambridge university press.

Carpenter, Bob. 2010. Integrating out multinomial parameters in latent Dirichlet allocation and naive

Bayes for collapsed Gibbs sampling. Technical report Technical report, LingPipe.

Chaney, Allison June-Barlow and David M Blei. 2012. Visualizing Topic Models. In ICWSM.

Chang, Jonathan. 2012. lda: Collapsed Gibbs sampling methods for topic models. R package version

1.3.2.
URL: http://CRAN.R-project.org/package=lda

Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-graber and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In Advances in neural information processing systems.
pp. 288–296.

Chuang, Jason, Christopher D Manning and Jeffrey Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of the International Working Conference on Advanced
Visual Interfaces. ACM pp. 74–77.

Chuang, Jason, Sonal Gupta, Christopher Manning and Jeffrey Heer. 2013. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In Proceedings of the 30th International Conference
on Machine Learning (ICML-13). pp. 612–620.

Cochocki, A and Rolf Unbehauen. 1993. Neural networks for optimization and signal processing. John

Wiley & Sons, Inc.

Council, National Research. 2013. Frontiers in Massive Data Analysis. The National Academies Press.

de Klerk, Etienne and Dmitrii V Pasechnik. 2002. “Approximation of the stability number of a graph via

copositive programming.” SIAM Journal on Optimization 12(4):875–892.

De Marchi, Scott, Christopher Gelpi and Jeffrey D Grynaviski. 2004. “Untangling neural nets.” American

Political Science Review 98(02):371–378.

Dean, Jeffrey and Sanjay Ghemawat. 2008. “MapReduce: simpliﬁed data processing on large clusters.”

Communications of the ACM 51(1):107–113.

Deb, Partha and Pravin K Trivedi. 2002. “The structure of demand for health care: latent class versus

two-part models.” Journal of health economics 21(4):601–625.

Dempster, Arthur P, Nan M Laird and Donald B Rubin. 1977. “Maximum likelihood from incomplete
data via the EM algorithm.” Journal of the Royal Statistical Society. Series B (Methodological) pp. 1–
38.

38

Ding, Weicong, Mohammad H Rohban, Prakash Ishwar and Venkatesh Saligrama. 2013. “Topic discov-

ery through data dependent and random projections.” arXiv preprint arXiv:1303.3664 .

Ding, Weicong, Prakash Ishwar, Mohammad H Rohban and Venkatesh Saligrama. 2013. “Necessary
and Sufﬁcient Conditions for Novel Word Detection in Separable Topic Models.” arXiv preprint
arXiv:1310.7994 .

Donoho, David and Victoria Stodden. 2003. When does non-negative matrix factorization give a correct

decomposition into parts? In Advances in neural information processing systems. p. None.

Doshi-Velez, Finale, Yaorong Ge and Isaac Kohane. 2014. “Comorbidity clusters in autism spectrum

disorders: an electronic health record time-series analysis.” Pediatrics 133(1):e54–e63.

Drton, Mathias and Thomas S Richardson. 2004. “Multimodality of the likelihood in the bivariate

seemingly unrelated regressions model.” Biometrika 91(2):383–392.

DuMouchel, William. 1999. “Bayesian data mining in large frequency tables, with an application to the

FDA spontaneous reporting system.” The American Statistician 53(3):177–190.

Efron, Bradley et al. 1978. “The geometry of exponential families.” The Annals of Statistics 6(2):362–

376.

Eisenstein, Jacob and Eric Xing. 2010. “The CMU 2008 Political Blog Corpus.”.

Fan, Jianqing, Fang Han and Han Liu. 2014. “Challenges of Big Data analysis.” National Science Review

p. nwt032.

Foulds, J. R. and P. Smyth. 2014. Annealing Paths for the Evaluation of Topic Models. In Proceedings

of the Thirtieth Conference Conference on Uncertainty in Artiﬁcial Intelligence.

Gardner, Matthew J, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric Ringger and Kevin Seppi.
2010. The topic browser: An interactive tool for browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.

Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari and Donald B Rubin. 2013.

Bayesian data analysis. CRC press.

Gillis, Nicolas. 2014.

arXiv:1401.5226 .

“The why and how of nonnegative matrix factorization.” arXiv preprint

Gillis, Nicolas and Robert Luce. 2013. “Robust near-separable nonnegative matrix factorization using

linear optimization.” arXiv preprint arXiv:1302.4385 .

Goldstone, AndrewUnderwood et al. 2014. “The Quiet Transformations of Literary Studies: What

Thirteen Thousand Scholars Could Tell Us.”.

Grifﬁths, DMBTL and MIJJB Tenenbaum. 2004. “Hierarchical topic models and the nested Chinese

restaurant process.” Advances in neural information processing systems 16:17.

Grifﬁths, Thomas L and Mark Steyvers. 2004. “Finding scientiﬁc topics.” Proceedings of the National

academy of Sciences of the United States of America 101(Suppl 1):5228–5235.

39

Grimmer, Justin. 2010a. “A Bayesian hierarchical topic model for political texts: Measuring expressed

agendas in Senate press releases.” Political Analysis 18(1):1–35.

Grimmer, Justin. 2010b. “An introduction to Bayesian inference via variational approximations.” Politi-

cal Analysis p. mpq027.

Grimmer, Justin. 2013. Representational Style in Congress: What Legislators Say and why it Matters.

Cambridge University Press.

Grimmer, Justin and Brandon M Stewart. 2013. “Text as data: The promise and pitfalls of automatic

content analysis methods for political texts.” Political Analysis p. mps028.

Grimmer, Justin and Gary King. 2011. “General purpose computer-assisted clustering and conceptual-

ization.” Proceedings of the National Academy of Sciences 108(7):2643–2650.

Hambleton, Ronald K. 1991. Fundamentals of item response theory. Vol. 2 Sage publications.

Hamilton, James D. 1989. “A new approach to the economic analysis of nonstationary time series and

the business cycle.” Econometrica: Journal of the Econometric Society pp. 357–384.

Hoff, Peter D, Adrian E Raftery and Mark S Handcock. 2002. “Latent space approaches to social network

analysis.” Journal of the american Statistical association 97(460):1090–1098.

Hopkins, Daniel J and Gary King. 2010. “A method of automated nonparametric content analysis for

social science.” American Journal of Political Science 54(1):229–247.

Hornik, Kurt. 2005. “A clue for cluster ensembles.” Journal of Statistical Software 14(12).

Hsu, Daniel and Sham M Kakade. 2013. Learning mixtures of spherical gaussians: moment methods and
spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer
Science. ACM pp. 11–20.

Hu, Yuening, Jordan Boyd-Graber, Brianna Satinoff and Alison Smith. 2014. “Interactive topic model-

ing.” Machine Learning 95(3):423–469.

Huang, Furong, UN Niranjan, M Hakeem and Animashree Anandkumar. 2013. “Fast Detection of

Overlapping Communities via Online Tensor Methods.”.

Jordan, Michael I, Zoubin Ghahramani, Tommi S Jaakkola and Lawrence K Saul. 1998. An introduction

to variational methods for graphical models. Springer.

Kalai, Adam Tauman, Ankur Moitra and Gregory Valiant. 2012. “Disentangling gaussians.” Communi-

cations of the ACM 55(2):113–120.

Kass, Robert E and Adrian E Raftery. 1995. “Bayes factors.” Journal of the american statistical associ-

ation 90(430):773–795.

King, Gary. 1989. “Unifying political methodology.” New York: Cambridge .

King, Gary, Jennifer Pan and Margaret E Roberts. 2013. “How censorship in China allows government

criticism but silences collective expression.” American Political Science Review 107(02):326–343.

40

Kivinen, Jyrki and Manfred K Warmuth. 1997. “Exponentiated gradient versus gradient descent for

linear predictors.” Information and Computation 132(1):1–63.

Koltcov, Sergei, Olessia Koltsova and Sergey Nikolenko. 2014. Latent dirichlet allocation: stability and
applications to studies of user-generated content. In Proceedings of the 2014 ACM conference on Web
science. ACM pp. 161–165.

Krippendorff, Klaus. 2012. Content analysis: An introduction to its methodology. Sage.

Kuhn, Harold W. 1955. “The Hungarian method for the assignment problem.” Naval research logistics

quarterly 2(1-2):83–97.

Kumar, Abhishek, Vikas Sindhwani and Prabhanjan Kambadur. 2012. “Fast conical hull algorithms for

near-separable non-negative matrix factorization.” arXiv preprint arXiv:1210.1190 .

Lancichinetti, Andrea, M Irmak Sirer, Jane X Wang, Daniel Acuna, Konrad K¨ording and Lu´ıs A Nunes
Amaral. 2014. “A high-reproducibility and high-accuracy method for automated topic classiﬁcation.”
arXiv preprint arXiv:1402.0422 .

Lau, Jey Han, David Newman and Timothy Baldwin. 2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality. In Proceedings of the European Chapter of the
Association for Computational Linguistics.

Lauderdale, Benjamin E and Tom S Clark. 2014. “Scaling politically meaningful dimensions using texts

and votes.” American Journal of Political Science .

Lazer, David, Alex Sandy Pentland, Lada Adamic, Sinan Aral, Albert Laszlo Barabasi, Devon Brewer,
Nicholas Christakis, Noshir Contractor, James Fowler, Myron Gutmann et al. 2009. “Life in the
network: the coming age of computational social science.” Science (New York, NY) 323(5915):721.

Lim, Yew Jin and Yee Whye Teh. 2007. Variational Bayesian approach to movie rating prediction. In

Proceedings of KDD Cup and Workshop. Vol. 7 Citeseer pp. 15–21.

Lloyd, Stuart. 1982. “Least squares quantization in PCM.” Information Theory, IEEE Transactions on

28(2):129–137.

Mahajan, Meena, Prajakta Nimbhorkar and Kasturi Varadarajan. 2009. The planar k-means problem is

NP-hard. In WALCOM: Algorithms and Computation. Springer pp. 274–285.

McLachlan, Geoffrey and David Peel. 2004. Finite mixture models. John Wiley & Sons.

Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality. In Advances in Neural Information Processing
Systems. pp. 3111–3119.

Mimno, David and David Blei. 2011. Bayesian checking for topic models. In Proceedings of the Con-
ference on Empirical Methods in Natural Language Processing. Association for Computational Lin-
guistics pp. 227–237.

Mimno, David, Hanna M Wallach, Edmund Talley, Miriam Leenders and Andrew McCallum. 2011. Op-
timizing semantic coherence in topic models. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing. Association for Computational Linguistics pp. 262–272.

41

Mullainathan, Sendhil. 2014. “What Big Data Means for Social Science.” Behavioral and Experimental

Seminar.

Murphy, Kevin P. 2012. Machine learning: a probabilistic perspective. MIT press.

Nguyen, Thang, Yuening Hu and Jordan Boyd-Graber. 2014. Anchors Regularized: Adding Robustness
and Extensibility to Scalable Topic-Modeling Algorithms. In Association for Computational Linguis-
tics.

Nielsen, Frank and Richard Nock. 2014. “Further heuristics for k-means: The merge-and-split heuristic

and the (k, l)-means.” arXiv preprint arXiv:1406.6314 .

O’Connor, Brendan. 2014. MiTextExplorer: Linked brushing and mutual information for exploratory
text data analysis. In Proceedings of the Workshop on Interactive Language Learning, Visualization,
and Interfaces. Association for Computational Linguistics pp. 1–13.

OConnor, Brendan, Brandon M Stewart and Noah A Smith. 2013. Learning to Extract International

Relations from Political Context. In Proceedings of ACL.

Papadimitriou, Christos H and Kenneth Steiglitz. 1998. Combinatorial optimization: algorithms and

complexity. Courier Dover Publications.

Park, Jong Hee. 2012. “A Uniﬁed Method for Dynamic and Cross-Sectional Heterogeneity: Introducing

Hidden Markov Panel Models.” American Journal of Political Science 56(4):1040–1054.

Pearson, Karl. 1894. “Contributions to the mathematical theory of evolution.” Philosophical Transac-

tions of the Royal Society of London. A pp. 71–110.

Poole, Keith T and Howard Rosenthal. 1997. Congress: A political-economic history of roll call voting.

Oxford University Press.

Quinn, Kevin M, Burt L Monroe, Michael Colaresi, Michael H Crespin and Dragomir R Radev. 2010.
“How to analyze political attention with minimal assumptions and costs.” American Journal of Politi-
cal Science 54(1):209–228.

Rabiner, Lawrence and Biing-Hwang Juang. 1986. “An introduction to hidden Markov models.” ASSP

Magazine, IEEE 3(1):4–16.

Recht, Ben, Christopher Re, Joel Tropp and Victor Bittorf. 2012. Factoring nonnegative matrices with

linear programs. In Advances in Neural Information Processing Systems. pp. 1214–1222.

Reich, Justin, Dustin Tingley, Jetson Leder-Luis, Margaret E Roberts and Brandon M Stewart. N.d.
Computer Assisted Reading and Discovery for Student Generated Text. Technical report Working
Paper. Export.

Rivers, Douglas. 2003. “Identiﬁcation of multidimensional spatial voting models.” Typescript. Stanford

University .

Robert, Christian P and George Casella. 2004. Monte Carlo statistical methods. Vol. 319 Springer New

York.

42

Roberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis,
Shana Kushner Gadarian, Bethany Albertson and David G Rand. 2014. “Structural Topic Models
for Open-Ended Survey Responses.” American Journal of Political Science .

Ruiz, Francisco J. R., Isabel Valera, Carlos Blanco and Fernando Perez-Cruz. 2014. “Bayesian Non-
parametric Comorbidity Analysis of Psychiatric Disorders.” Journal of Machine Learning Research
15:1215–1247.
URL: http://jmlr.org/papers/v15/ruiz14a.html

Shneiderman, Ben. 1996. The eyes have it: A task by data type taxonomy for information visualizations.

In Visual Languages, 1996. Proceedings., IEEE Symposium on. IEEE pp. 336–343.

Song, Le, Eric P Xing and Ankur P Parikh. 2011. A spectral algorithm for latent tree graphical models. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11). pp. 1065–1072.

Sontag, David and Dan Roy. 2011. Complexity of inference in latent dirichlet allocation. In Advances in

Neural Information Processing Systems. pp. 1008–1016.

Srivastava, Virendera K and David EA Giles. 1987. Seemingly unrelated regression equations models:

estimation and inference. Vol. 80 CRC Press.

Teh, Yee Whye, Michael I Jordan, Matthew J Beal and David M Blei. 2006. “Hierarchical dirichlet

processes.” Journal of the american statistical association 101(476).

Tenenbaum, Joshua B, Charles Kemp, Thomas L Grifﬁths and Noah D Goodman. 2011. “How to grow

a mind: Statistics, structure, and abstraction.” science 331(6022):1279–1285.

Tibshirani, Robert. 1996. “Regression shrinkage and selection via the lasso.” Journal of the Royal Sta-

tistical Society. Series B (Methodological) pp. 267–288.

Vavasis, Stephen A. 2009. “On the complexity of nonnegative matrix factorization.” SIAM Journal on

Optimization 20(3):1364–1377.

Wallach, Hanna M, Iain Murray, Ruslan Salakhutdinov and David Mimno. 2009. Evaluation methods
for topic models. In Proceedings of the 26th Annual International Conference on Machine Learning.
ACM pp. 1105–1112.

Ward, Michael D, Brian D Greenhill and Kristin M Bakke. 2010. “The perils of policy by p-value:

Predicting civil conﬂicts.” Journal of Peace Research 47(4):363–375.

Wolpert, David H and William G Macready. 1997. “No free lunch theorems for optimization.” Evolu-

tionary Computation, IEEE Transactions on 1(1):67–82.

Yao, Limin, David Mimno and Andrew McCallum. 2009. Efﬁcient methods for topic model inference on
streaming document collections. In Proceedings of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining. ACM pp. 937–946.

Zhou, Tianyi, Jeff Bilmes and Carlos Guestrin. 2014. “Divide-and-Conquer Learning by Anchoring a

Conical Hull.” arXiv preprint arXiv:1406.5752 .

43

Figure 1: Five example topics from the reference model. These are given the labels Supreme Court,
Cheney, Global Warming, Iran/N.K. Nukes, and Wright respectively.

44

Topic 18:  law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,feder, requir, amend, protect, gun, govern, allow, appeal, citizenTopic 30:  presid, vice, cheney, offic, presidenti, first, execut, dick, decis, leader,role, histori, nation, branch, power, part, govern, order, idea, washingtonTopic 48:  global, warm, research, climat, studi, chang, scienc, scientist, gore, caus,human, scientif, earth, emiss, planet, cell, environment, report, water, greenTopic 60:  iran, nuclear, threat, weapon, iranian, program, missil, north, bomb, defens,korea, strike, sanction, intern, build, militari, intellig, capabl, pose,developTopic 71:  black, wright, white, race, church, racial, racist, pastor, jeremiah,africanamerican, racism, african, comment, reverend, king, controversi, rev,view, communiti, southFigure 2: Relation between three measures of topic similarity across all topics and modes. Plotted
surface is a kernel smoothed density estimate.

45

02468100.00.51.01.52.0L1 and Top WordsTop 10 Words in CommonL1 Distance02468100.00.51.01.52.0L1 and Top DocsTop 10 Docs in CommonL1 Distance02468100246810Words and DocsTop 10 Docs in CommonTop 10 Words in CommonFigure 3: Comparison between the approximation to the bound on the marginal likelihood (the objective
function) with similarity metrics aggregated to the model level.

46

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.40.50.60.70.80.91.0Bound and WordsBoundProportion of 10 Words in Commonlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.40.60.81.0Bound and DocsBoundProportion of 10 Docs in Commonlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.00.20.40.60.81.01.2Bound and L1BoundExpected L1 DistFigure 4: Aggregation of similarity metrics to the topic level.

47

Word Expectations over RunsExpected Common WordsFrequency2468051015CourtCheneyClimateIran/N.K.WrightDoc Expectations over RunsExpected Common DocsFrequency0246802468101214CourtCheneyClimateIran/N.K.WrightL1 Expectations over RunsExpected DistFrequency0.51.01.50246810CourtCheneyClimateIran/N.K.WrightFigure 5: Differences in topical coverage by rating (controlling for time). Effects to the right of 0
indicate a topic more heavily used by Conservatives. Lines indicate 95% conﬁdence intervals using the
“global” approximation to measurement uncertainty (Roberts et al., 2014).

48

l−0.006−0.004−0.0020.0000.0020.0040.006Partisan Rating Effects by TopicExpected Difference in Topic Proportion (Conservative minus Liberal)lWrightlIran/North Korea NukelGlobal WarminglCheneylSupreme CourtFigure 6: The ﬁrst 300 characters of the three posts most associated with the global warming topic.
Posts 1 and 3 come from American Thinker and post 2 comes from Think Progress.

49

Deathly news for the religion of Global Warming. Looks like at least oneprominent scientific group has changed its mind about the irrefutability ofevidence regarding man made climate change.The American Physical Societyrepresenting nearly 50,000 physicists "has reversed its stance on climateClimate change report forecasts global sea levels to rise up to 4 feet by 2100.According to a new report led by the U.S. Geological Survey, the U.S. â€œfacesthe possibility of much more rapid climate change by the end of the centurythan previous studies have suggested.â€(cid:157) The report,NASA has confirmed that a developing natural climate pattern will likely resultin much colder temperatures.Â(cid:160) Of course, the climate alarmists' favoritedubious data source was also quick to point out that such natural phenomenashould not confuse the issue of manmade greenhouse gas induced globalFigure 7: Distribution of the partisan rating effect across modes for the ﬁve example topics. The black
solid line shows the effect at the reference mode and the black dashed line marks an effect size of 0.

50

Supreme CourtRating EffectFrequency−0.008−0.006−0.004−0.0020.0000.002020406080No effectReference modelCheneyRating EffectFrequency−0.010−0.0050.000020406080Global WarmingRating EffectFrequency0.0000.0010.0020.0030.0040.0050.006020406080100Iran/N.K. NukesRating EffectFrequency−0.010−0.0050.0000.0050.010050100150Rev. WrightRating EffectFrequency−0.0020.0000.0020.0040.006020406080100120Navigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Even in traditional general linear models, there is also always the choice of model speciﬁcation in both

variables and functional form. Although multimodality brings new issues to the table, the responsibility

of the researcher to carefully validate the chosen model is fundamentally the same. This is true regardless

of whether the choice between competing models arises due to a non-convex latent variable model or

due to the selection of an important model tuning parameter in a globally convex problem. Thus even if

multimodality is an unfamiliar problem, social scientists can draw on the same set of best practices that

they employ throughout their research.

An important practical contribution of this chapter is that it extends the set of tools available to

scholars using topic models in applied research. While we have focused on STM, many of the procedures

we use will be helpful for a broader class of latent variable models. For instance, the approaches to

aligning topics and calculating stability across runs can all be applied directly to the broader class of

statistical topic models and with minor modiﬁcations to most latent variable models.

We see great potential for the analysis of “big” data in the social sciences, but rather than focus on

the data we have taken a more methodological focus. We think this has important implications for both

methodological development but also could structure the types of questions we ask, and the types of data

sets we seek to build. Methodologically, we think that there will be important advances in areas such

as optimal initialization strategies, which is especially important as our data sets grow in size. From an

applied perspective, users will be unlikely to want to wait for extended periods of time in order to get

even a single set of results. Advances in computational power needs to be matched with smart ways to

leverage that power. From a research design perspective, we think more focus should be put on bringing

greater structure to so called “unstructured” data. In the STM we focus on the inclusion of metadata

for modeling and hypothesis testing, but this is only one possible use. Can more direct supervision help

us with issues of multimodality? Of course, in the end, big data will be at its best when there is active

dialogue between those who pose the big question and those who might provide the big answers.

References
Airoldi, Edoardo M, David M Blei, Stephen E Fienberg and Eric P Xing. 2009. Mixed membership

stochastic blockmodels. In Advances in Neural Information Processing Systems. pp. 33–40.

Anandkumar, Anima, Rong Ge, Daniel Hsu and Sham M Kakade. 2013. “A tensor spectral approach to

learning mixed membership community models.” arXiv preprint arXiv:1302.2684 .

36

Anandkumar, Anima, Rong Ge, Daniel Hsu, Sham M Kakade and Matus Telgarsky. 2012. “Tensor

decompositions for learning latent variable models.” arXiv preprint arXiv:1210.7559 .

Anandkumar, Anima, Yi-kai Liu, Daniel J Hsu, Dean P Foster and Sham M Kakade. 2012. A spec-
tral algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems.
pp. 917–925.

Anandkumar, Animashree, Daniel Hsu, Adel Javanmard and Sham Kakade. 2013. Learning linear
bayesian networks with latent variables. In Proceedings of The 30th International Conference on Ma-
chine Learning. pp. 249–257.

Anandkumar, Animashree, Daniel Hsu and Sham M Kakade. 2012. “A method of moments for mixture

models and hidden Markov models.” arXiv preprint arXiv:1203.0683 .

Arora, Sanjeev, Aditya Bhaskara, Rong Ge and Tengyu Ma. 2013. “Provable bounds for learning some

deep representations.” arXiv preprint .

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2012. Learning topic models–going beyond svd. In Foun-

dations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE pp. 1–10.

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2013. “New algorithms for learning incoherent and over-

complete dictionaries.” arXiv preprint arXiv:1308.6273 .

Arora, Sanjeev, Rong Ge, Ankur Moitra and Sushant Sachdeva. 2012. Provable ICA with unknown
Gaussian noise, with implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems. pp. 2375–2383.

Arora, Sanjeev, Rong Ge, Ravindran Kannan and Ankur Moitra. 2012. Computing a nonnegative ma-
trix factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of
computing. ACM pp. 145–162.

Arora, Sanjeev, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu
and Michael Zhu. 2013. A Practical Algorithm for Topic Modeling with Provable Guarantees. In
Proceedings of The 30th International Conference on Machine Learning. pp. 280–288.

Arthur, David and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial
and Applied Mathematics pp. 1027–1035.

Asuncion Jr, Arthur Uy. 2011. Distributed and accelerated inference algorithms for probabilistic graph-

ical models. Technical report California State University at Long Beach.

Bahmani, Bahman, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei Vassilvitskii. 2012.

“Scalable k-means++.” Proceedings of the VLDB Endowment 5(7):622–633.

Beck, Nathaniel, Gary King and Langche Zeng. 2000. “Improving quantitative studies of international

conﬂict: A conjecture.” American Political Science Review pp. 21–35.

Belloni, Alexandre, Victor Chernozhukov and Christian Hansen. 2014. “High-Dimensional Methods and

Inference on Structural and Treatment Effects.” The Journal of Economic Perspectives 28(2):29–50.

37

Bishop, Christopher M et al. 2006. Pattern recognition and machine learning. Vol. 1 springer New York.

Blei, David M. 2012. “Probabilistic topic models.” Communications of the ACM 55(4):77–84.

Blei, David M. 2014. “Build, compute, critique, repeat: data analysis with latent variable models.”

Annual Review of Statistics and Its Application 1:203–232.

Blei, David M, Andrew Y Ng and Michael I Jordan. 2003. “Latent dirichlet allocation.” the Journal of

machine Learning research 3:993–1022.

Boyd, Stephen and Lieven Vandenberghe. 2009. Convex optimization. Cambridge university press.

Carpenter, Bob. 2010. Integrating out multinomial parameters in latent Dirichlet allocation and naive

Bayes for collapsed Gibbs sampling. Technical report Technical report, LingPipe.

Chaney, Allison June-Barlow and David M Blei. 2012. Visualizing Topic Models. In ICWSM.

Chang, Jonathan. 2012. lda: Collapsed Gibbs sampling methods for topic models. R package version

1.3.2.
URL: http://CRAN.R-project.org/package=lda

Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-graber and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In Advances in neural information processing systems.
pp. 288–296.

Chuang, Jason, Christopher D Manning and Jeffrey Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of the International Working Conference on Advanced
Visual Interfaces. ACM pp. 74–77.

Chuang, Jason, Sonal Gupta, Christopher Manning and Jeffrey Heer. 2013. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In Proceedings of the 30th International Conference
on Machine Learning (ICML-13). pp. 612–620.

Cochocki, A and Rolf Unbehauen. 1993. Neural networks for optimization and signal processing. John

Wiley & Sons, Inc.

Council, National Research. 2013. Frontiers in Massive Data Analysis. The National Academies Press.

de Klerk, Etienne and Dmitrii V Pasechnik. 2002. “Approximation of the stability number of a graph via

copositive programming.” SIAM Journal on Optimization 12(4):875–892.

De Marchi, Scott, Christopher Gelpi and Jeffrey D Grynaviski. 2004. “Untangling neural nets.” American

Political Science Review 98(02):371–378.

Dean, Jeffrey and Sanjay Ghemawat. 2008. “MapReduce: simpliﬁed data processing on large clusters.”

Communications of the ACM 51(1):107–113.

Deb, Partha and Pravin K Trivedi. 2002. “The structure of demand for health care: latent class versus

two-part models.” Journal of health economics 21(4):601–625.

Dempster, Arthur P, Nan M Laird and Donald B Rubin. 1977. “Maximum likelihood from incomplete
data via the EM algorithm.” Journal of the Royal Statistical Society. Series B (Methodological) pp. 1–
38.

38

Ding, Weicong, Mohammad H Rohban, Prakash Ishwar and Venkatesh Saligrama. 2013. “Topic discov-

ery through data dependent and random projections.” arXiv preprint arXiv:1303.3664 .

Ding, Weicong, Prakash Ishwar, Mohammad H Rohban and Venkatesh Saligrama. 2013. “Necessary
and Sufﬁcient Conditions for Novel Word Detection in Separable Topic Models.” arXiv preprint
arXiv:1310.7994 .

Donoho, David and Victoria Stodden. 2003. When does non-negative matrix factorization give a correct

decomposition into parts? In Advances in neural information processing systems. p. None.

Doshi-Velez, Finale, Yaorong Ge and Isaac Kohane. 2014. “Comorbidity clusters in autism spectrum

disorders: an electronic health record time-series analysis.” Pediatrics 133(1):e54–e63.

Drton, Mathias and Thomas S Richardson. 2004. “Multimodality of the likelihood in the bivariate

seemingly unrelated regressions model.” Biometrika 91(2):383–392.

DuMouchel, William. 1999. “Bayesian data mining in large frequency tables, with an application to the

FDA spontaneous reporting system.” The American Statistician 53(3):177–190.

Efron, Bradley et al. 1978. “The geometry of exponential families.” The Annals of Statistics 6(2):362–

376.

Eisenstein, Jacob and Eric Xing. 2010. “The CMU 2008 Political Blog Corpus.”.

Fan, Jianqing, Fang Han and Han Liu. 2014. “Challenges of Big Data analysis.” National Science Review

p. nwt032.

Foulds, J. R. and P. Smyth. 2014. Annealing Paths for the Evaluation of Topic Models. In Proceedings

of the Thirtieth Conference Conference on Uncertainty in Artiﬁcial Intelligence.

Gardner, Matthew J, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric Ringger and Kevin Seppi.
2010. The topic browser: An interactive tool for browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.

Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari and Donald B Rubin. 2013.

Bayesian data analysis. CRC press.

Gillis, Nicolas. 2014.

arXiv:1401.5226 .

“The why and how of nonnegative matrix factorization.” arXiv preprint

Gillis, Nicolas and Robert Luce. 2013. “Robust near-separable nonnegative matrix factorization using

linear optimization.” arXiv preprint arXiv:1302.4385 .

Goldstone, AndrewUnderwood et al. 2014. “The Quiet Transformations of Literary Studies: What

Thirteen Thousand Scholars Could Tell Us.”.

Grifﬁths, DMBTL and MIJJB Tenenbaum. 2004. “Hierarchical topic models and the nested Chinese

restaurant process.” Advances in neural information processing systems 16:17.

Grifﬁths, Thomas L and Mark Steyvers. 2004. “Finding scientiﬁc topics.” Proceedings of the National

academy of Sciences of the United States of America 101(Suppl 1):5228–5235.

39

Grimmer, Justin. 2010a. “A Bayesian hierarchical topic model for political texts: Measuring expressed

agendas in Senate press releases.” Political Analysis 18(1):1–35.

Grimmer, Justin. 2010b. “An introduction to Bayesian inference via variational approximations.” Politi-

cal Analysis p. mpq027.

Grimmer, Justin. 2013. Representational Style in Congress: What Legislators Say and why it Matters.

Cambridge University Press.

Grimmer, Justin and Brandon M Stewart. 2013. “Text as data: The promise and pitfalls of automatic

content analysis methods for political texts.” Political Analysis p. mps028.

Grimmer, Justin and Gary King. 2011. “General purpose computer-assisted clustering and conceptual-

ization.” Proceedings of the National Academy of Sciences 108(7):2643–2650.

Hambleton, Ronald K. 1991. Fundamentals of item response theory. Vol. 2 Sage publications.

Hamilton, James D. 1989. “A new approach to the economic analysis of nonstationary time series and

the business cycle.” Econometrica: Journal of the Econometric Society pp. 357–384.

Hoff, Peter D, Adrian E Raftery and Mark S Handcock. 2002. “Latent space approaches to social network

analysis.” Journal of the american Statistical association 97(460):1090–1098.

Hopkins, Daniel J and Gary King. 2010. “A method of automated nonparametric content analysis for

social science.” American Journal of Political Science 54(1):229–247.

Hornik, Kurt. 2005. “A clue for cluster ensembles.” Journal of Statistical Software 14(12).

Hsu, Daniel and Sham M Kakade. 2013. Learning mixtures of spherical gaussians: moment methods and
spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer
Science. ACM pp. 11–20.

Hu, Yuening, Jordan Boyd-Graber, Brianna Satinoff and Alison Smith. 2014. “Interactive topic model-

ing.” Machine Learning 95(3):423–469.

Huang, Furong, UN Niranjan, M Hakeem and Animashree Anandkumar. 2013. “Fast Detection of

Overlapping Communities via Online Tensor Methods.”.

Jordan, Michael I, Zoubin Ghahramani, Tommi S Jaakkola and Lawrence K Saul. 1998. An introduction

to variational methods for graphical models. Springer.

Kalai, Adam Tauman, Ankur Moitra and Gregory Valiant. 2012. “Disentangling gaussians.” Communi-

cations of the ACM 55(2):113–120.

Kass, Robert E and Adrian E Raftery. 1995. “Bayes factors.” Journal of the american statistical associ-

ation 90(430):773–795.

King, Gary. 1989. “Unifying political methodology.” New York: Cambridge .

King, Gary, Jennifer Pan and Margaret E Roberts. 2013. “How censorship in China allows government

criticism but silences collective expression.” American Political Science Review 107(02):326–343.

40

Kivinen, Jyrki and Manfred K Warmuth. 1997. “Exponentiated gradient versus gradient descent for

linear predictors.” Information and Computation 132(1):1–63.

Koltcov, Sergei, Olessia Koltsova and Sergey Nikolenko. 2014. Latent dirichlet allocation: stability and
applications to studies of user-generated content. In Proceedings of the 2014 ACM conference on Web
science. ACM pp. 161–165.

Krippendorff, Klaus. 2012. Content analysis: An introduction to its methodology. Sage.

Kuhn, Harold W. 1955. “The Hungarian method for the assignment problem.” Naval research logistics

quarterly 2(1-2):83–97.

Kumar, Abhishek, Vikas Sindhwani and Prabhanjan Kambadur. 2012. “Fast conical hull algorithms for

near-separable non-negative matrix factorization.” arXiv preprint arXiv:1210.1190 .

Lancichinetti, Andrea, M Irmak Sirer, Jane X Wang, Daniel Acuna, Konrad K¨ording and Lu´ıs A Nunes
Amaral. 2014. “A high-reproducibility and high-accuracy method for automated topic classiﬁcation.”
arXiv preprint arXiv:1402.0422 .

Lau, Jey Han, David Newman and Timothy Baldwin. 2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality. In Proceedings of the European Chapter of the
Association for Computational Linguistics.

Lauderdale, Benjamin E and Tom S Clark. 2014. “Scaling politically meaningful dimensions using texts

and votes.” American Journal of Political Science .

Lazer, David, Alex Sandy Pentland, Lada Adamic, Sinan Aral, Albert Laszlo Barabasi, Devon Brewer,
Nicholas Christakis, Noshir Contractor, James Fowler, Myron Gutmann et al. 2009. “Life in the
network: the coming age of computational social science.” Science (New York, NY) 323(5915):721.

Lim, Yew Jin and Yee Whye Teh. 2007. Variational Bayesian approach to movie rating prediction. In

Proceedings of KDD Cup and Workshop. Vol. 7 Citeseer pp. 15–21.

Lloyd, Stuart. 1982. “Least squares quantization in PCM.” Information Theory, IEEE Transactions on

28(2):129–137.

Mahajan, Meena, Prajakta Nimbhorkar and Kasturi Varadarajan. 2009. The planar k-means problem is

NP-hard. In WALCOM: Algorithms and Computation. Springer pp. 274–285.

McLachlan, Geoffrey and David Peel. 2004. Finite mixture models. John Wiley & Sons.

Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality. In Advances in Neural Information Processing
Systems. pp. 3111–3119.

Mimno, David and David Blei. 2011. Bayesian checking for topic models. In Proceedings of the Con-
ference on Empirical Methods in Natural Language Processing. Association for Computational Lin-
guistics pp. 227–237.

Mimno, David, Hanna M Wallach, Edmund Talley, Miriam Leenders and Andrew McCallum. 2011. Op-
timizing semantic coherence in topic models. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing. Association for Computational Linguistics pp. 262–272.

41

Mullainathan, Sendhil. 2014. “What Big Data Means for Social Science.” Behavioral and Experimental

Seminar.

Murphy, Kevin P. 2012. Machine learning: a probabilistic perspective. MIT press.

Nguyen, Thang, Yuening Hu and Jordan Boyd-Graber. 2014. Anchors Regularized: Adding Robustness
and Extensibility to Scalable Topic-Modeling Algorithms. In Association for Computational Linguis-
tics.

Nielsen, Frank and Richard Nock. 2014. “Further heuristics for k-means: The merge-and-split heuristic

and the (k, l)-means.” arXiv preprint arXiv:1406.6314 .

O’Connor, Brendan. 2014. MiTextExplorer: Linked brushing and mutual information for exploratory
text data analysis. In Proceedings of the Workshop on Interactive Language Learning, Visualization,
and Interfaces. Association for Computational Linguistics pp. 1–13.

OConnor, Brendan, Brandon M Stewart and Noah A Smith. 2013. Learning to Extract International

Relations from Political Context. In Proceedings of ACL.

Papadimitriou, Christos H and Kenneth Steiglitz. 1998. Combinatorial optimization: algorithms and

complexity. Courier Dover Publications.

Park, Jong Hee. 2012. “A Uniﬁed Method for Dynamic and Cross-Sectional Heterogeneity: Introducing

Hidden Markov Panel Models.” American Journal of Political Science 56(4):1040–1054.

Pearson, Karl. 1894. “Contributions to the mathematical theory of evolution.” Philosophical Transac-

tions of the Royal Society of London. A pp. 71–110.

Poole, Keith T and Howard Rosenthal. 1997. Congress: A political-economic history of roll call voting.

Oxford University Press.

Quinn, Kevin M, Burt L Monroe, Michael Colaresi, Michael H Crespin and Dragomir R Radev. 2010.
“How to analyze political attention with minimal assumptions and costs.” American Journal of Politi-
cal Science 54(1):209–228.

Rabiner, Lawrence and Biing-Hwang Juang. 1986. “An introduction to hidden Markov models.” ASSP

Magazine, IEEE 3(1):4–16.

Recht, Ben, Christopher Re, Joel Tropp and Victor Bittorf. 2012. Factoring nonnegative matrices with

linear programs. In Advances in Neural Information Processing Systems. pp. 1214–1222.

Reich, Justin, Dustin Tingley, Jetson Leder-Luis, Margaret E Roberts and Brandon M Stewart. N.d.
Computer Assisted Reading and Discovery for Student Generated Text. Technical report Working
Paper. Export.

Rivers, Douglas. 2003. “Identiﬁcation of multidimensional spatial voting models.” Typescript. Stanford

University .

Robert, Christian P and George Casella. 2004. Monte Carlo statistical methods. Vol. 319 Springer New

York.

42

Roberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis,
Shana Kushner Gadarian, Bethany Albertson and David G Rand. 2014. “Structural Topic Models
for Open-Ended Survey Responses.” American Journal of Political Science .

Ruiz, Francisco J. R., Isabel Valera, Carlos Blanco and Fernando Perez-Cruz. 2014. “Bayesian Non-
parametric Comorbidity Analysis of Psychiatric Disorders.” Journal of Machine Learning Research
15:1215–1247.
URL: http://jmlr.org/papers/v15/ruiz14a.html

Shneiderman, Ben. 1996. The eyes have it: A task by data type taxonomy for information visualizations.

In Visual Languages, 1996. Proceedings., IEEE Symposium on. IEEE pp. 336–343.

Song, Le, Eric P Xing and Ankur P Parikh. 2011. A spectral algorithm for latent tree graphical models. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11). pp. 1065–1072.

Sontag, David and Dan Roy. 2011. Complexity of inference in latent dirichlet allocation. In Advances in

Neural Information Processing Systems. pp. 1008–1016.

Srivastava, Virendera K and David EA Giles. 1987. Seemingly unrelated regression equations models:

estimation and inference. Vol. 80 CRC Press.

Teh, Yee Whye, Michael I Jordan, Matthew J Beal and David M Blei. 2006. “Hierarchical dirichlet

processes.” Journal of the american statistical association 101(476).

Tenenbaum, Joshua B, Charles Kemp, Thomas L Grifﬁths and Noah D Goodman. 2011. “How to grow

a mind: Statistics, structure, and abstraction.” science 331(6022):1279–1285.

Tibshirani, Robert. 1996. “Regression shrinkage and selection via the lasso.” Journal of the Royal Sta-

tistical Society. Series B (Methodological) pp. 267–288.

Vavasis, Stephen A. 2009. “On the complexity of nonnegative matrix factorization.” SIAM Journal on

Optimization 20(3):1364–1377.

Wallach, Hanna M, Iain Murray, Ruslan Salakhutdinov and David Mimno. 2009. Evaluation methods
for topic models. In Proceedings of the 26th Annual International Conference on Machine Learning.
ACM pp. 1105–1112.

Ward, Michael D, Brian D Greenhill and Kristin M Bakke. 2010. “The perils of policy by p-value:

Predicting civil conﬂicts.” Journal of Peace Research 47(4):363–375.

Wolpert, David H and William G Macready. 1997. “No free lunch theorems for optimization.” Evolu-

tionary Computation, IEEE Transactions on 1(1):67–82.

Yao, Limin, David Mimno and Andrew McCallum. 2009. Efﬁcient methods for topic model inference on
streaming document collections. In Proceedings of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining. ACM pp. 937–946.

Zhou, Tianyi, Jeff Bilmes and Carlos Guestrin. 2014. “Divide-and-Conquer Learning by Anchoring a

Conical Hull.” arXiv preprint arXiv:1406.5752 .

43

Figure 1: Five example topics from the reference model. These are given the labels Supreme Court,
Cheney, Global Warming, Iran/N.K. Nukes, and Wright respectively.

44

Topic 18:  law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,feder, requir, amend, protect, gun, govern, allow, appeal, citizenTopic 30:  presid, vice, cheney, offic, presidenti, first, execut, dick, decis, leader,role, histori, nation, branch, power, part, govern, order, idea, washingtonTopic 48:  global, warm, research, climat, studi, chang, scienc, scientist, gore, caus,human, scientif, earth, emiss, planet, cell, environment, report, water, greenTopic 60:  iran, nuclear, threat, weapon, iranian, program, missil, north, bomb, defens,korea, strike, sanction, intern, build, militari, intellig, capabl, pose,developTopic 71:  black, wright, white, race, church, racial, racist, pastor, jeremiah,africanamerican, racism, african, comment, reverend, king, controversi, rev,view, communiti, southFigure 2: Relation between three measures of topic similarity across all topics and modes. Plotted
surface is a kernel smoothed density estimate.

45

02468100.00.51.01.52.0L1 and Top WordsTop 10 Words in CommonL1 Distance02468100.00.51.01.52.0L1 and Top DocsTop 10 Docs in CommonL1 Distance02468100246810Words and DocsTop 10 Docs in CommonTop 10 Words in CommonFigure 3: Comparison between the approximation to the bound on the marginal likelihood (the objective
function) with similarity metrics aggregated to the model level.

46

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.40.50.60.70.80.91.0Bound and WordsBoundProportion of 10 Words in Commonlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.40.60.81.0Bound and DocsBoundProportion of 10 Docs in Commonlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.00.20.40.60.81.01.2Bound and L1BoundExpected L1 DistFigure 4: Aggregation of similarity metrics to the topic level.

47

Word Expectations over RunsExpected Common WordsFrequency2468051015CourtCheneyClimateIran/N.K.WrightDoc Expectations over RunsExpected Common DocsFrequency0246802468101214CourtCheneyClimateIran/N.K.WrightL1 Expectations over RunsExpected DistFrequency0.51.01.50246810CourtCheneyClimateIran/N.K.WrightFigure 5: Differences in topical coverage by rating (controlling for time). Effects to the right of 0
indicate a topic more heavily used by Conservatives. Lines indicate 95% conﬁdence intervals using the
“global” approximation to measurement uncertainty (Roberts et al., 2014).

48

l−0.006−0.004−0.0020.0000.0020.0040.006Partisan Rating Effects by TopicExpected Difference in Topic Proportion (Conservative minus Liberal)lWrightlIran/North Korea NukelGlobal WarminglCheneylSupreme CourtFigure 6: The ﬁrst 300 characters of the three posts most associated with the global warming topic.
Posts 1 and 3 come from American Thinker and post 2 comes from Think Progress.

49

Deathly news for the religion of Global Warming. Looks like at least oneprominent scientific group has changed its mind about the irrefutability ofevidence regarding man made climate change.The American Physical Societyrepresenting nearly 50,000 physicists "has reversed its stance on climateClimate change report forecasts global sea levels to rise up to 4 feet by 2100.According to a new report led by the U.S. Geological Survey, the U.S. â€œfacesthe possibility of much more rapid climate change by the end of the centurythan previous studies have suggested.â€(cid:157) The report,NASA has confirmed that a developing natural climate pattern will likely resultin much colder temperatures.Â(cid:160) Of course, the climate alarmists' favoritedubious data source was also quick to point out that such natural phenomenashould not confuse the issue of manmade greenhouse gas induced globalFigure 7: Distribution of the partisan rating effect across modes for the ﬁve example topics. The black
solid line shows the effect at the reference mode and the black dashed line marks an effect size of 0.

50

Supreme CourtRating EffectFrequency−0.008−0.006−0.004−0.0020.0000.002020406080No effectReference modelCheneyRating EffectFrequency−0.010−0.0050.000020406080Global WarmingRating EffectFrequency0.0000.0010.0020.0030.0040.0050.006020406080100Iran/N.K. NukesRating EffectFrequency−0.010−0.0050.0000.0050.010050100150Rev. WrightRating EffectFrequency−0.0020.0000.0020.0040.006020406080100120Figure 8: Comparison of metric based on cosine similarity.

51

02468100.00.20.40.60.81.0Cosine Sim. and Top WordsTop 10 Words in CommonCosine Similarity02468100.00.20.40.60.81.0Cosine Sim. and Top DocsTop 10 Docs in CommonCosine Similarity02468100246810Words and DocsTop 10 Docs in CommonTop 10 Words in CommonNavigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Even in traditional general linear models, there is also always the choice of model speciﬁcation in both

variables and functional form. Although multimodality brings new issues to the table, the responsibility

of the researcher to carefully validate the chosen model is fundamentally the same. This is true regardless

of whether the choice between competing models arises due to a non-convex latent variable model or

due to the selection of an important model tuning parameter in a globally convex problem. Thus even if

multimodality is an unfamiliar problem, social scientists can draw on the same set of best practices that

they employ throughout their research.

An important practical contribution of this chapter is that it extends the set of tools available to

scholars using topic models in applied research. While we have focused on STM, many of the procedures

we use will be helpful for a broader class of latent variable models. For instance, the approaches to

aligning topics and calculating stability across runs can all be applied directly to the broader class of

statistical topic models and with minor modiﬁcations to most latent variable models.

We see great potential for the analysis of “big” data in the social sciences, but rather than focus on

the data we have taken a more methodological focus. We think this has important implications for both

methodological development but also could structure the types of questions we ask, and the types of data

sets we seek to build. Methodologically, we think that there will be important advances in areas such

as optimal initialization strategies, which is especially important as our data sets grow in size. From an

applied perspective, users will be unlikely to want to wait for extended periods of time in order to get

even a single set of results. Advances in computational power needs to be matched with smart ways to

leverage that power. From a research design perspective, we think more focus should be put on bringing

greater structure to so called “unstructured” data. In the STM we focus on the inclusion of metadata

for modeling and hypothesis testing, but this is only one possible use. Can more direct supervision help

us with issues of multimodality? Of course, in the end, big data will be at its best when there is active

dialogue between those who pose the big question and those who might provide the big answers.

References
Airoldi, Edoardo M, David M Blei, Stephen E Fienberg and Eric P Xing. 2009. Mixed membership

stochastic blockmodels. In Advances in Neural Information Processing Systems. pp. 33–40.

Anandkumar, Anima, Rong Ge, Daniel Hsu and Sham M Kakade. 2013. “A tensor spectral approach to

learning mixed membership community models.” arXiv preprint arXiv:1302.2684 .

36

Anandkumar, Anima, Rong Ge, Daniel Hsu, Sham M Kakade and Matus Telgarsky. 2012. “Tensor

decompositions for learning latent variable models.” arXiv preprint arXiv:1210.7559 .

Anandkumar, Anima, Yi-kai Liu, Daniel J Hsu, Dean P Foster and Sham M Kakade. 2012. A spec-
tral algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems.
pp. 917–925.

Anandkumar, Animashree, Daniel Hsu, Adel Javanmard and Sham Kakade. 2013. Learning linear
bayesian networks with latent variables. In Proceedings of The 30th International Conference on Ma-
chine Learning. pp. 249–257.

Anandkumar, Animashree, Daniel Hsu and Sham M Kakade. 2012. “A method of moments for mixture

models and hidden Markov models.” arXiv preprint arXiv:1203.0683 .

Arora, Sanjeev, Aditya Bhaskara, Rong Ge and Tengyu Ma. 2013. “Provable bounds for learning some

deep representations.” arXiv preprint .

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2012. Learning topic models–going beyond svd. In Foun-

dations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE pp. 1–10.

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2013. “New algorithms for learning incoherent and over-

complete dictionaries.” arXiv preprint arXiv:1308.6273 .

Arora, Sanjeev, Rong Ge, Ankur Moitra and Sushant Sachdeva. 2012. Provable ICA with unknown
Gaussian noise, with implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems. pp. 2375–2383.

Arora, Sanjeev, Rong Ge, Ravindran Kannan and Ankur Moitra. 2012. Computing a nonnegative ma-
trix factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of
computing. ACM pp. 145–162.

Arora, Sanjeev, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu
and Michael Zhu. 2013. A Practical Algorithm for Topic Modeling with Provable Guarantees. In
Proceedings of The 30th International Conference on Machine Learning. pp. 280–288.

Arthur, David and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial
and Applied Mathematics pp. 1027–1035.

Asuncion Jr, Arthur Uy. 2011. Distributed and accelerated inference algorithms for probabilistic graph-

ical models. Technical report California State University at Long Beach.

Bahmani, Bahman, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei Vassilvitskii. 2012.

“Scalable k-means++.” Proceedings of the VLDB Endowment 5(7):622–633.

Beck, Nathaniel, Gary King and Langche Zeng. 2000. “Improving quantitative studies of international

conﬂict: A conjecture.” American Political Science Review pp. 21–35.

Belloni, Alexandre, Victor Chernozhukov and Christian Hansen. 2014. “High-Dimensional Methods and

Inference on Structural and Treatment Effects.” The Journal of Economic Perspectives 28(2):29–50.

37

Bishop, Christopher M et al. 2006. Pattern recognition and machine learning. Vol. 1 springer New York.

Blei, David M. 2012. “Probabilistic topic models.” Communications of the ACM 55(4):77–84.

Blei, David M. 2014. “Build, compute, critique, repeat: data analysis with latent variable models.”

Annual Review of Statistics and Its Application 1:203–232.

Blei, David M, Andrew Y Ng and Michael I Jordan. 2003. “Latent dirichlet allocation.” the Journal of

machine Learning research 3:993–1022.

Boyd, Stephen and Lieven Vandenberghe. 2009. Convex optimization. Cambridge university press.

Carpenter, Bob. 2010. Integrating out multinomial parameters in latent Dirichlet allocation and naive

Bayes for collapsed Gibbs sampling. Technical report Technical report, LingPipe.

Chaney, Allison June-Barlow and David M Blei. 2012. Visualizing Topic Models. In ICWSM.

Chang, Jonathan. 2012. lda: Collapsed Gibbs sampling methods for topic models. R package version

1.3.2.
URL: http://CRAN.R-project.org/package=lda

Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-graber and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In Advances in neural information processing systems.
pp. 288–296.

Chuang, Jason, Christopher D Manning and Jeffrey Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of the International Working Conference on Advanced
Visual Interfaces. ACM pp. 74–77.

Chuang, Jason, Sonal Gupta, Christopher Manning and Jeffrey Heer. 2013. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In Proceedings of the 30th International Conference
on Machine Learning (ICML-13). pp. 612–620.

Cochocki, A and Rolf Unbehauen. 1993. Neural networks for optimization and signal processing. John

Wiley & Sons, Inc.

Council, National Research. 2013. Frontiers in Massive Data Analysis. The National Academies Press.

de Klerk, Etienne and Dmitrii V Pasechnik. 2002. “Approximation of the stability number of a graph via

copositive programming.” SIAM Journal on Optimization 12(4):875–892.

De Marchi, Scott, Christopher Gelpi and Jeffrey D Grynaviski. 2004. “Untangling neural nets.” American

Political Science Review 98(02):371–378.

Dean, Jeffrey and Sanjay Ghemawat. 2008. “MapReduce: simpliﬁed data processing on large clusters.”

Communications of the ACM 51(1):107–113.

Deb, Partha and Pravin K Trivedi. 2002. “The structure of demand for health care: latent class versus

two-part models.” Journal of health economics 21(4):601–625.

Dempster, Arthur P, Nan M Laird and Donald B Rubin. 1977. “Maximum likelihood from incomplete
data via the EM algorithm.” Journal of the Royal Statistical Society. Series B (Methodological) pp. 1–
38.

38

Ding, Weicong, Mohammad H Rohban, Prakash Ishwar and Venkatesh Saligrama. 2013. “Topic discov-

ery through data dependent and random projections.” arXiv preprint arXiv:1303.3664 .

Ding, Weicong, Prakash Ishwar, Mohammad H Rohban and Venkatesh Saligrama. 2013. “Necessary
and Sufﬁcient Conditions for Novel Word Detection in Separable Topic Models.” arXiv preprint
arXiv:1310.7994 .

Donoho, David and Victoria Stodden. 2003. When does non-negative matrix factorization give a correct

decomposition into parts? In Advances in neural information processing systems. p. None.

Doshi-Velez, Finale, Yaorong Ge and Isaac Kohane. 2014. “Comorbidity clusters in autism spectrum

disorders: an electronic health record time-series analysis.” Pediatrics 133(1):e54–e63.

Drton, Mathias and Thomas S Richardson. 2004. “Multimodality of the likelihood in the bivariate

seemingly unrelated regressions model.” Biometrika 91(2):383–392.

DuMouchel, William. 1999. “Bayesian data mining in large frequency tables, with an application to the

FDA spontaneous reporting system.” The American Statistician 53(3):177–190.

Efron, Bradley et al. 1978. “The geometry of exponential families.” The Annals of Statistics 6(2):362–

376.

Eisenstein, Jacob and Eric Xing. 2010. “The CMU 2008 Political Blog Corpus.”.

Fan, Jianqing, Fang Han and Han Liu. 2014. “Challenges of Big Data analysis.” National Science Review

p. nwt032.

Foulds, J. R. and P. Smyth. 2014. Annealing Paths for the Evaluation of Topic Models. In Proceedings

of the Thirtieth Conference Conference on Uncertainty in Artiﬁcial Intelligence.

Gardner, Matthew J, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric Ringger and Kevin Seppi.
2010. The topic browser: An interactive tool for browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.

Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari and Donald B Rubin. 2013.

Bayesian data analysis. CRC press.

Gillis, Nicolas. 2014.

arXiv:1401.5226 .

“The why and how of nonnegative matrix factorization.” arXiv preprint

Gillis, Nicolas and Robert Luce. 2013. “Robust near-separable nonnegative matrix factorization using

linear optimization.” arXiv preprint arXiv:1302.4385 .

Goldstone, AndrewUnderwood et al. 2014. “The Quiet Transformations of Literary Studies: What

Thirteen Thousand Scholars Could Tell Us.”.

Grifﬁths, DMBTL and MIJJB Tenenbaum. 2004. “Hierarchical topic models and the nested Chinese

restaurant process.” Advances in neural information processing systems 16:17.

Grifﬁths, Thomas L and Mark Steyvers. 2004. “Finding scientiﬁc topics.” Proceedings of the National

academy of Sciences of the United States of America 101(Suppl 1):5228–5235.

39

Grimmer, Justin. 2010a. “A Bayesian hierarchical topic model for political texts: Measuring expressed

agendas in Senate press releases.” Political Analysis 18(1):1–35.

Grimmer, Justin. 2010b. “An introduction to Bayesian inference via variational approximations.” Politi-

cal Analysis p. mpq027.

Grimmer, Justin. 2013. Representational Style in Congress: What Legislators Say and why it Matters.

Cambridge University Press.

Grimmer, Justin and Brandon M Stewart. 2013. “Text as data: The promise and pitfalls of automatic

content analysis methods for political texts.” Political Analysis p. mps028.

Grimmer, Justin and Gary King. 2011. “General purpose computer-assisted clustering and conceptual-

ization.” Proceedings of the National Academy of Sciences 108(7):2643–2650.

Hambleton, Ronald K. 1991. Fundamentals of item response theory. Vol. 2 Sage publications.

Hamilton, James D. 1989. “A new approach to the economic analysis of nonstationary time series and

the business cycle.” Econometrica: Journal of the Econometric Society pp. 357–384.

Hoff, Peter D, Adrian E Raftery and Mark S Handcock. 2002. “Latent space approaches to social network

analysis.” Journal of the american Statistical association 97(460):1090–1098.

Hopkins, Daniel J and Gary King. 2010. “A method of automated nonparametric content analysis for

social science.” American Journal of Political Science 54(1):229–247.

Hornik, Kurt. 2005. “A clue for cluster ensembles.” Journal of Statistical Software 14(12).

Hsu, Daniel and Sham M Kakade. 2013. Learning mixtures of spherical gaussians: moment methods and
spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer
Science. ACM pp. 11–20.

Hu, Yuening, Jordan Boyd-Graber, Brianna Satinoff and Alison Smith. 2014. “Interactive topic model-

ing.” Machine Learning 95(3):423–469.

Huang, Furong, UN Niranjan, M Hakeem and Animashree Anandkumar. 2013. “Fast Detection of

Overlapping Communities via Online Tensor Methods.”.

Jordan, Michael I, Zoubin Ghahramani, Tommi S Jaakkola and Lawrence K Saul. 1998. An introduction

to variational methods for graphical models. Springer.

Kalai, Adam Tauman, Ankur Moitra and Gregory Valiant. 2012. “Disentangling gaussians.” Communi-

cations of the ACM 55(2):113–120.

Kass, Robert E and Adrian E Raftery. 1995. “Bayes factors.” Journal of the american statistical associ-

ation 90(430):773–795.

King, Gary. 1989. “Unifying political methodology.” New York: Cambridge .

King, Gary, Jennifer Pan and Margaret E Roberts. 2013. “How censorship in China allows government

criticism but silences collective expression.” American Political Science Review 107(02):326–343.

40

Kivinen, Jyrki and Manfred K Warmuth. 1997. “Exponentiated gradient versus gradient descent for

linear predictors.” Information and Computation 132(1):1–63.

Koltcov, Sergei, Olessia Koltsova and Sergey Nikolenko. 2014. Latent dirichlet allocation: stability and
applications to studies of user-generated content. In Proceedings of the 2014 ACM conference on Web
science. ACM pp. 161–165.

Krippendorff, Klaus. 2012. Content analysis: An introduction to its methodology. Sage.

Kuhn, Harold W. 1955. “The Hungarian method for the assignment problem.” Naval research logistics

quarterly 2(1-2):83–97.

Kumar, Abhishek, Vikas Sindhwani and Prabhanjan Kambadur. 2012. “Fast conical hull algorithms for

near-separable non-negative matrix factorization.” arXiv preprint arXiv:1210.1190 .

Lancichinetti, Andrea, M Irmak Sirer, Jane X Wang, Daniel Acuna, Konrad K¨ording and Lu´ıs A Nunes
Amaral. 2014. “A high-reproducibility and high-accuracy method for automated topic classiﬁcation.”
arXiv preprint arXiv:1402.0422 .

Lau, Jey Han, David Newman and Timothy Baldwin. 2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality. In Proceedings of the European Chapter of the
Association for Computational Linguistics.

Lauderdale, Benjamin E and Tom S Clark. 2014. “Scaling politically meaningful dimensions using texts

and votes.” American Journal of Political Science .

Lazer, David, Alex Sandy Pentland, Lada Adamic, Sinan Aral, Albert Laszlo Barabasi, Devon Brewer,
Nicholas Christakis, Noshir Contractor, James Fowler, Myron Gutmann et al. 2009. “Life in the
network: the coming age of computational social science.” Science (New York, NY) 323(5915):721.

Lim, Yew Jin and Yee Whye Teh. 2007. Variational Bayesian approach to movie rating prediction. In

Proceedings of KDD Cup and Workshop. Vol. 7 Citeseer pp. 15–21.

Lloyd, Stuart. 1982. “Least squares quantization in PCM.” Information Theory, IEEE Transactions on

28(2):129–137.

Mahajan, Meena, Prajakta Nimbhorkar and Kasturi Varadarajan. 2009. The planar k-means problem is

NP-hard. In WALCOM: Algorithms and Computation. Springer pp. 274–285.

McLachlan, Geoffrey and David Peel. 2004. Finite mixture models. John Wiley & Sons.

Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality. In Advances in Neural Information Processing
Systems. pp. 3111–3119.

Mimno, David and David Blei. 2011. Bayesian checking for topic models. In Proceedings of the Con-
ference on Empirical Methods in Natural Language Processing. Association for Computational Lin-
guistics pp. 227–237.

Mimno, David, Hanna M Wallach, Edmund Talley, Miriam Leenders and Andrew McCallum. 2011. Op-
timizing semantic coherence in topic models. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing. Association for Computational Linguistics pp. 262–272.

41

Mullainathan, Sendhil. 2014. “What Big Data Means for Social Science.” Behavioral and Experimental

Seminar.

Murphy, Kevin P. 2012. Machine learning: a probabilistic perspective. MIT press.

Nguyen, Thang, Yuening Hu and Jordan Boyd-Graber. 2014. Anchors Regularized: Adding Robustness
and Extensibility to Scalable Topic-Modeling Algorithms. In Association for Computational Linguis-
tics.

Nielsen, Frank and Richard Nock. 2014. “Further heuristics for k-means: The merge-and-split heuristic

and the (k, l)-means.” arXiv preprint arXiv:1406.6314 .

O’Connor, Brendan. 2014. MiTextExplorer: Linked brushing and mutual information for exploratory
text data analysis. In Proceedings of the Workshop on Interactive Language Learning, Visualization,
and Interfaces. Association for Computational Linguistics pp. 1–13.

OConnor, Brendan, Brandon M Stewart and Noah A Smith. 2013. Learning to Extract International

Relations from Political Context. In Proceedings of ACL.

Papadimitriou, Christos H and Kenneth Steiglitz. 1998. Combinatorial optimization: algorithms and

complexity. Courier Dover Publications.

Park, Jong Hee. 2012. “A Uniﬁed Method for Dynamic and Cross-Sectional Heterogeneity: Introducing

Hidden Markov Panel Models.” American Journal of Political Science 56(4):1040–1054.

Pearson, Karl. 1894. “Contributions to the mathematical theory of evolution.” Philosophical Transac-

tions of the Royal Society of London. A pp. 71–110.

Poole, Keith T and Howard Rosenthal. 1997. Congress: A political-economic history of roll call voting.

Oxford University Press.

Quinn, Kevin M, Burt L Monroe, Michael Colaresi, Michael H Crespin and Dragomir R Radev. 2010.
“How to analyze political attention with minimal assumptions and costs.” American Journal of Politi-
cal Science 54(1):209–228.

Rabiner, Lawrence and Biing-Hwang Juang. 1986. “An introduction to hidden Markov models.” ASSP

Magazine, IEEE 3(1):4–16.

Recht, Ben, Christopher Re, Joel Tropp and Victor Bittorf. 2012. Factoring nonnegative matrices with

linear programs. In Advances in Neural Information Processing Systems. pp. 1214–1222.

Reich, Justin, Dustin Tingley, Jetson Leder-Luis, Margaret E Roberts and Brandon M Stewart. N.d.
Computer Assisted Reading and Discovery for Student Generated Text. Technical report Working
Paper. Export.

Rivers, Douglas. 2003. “Identiﬁcation of multidimensional spatial voting models.” Typescript. Stanford

University .

Robert, Christian P and George Casella. 2004. Monte Carlo statistical methods. Vol. 319 Springer New

York.

42

Roberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis,
Shana Kushner Gadarian, Bethany Albertson and David G Rand. 2014. “Structural Topic Models
for Open-Ended Survey Responses.” American Journal of Political Science .

Ruiz, Francisco J. R., Isabel Valera, Carlos Blanco and Fernando Perez-Cruz. 2014. “Bayesian Non-
parametric Comorbidity Analysis of Psychiatric Disorders.” Journal of Machine Learning Research
15:1215–1247.
URL: http://jmlr.org/papers/v15/ruiz14a.html

Shneiderman, Ben. 1996. The eyes have it: A task by data type taxonomy for information visualizations.

In Visual Languages, 1996. Proceedings., IEEE Symposium on. IEEE pp. 336–343.

Song, Le, Eric P Xing and Ankur P Parikh. 2011. A spectral algorithm for latent tree graphical models. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11). pp. 1065–1072.

Sontag, David and Dan Roy. 2011. Complexity of inference in latent dirichlet allocation. In Advances in

Neural Information Processing Systems. pp. 1008–1016.

Srivastava, Virendera K and David EA Giles. 1987. Seemingly unrelated regression equations models:

estimation and inference. Vol. 80 CRC Press.

Teh, Yee Whye, Michael I Jordan, Matthew J Beal and David M Blei. 2006. “Hierarchical dirichlet

processes.” Journal of the american statistical association 101(476).

Tenenbaum, Joshua B, Charles Kemp, Thomas L Grifﬁths and Noah D Goodman. 2011. “How to grow

a mind: Statistics, structure, and abstraction.” science 331(6022):1279–1285.

Tibshirani, Robert. 1996. “Regression shrinkage and selection via the lasso.” Journal of the Royal Sta-

tistical Society. Series B (Methodological) pp. 267–288.

Vavasis, Stephen A. 2009. “On the complexity of nonnegative matrix factorization.” SIAM Journal on

Optimization 20(3):1364–1377.

Wallach, Hanna M, Iain Murray, Ruslan Salakhutdinov and David Mimno. 2009. Evaluation methods
for topic models. In Proceedings of the 26th Annual International Conference on Machine Learning.
ACM pp. 1105–1112.

Ward, Michael D, Brian D Greenhill and Kristin M Bakke. 2010. “The perils of policy by p-value:

Predicting civil conﬂicts.” Journal of Peace Research 47(4):363–375.

Wolpert, David H and William G Macready. 1997. “No free lunch theorems for optimization.” Evolu-

tionary Computation, IEEE Transactions on 1(1):67–82.

Yao, Limin, David Mimno and Andrew McCallum. 2009. Efﬁcient methods for topic model inference on
streaming document collections. In Proceedings of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining. ACM pp. 937–946.

Zhou, Tianyi, Jeff Bilmes and Carlos Guestrin. 2014. “Divide-and-Conquer Learning by Anchoring a

Conical Hull.” arXiv preprint arXiv:1406.5752 .

43

Figure 1: Five example topics from the reference model. These are given the labels Supreme Court,
Cheney, Global Warming, Iran/N.K. Nukes, and Wright respectively.

44

Topic 18:  law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,feder, requir, amend, protect, gun, govern, allow, appeal, citizenTopic 30:  presid, vice, cheney, offic, presidenti, first, execut, dick, decis, leader,role, histori, nation, branch, power, part, govern, order, idea, washingtonTopic 48:  global, warm, research, climat, studi, chang, scienc, scientist, gore, caus,human, scientif, earth, emiss, planet, cell, environment, report, water, greenTopic 60:  iran, nuclear, threat, weapon, iranian, program, missil, north, bomb, defens,korea, strike, sanction, intern, build, militari, intellig, capabl, pose,developTopic 71:  black, wright, white, race, church, racial, racist, pastor, jeremiah,africanamerican, racism, african, comment, reverend, king, controversi, rev,view, communiti, southFigure 2: Relation between three measures of topic similarity across all topics and modes. Plotted
surface is a kernel smoothed density estimate.

45

02468100.00.51.01.52.0L1 and Top WordsTop 10 Words in CommonL1 Distance02468100.00.51.01.52.0L1 and Top DocsTop 10 Docs in CommonL1 Distance02468100246810Words and DocsTop 10 Docs in CommonTop 10 Words in CommonFigure 3: Comparison between the approximation to the bound on the marginal likelihood (the objective
function) with similarity metrics aggregated to the model level.

46

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.40.50.60.70.80.91.0Bound and WordsBoundProportion of 10 Words in Commonlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.40.60.81.0Bound and DocsBoundProportion of 10 Docs in Commonlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.00.20.40.60.81.01.2Bound and L1BoundExpected L1 DistFigure 4: Aggregation of similarity metrics to the topic level.

47

Word Expectations over RunsExpected Common WordsFrequency2468051015CourtCheneyClimateIran/N.K.WrightDoc Expectations over RunsExpected Common DocsFrequency0246802468101214CourtCheneyClimateIran/N.K.WrightL1 Expectations over RunsExpected DistFrequency0.51.01.50246810CourtCheneyClimateIran/N.K.WrightFigure 5: Differences in topical coverage by rating (controlling for time). Effects to the right of 0
indicate a topic more heavily used by Conservatives. Lines indicate 95% conﬁdence intervals using the
“global” approximation to measurement uncertainty (Roberts et al., 2014).

48

l−0.006−0.004−0.0020.0000.0020.0040.006Partisan Rating Effects by TopicExpected Difference in Topic Proportion (Conservative minus Liberal)lWrightlIran/North Korea NukelGlobal WarminglCheneylSupreme CourtFigure 6: The ﬁrst 300 characters of the three posts most associated with the global warming topic.
Posts 1 and 3 come from American Thinker and post 2 comes from Think Progress.

49

Deathly news for the religion of Global Warming. Looks like at least oneprominent scientific group has changed its mind about the irrefutability ofevidence regarding man made climate change.The American Physical Societyrepresenting nearly 50,000 physicists "has reversed its stance on climateClimate change report forecasts global sea levels to rise up to 4 feet by 2100.According to a new report led by the U.S. Geological Survey, the U.S. â€œfacesthe possibility of much more rapid climate change by the end of the centurythan previous studies have suggested.â€(cid:157) The report,NASA has confirmed that a developing natural climate pattern will likely resultin much colder temperatures.Â(cid:160) Of course, the climate alarmists' favoritedubious data source was also quick to point out that such natural phenomenashould not confuse the issue of manmade greenhouse gas induced globalFigure 7: Distribution of the partisan rating effect across modes for the ﬁve example topics. The black
solid line shows the effect at the reference mode and the black dashed line marks an effect size of 0.

50

Supreme CourtRating EffectFrequency−0.008−0.006−0.004−0.0020.0000.002020406080No effectReference modelCheneyRating EffectFrequency−0.010−0.0050.000020406080Global WarmingRating EffectFrequency0.0000.0010.0020.0030.0040.0050.006020406080100Iran/N.K. NukesRating EffectFrequency−0.010−0.0050.0000.0050.010050100150Rev. WrightRating EffectFrequency−0.0020.0000.0020.0040.006020406080100120Figure 8: Comparison of metric based on cosine similarity.

51

02468100.00.20.40.60.81.0Cosine Sim. and Top WordsTop 10 Words in CommonCosine Similarity02468100.00.20.40.60.81.0Cosine Sim. and Top DocsTop 10 Docs in CommonCosine Similarity02468100246810Words and DocsTop 10 Docs in CommonTop 10 Words in CommonFigure 9: A comparison of initialization strategies for the K = 100 STM models.

52

Comparing Initialization StrategiesLower Bound at ConvergenceDensity−17960000−17920000−17880000−178400000e+001e−052e−053e−054e−05RandomLDANavigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Even in traditional general linear models, there is also always the choice of model speciﬁcation in both

variables and functional form. Although multimodality brings new issues to the table, the responsibility

of the researcher to carefully validate the chosen model is fundamentally the same. This is true regardless

of whether the choice between competing models arises due to a non-convex latent variable model or

due to the selection of an important model tuning parameter in a globally convex problem. Thus even if

multimodality is an unfamiliar problem, social scientists can draw on the same set of best practices that

they employ throughout their research.

An important practical contribution of this chapter is that it extends the set of tools available to

scholars using topic models in applied research. While we have focused on STM, many of the procedures

we use will be helpful for a broader class of latent variable models. For instance, the approaches to

aligning topics and calculating stability across runs can all be applied directly to the broader class of

statistical topic models and with minor modiﬁcations to most latent variable models.

We see great potential for the analysis of “big” data in the social sciences, but rather than focus on

the data we have taken a more methodological focus. We think this has important implications for both

methodological development but also could structure the types of questions we ask, and the types of data

sets we seek to build. Methodologically, we think that there will be important advances in areas such

as optimal initialization strategies, which is especially important as our data sets grow in size. From an

applied perspective, users will be unlikely to want to wait for extended periods of time in order to get

even a single set of results. Advances in computational power needs to be matched with smart ways to

leverage that power. From a research design perspective, we think more focus should be put on bringing

greater structure to so called “unstructured” data. In the STM we focus on the inclusion of metadata

for modeling and hypothesis testing, but this is only one possible use. Can more direct supervision help

us with issues of multimodality? Of course, in the end, big data will be at its best when there is active

dialogue between those who pose the big question and those who might provide the big answers.

References
Airoldi, Edoardo M, David M Blei, Stephen E Fienberg and Eric P Xing. 2009. Mixed membership

stochastic blockmodels. In Advances in Neural Information Processing Systems. pp. 33–40.

Anandkumar, Anima, Rong Ge, Daniel Hsu and Sham M Kakade. 2013. “A tensor spectral approach to

learning mixed membership community models.” arXiv preprint arXiv:1302.2684 .

36

Anandkumar, Anima, Rong Ge, Daniel Hsu, Sham M Kakade and Matus Telgarsky. 2012. “Tensor

decompositions for learning latent variable models.” arXiv preprint arXiv:1210.7559 .

Anandkumar, Anima, Yi-kai Liu, Daniel J Hsu, Dean P Foster and Sham M Kakade. 2012. A spec-
tral algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems.
pp. 917–925.

Anandkumar, Animashree, Daniel Hsu, Adel Javanmard and Sham Kakade. 2013. Learning linear
bayesian networks with latent variables. In Proceedings of The 30th International Conference on Ma-
chine Learning. pp. 249–257.

Anandkumar, Animashree, Daniel Hsu and Sham M Kakade. 2012. “A method of moments for mixture

models and hidden Markov models.” arXiv preprint arXiv:1203.0683 .

Arora, Sanjeev, Aditya Bhaskara, Rong Ge and Tengyu Ma. 2013. “Provable bounds for learning some

deep representations.” arXiv preprint .

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2012. Learning topic models–going beyond svd. In Foun-

dations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE pp. 1–10.

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2013. “New algorithms for learning incoherent and over-

complete dictionaries.” arXiv preprint arXiv:1308.6273 .

Arora, Sanjeev, Rong Ge, Ankur Moitra and Sushant Sachdeva. 2012. Provable ICA with unknown
Gaussian noise, with implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems. pp. 2375–2383.

Arora, Sanjeev, Rong Ge, Ravindran Kannan and Ankur Moitra. 2012. Computing a nonnegative ma-
trix factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of
computing. ACM pp. 145–162.

Arora, Sanjeev, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu
and Michael Zhu. 2013. A Practical Algorithm for Topic Modeling with Provable Guarantees. In
Proceedings of The 30th International Conference on Machine Learning. pp. 280–288.

Arthur, David and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial
and Applied Mathematics pp. 1027–1035.

Asuncion Jr, Arthur Uy. 2011. Distributed and accelerated inference algorithms for probabilistic graph-

ical models. Technical report California State University at Long Beach.

Bahmani, Bahman, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei Vassilvitskii. 2012.

“Scalable k-means++.” Proceedings of the VLDB Endowment 5(7):622–633.

Beck, Nathaniel, Gary King and Langche Zeng. 2000. “Improving quantitative studies of international

conﬂict: A conjecture.” American Political Science Review pp. 21–35.

Belloni, Alexandre, Victor Chernozhukov and Christian Hansen. 2014. “High-Dimensional Methods and

Inference on Structural and Treatment Effects.” The Journal of Economic Perspectives 28(2):29–50.

37

Bishop, Christopher M et al. 2006. Pattern recognition and machine learning. Vol. 1 springer New York.

Blei, David M. 2012. “Probabilistic topic models.” Communications of the ACM 55(4):77–84.

Blei, David M. 2014. “Build, compute, critique, repeat: data analysis with latent variable models.”

Annual Review of Statistics and Its Application 1:203–232.

Blei, David M, Andrew Y Ng and Michael I Jordan. 2003. “Latent dirichlet allocation.” the Journal of

machine Learning research 3:993–1022.

Boyd, Stephen and Lieven Vandenberghe. 2009. Convex optimization. Cambridge university press.

Carpenter, Bob. 2010. Integrating out multinomial parameters in latent Dirichlet allocation and naive

Bayes for collapsed Gibbs sampling. Technical report Technical report, LingPipe.

Chaney, Allison June-Barlow and David M Blei. 2012. Visualizing Topic Models. In ICWSM.

Chang, Jonathan. 2012. lda: Collapsed Gibbs sampling methods for topic models. R package version

1.3.2.
URL: http://CRAN.R-project.org/package=lda

Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-graber and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In Advances in neural information processing systems.
pp. 288–296.

Chuang, Jason, Christopher D Manning and Jeffrey Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of the International Working Conference on Advanced
Visual Interfaces. ACM pp. 74–77.

Chuang, Jason, Sonal Gupta, Christopher Manning and Jeffrey Heer. 2013. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In Proceedings of the 30th International Conference
on Machine Learning (ICML-13). pp. 612–620.

Cochocki, A and Rolf Unbehauen. 1993. Neural networks for optimization and signal processing. John

Wiley & Sons, Inc.

Council, National Research. 2013. Frontiers in Massive Data Analysis. The National Academies Press.

de Klerk, Etienne and Dmitrii V Pasechnik. 2002. “Approximation of the stability number of a graph via

copositive programming.” SIAM Journal on Optimization 12(4):875–892.

De Marchi, Scott, Christopher Gelpi and Jeffrey D Grynaviski. 2004. “Untangling neural nets.” American

Political Science Review 98(02):371–378.

Dean, Jeffrey and Sanjay Ghemawat. 2008. “MapReduce: simpliﬁed data processing on large clusters.”

Communications of the ACM 51(1):107–113.

Deb, Partha and Pravin K Trivedi. 2002. “The structure of demand for health care: latent class versus

two-part models.” Journal of health economics 21(4):601–625.

Dempster, Arthur P, Nan M Laird and Donald B Rubin. 1977. “Maximum likelihood from incomplete
data via the EM algorithm.” Journal of the Royal Statistical Society. Series B (Methodological) pp. 1–
38.

38

Ding, Weicong, Mohammad H Rohban, Prakash Ishwar and Venkatesh Saligrama. 2013. “Topic discov-

ery through data dependent and random projections.” arXiv preprint arXiv:1303.3664 .

Ding, Weicong, Prakash Ishwar, Mohammad H Rohban and Venkatesh Saligrama. 2013. “Necessary
and Sufﬁcient Conditions for Novel Word Detection in Separable Topic Models.” arXiv preprint
arXiv:1310.7994 .

Donoho, David and Victoria Stodden. 2003. When does non-negative matrix factorization give a correct

decomposition into parts? In Advances in neural information processing systems. p. None.

Doshi-Velez, Finale, Yaorong Ge and Isaac Kohane. 2014. “Comorbidity clusters in autism spectrum

disorders: an electronic health record time-series analysis.” Pediatrics 133(1):e54–e63.

Drton, Mathias and Thomas S Richardson. 2004. “Multimodality of the likelihood in the bivariate

seemingly unrelated regressions model.” Biometrika 91(2):383–392.

DuMouchel, William. 1999. “Bayesian data mining in large frequency tables, with an application to the

FDA spontaneous reporting system.” The American Statistician 53(3):177–190.

Efron, Bradley et al. 1978. “The geometry of exponential families.” The Annals of Statistics 6(2):362–

376.

Eisenstein, Jacob and Eric Xing. 2010. “The CMU 2008 Political Blog Corpus.”.

Fan, Jianqing, Fang Han and Han Liu. 2014. “Challenges of Big Data analysis.” National Science Review

p. nwt032.

Foulds, J. R. and P. Smyth. 2014. Annealing Paths for the Evaluation of Topic Models. In Proceedings

of the Thirtieth Conference Conference on Uncertainty in Artiﬁcial Intelligence.

Gardner, Matthew J, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric Ringger and Kevin Seppi.
2010. The topic browser: An interactive tool for browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.

Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari and Donald B Rubin. 2013.

Bayesian data analysis. CRC press.

Gillis, Nicolas. 2014.

arXiv:1401.5226 .

“The why and how of nonnegative matrix factorization.” arXiv preprint

Gillis, Nicolas and Robert Luce. 2013. “Robust near-separable nonnegative matrix factorization using

linear optimization.” arXiv preprint arXiv:1302.4385 .

Goldstone, AndrewUnderwood et al. 2014. “The Quiet Transformations of Literary Studies: What

Thirteen Thousand Scholars Could Tell Us.”.

Grifﬁths, DMBTL and MIJJB Tenenbaum. 2004. “Hierarchical topic models and the nested Chinese

restaurant process.” Advances in neural information processing systems 16:17.

Grifﬁths, Thomas L and Mark Steyvers. 2004. “Finding scientiﬁc topics.” Proceedings of the National

academy of Sciences of the United States of America 101(Suppl 1):5228–5235.

39

Grimmer, Justin. 2010a. “A Bayesian hierarchical topic model for political texts: Measuring expressed

agendas in Senate press releases.” Political Analysis 18(1):1–35.

Grimmer, Justin. 2010b. “An introduction to Bayesian inference via variational approximations.” Politi-

cal Analysis p. mpq027.

Grimmer, Justin. 2013. Representational Style in Congress: What Legislators Say and why it Matters.

Cambridge University Press.

Grimmer, Justin and Brandon M Stewart. 2013. “Text as data: The promise and pitfalls of automatic

content analysis methods for political texts.” Political Analysis p. mps028.

Grimmer, Justin and Gary King. 2011. “General purpose computer-assisted clustering and conceptual-

ization.” Proceedings of the National Academy of Sciences 108(7):2643–2650.

Hambleton, Ronald K. 1991. Fundamentals of item response theory. Vol. 2 Sage publications.

Hamilton, James D. 1989. “A new approach to the economic analysis of nonstationary time series and

the business cycle.” Econometrica: Journal of the Econometric Society pp. 357–384.

Hoff, Peter D, Adrian E Raftery and Mark S Handcock. 2002. “Latent space approaches to social network

analysis.” Journal of the american Statistical association 97(460):1090–1098.

Hopkins, Daniel J and Gary King. 2010. “A method of automated nonparametric content analysis for

social science.” American Journal of Political Science 54(1):229–247.

Hornik, Kurt. 2005. “A clue for cluster ensembles.” Journal of Statistical Software 14(12).

Hsu, Daniel and Sham M Kakade. 2013. Learning mixtures of spherical gaussians: moment methods and
spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer
Science. ACM pp. 11–20.

Hu, Yuening, Jordan Boyd-Graber, Brianna Satinoff and Alison Smith. 2014. “Interactive topic model-

ing.” Machine Learning 95(3):423–469.

Huang, Furong, UN Niranjan, M Hakeem and Animashree Anandkumar. 2013. “Fast Detection of

Overlapping Communities via Online Tensor Methods.”.

Jordan, Michael I, Zoubin Ghahramani, Tommi S Jaakkola and Lawrence K Saul. 1998. An introduction

to variational methods for graphical models. Springer.

Kalai, Adam Tauman, Ankur Moitra and Gregory Valiant. 2012. “Disentangling gaussians.” Communi-

cations of the ACM 55(2):113–120.

Kass, Robert E and Adrian E Raftery. 1995. “Bayes factors.” Journal of the american statistical associ-

ation 90(430):773–795.

King, Gary. 1989. “Unifying political methodology.” New York: Cambridge .

King, Gary, Jennifer Pan and Margaret E Roberts. 2013. “How censorship in China allows government

criticism but silences collective expression.” American Political Science Review 107(02):326–343.

40

Kivinen, Jyrki and Manfred K Warmuth. 1997. “Exponentiated gradient versus gradient descent for

linear predictors.” Information and Computation 132(1):1–63.

Koltcov, Sergei, Olessia Koltsova and Sergey Nikolenko. 2014. Latent dirichlet allocation: stability and
applications to studies of user-generated content. In Proceedings of the 2014 ACM conference on Web
science. ACM pp. 161–165.

Krippendorff, Klaus. 2012. Content analysis: An introduction to its methodology. Sage.

Kuhn, Harold W. 1955. “The Hungarian method for the assignment problem.” Naval research logistics

quarterly 2(1-2):83–97.

Kumar, Abhishek, Vikas Sindhwani and Prabhanjan Kambadur. 2012. “Fast conical hull algorithms for

near-separable non-negative matrix factorization.” arXiv preprint arXiv:1210.1190 .

Lancichinetti, Andrea, M Irmak Sirer, Jane X Wang, Daniel Acuna, Konrad K¨ording and Lu´ıs A Nunes
Amaral. 2014. “A high-reproducibility and high-accuracy method for automated topic classiﬁcation.”
arXiv preprint arXiv:1402.0422 .

Lau, Jey Han, David Newman and Timothy Baldwin. 2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality. In Proceedings of the European Chapter of the
Association for Computational Linguistics.

Lauderdale, Benjamin E and Tom S Clark. 2014. “Scaling politically meaningful dimensions using texts

and votes.” American Journal of Political Science .

Lazer, David, Alex Sandy Pentland, Lada Adamic, Sinan Aral, Albert Laszlo Barabasi, Devon Brewer,
Nicholas Christakis, Noshir Contractor, James Fowler, Myron Gutmann et al. 2009. “Life in the
network: the coming age of computational social science.” Science (New York, NY) 323(5915):721.

Lim, Yew Jin and Yee Whye Teh. 2007. Variational Bayesian approach to movie rating prediction. In

Proceedings of KDD Cup and Workshop. Vol. 7 Citeseer pp. 15–21.

Lloyd, Stuart. 1982. “Least squares quantization in PCM.” Information Theory, IEEE Transactions on

28(2):129–137.

Mahajan, Meena, Prajakta Nimbhorkar and Kasturi Varadarajan. 2009. The planar k-means problem is

NP-hard. In WALCOM: Algorithms and Computation. Springer pp. 274–285.

McLachlan, Geoffrey and David Peel. 2004. Finite mixture models. John Wiley & Sons.

Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality. In Advances in Neural Information Processing
Systems. pp. 3111–3119.

Mimno, David and David Blei. 2011. Bayesian checking for topic models. In Proceedings of the Con-
ference on Empirical Methods in Natural Language Processing. Association for Computational Lin-
guistics pp. 227–237.

Mimno, David, Hanna M Wallach, Edmund Talley, Miriam Leenders and Andrew McCallum. 2011. Op-
timizing semantic coherence in topic models. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing. Association for Computational Linguistics pp. 262–272.

41

Mullainathan, Sendhil. 2014. “What Big Data Means for Social Science.” Behavioral and Experimental

Seminar.

Murphy, Kevin P. 2012. Machine learning: a probabilistic perspective. MIT press.

Nguyen, Thang, Yuening Hu and Jordan Boyd-Graber. 2014. Anchors Regularized: Adding Robustness
and Extensibility to Scalable Topic-Modeling Algorithms. In Association for Computational Linguis-
tics.

Nielsen, Frank and Richard Nock. 2014. “Further heuristics for k-means: The merge-and-split heuristic

and the (k, l)-means.” arXiv preprint arXiv:1406.6314 .

O’Connor, Brendan. 2014. MiTextExplorer: Linked brushing and mutual information for exploratory
text data analysis. In Proceedings of the Workshop on Interactive Language Learning, Visualization,
and Interfaces. Association for Computational Linguistics pp. 1–13.

OConnor, Brendan, Brandon M Stewart and Noah A Smith. 2013. Learning to Extract International

Relations from Political Context. In Proceedings of ACL.

Papadimitriou, Christos H and Kenneth Steiglitz. 1998. Combinatorial optimization: algorithms and

complexity. Courier Dover Publications.

Park, Jong Hee. 2012. “A Uniﬁed Method for Dynamic and Cross-Sectional Heterogeneity: Introducing

Hidden Markov Panel Models.” American Journal of Political Science 56(4):1040–1054.

Pearson, Karl. 1894. “Contributions to the mathematical theory of evolution.” Philosophical Transac-

tions of the Royal Society of London. A pp. 71–110.

Poole, Keith T and Howard Rosenthal. 1997. Congress: A political-economic history of roll call voting.

Oxford University Press.

Quinn, Kevin M, Burt L Monroe, Michael Colaresi, Michael H Crespin and Dragomir R Radev. 2010.
“How to analyze political attention with minimal assumptions and costs.” American Journal of Politi-
cal Science 54(1):209–228.

Rabiner, Lawrence and Biing-Hwang Juang. 1986. “An introduction to hidden Markov models.” ASSP

Magazine, IEEE 3(1):4–16.

Recht, Ben, Christopher Re, Joel Tropp and Victor Bittorf. 2012. Factoring nonnegative matrices with

linear programs. In Advances in Neural Information Processing Systems. pp. 1214–1222.

Reich, Justin, Dustin Tingley, Jetson Leder-Luis, Margaret E Roberts and Brandon M Stewart. N.d.
Computer Assisted Reading and Discovery for Student Generated Text. Technical report Working
Paper. Export.

Rivers, Douglas. 2003. “Identiﬁcation of multidimensional spatial voting models.” Typescript. Stanford

University .

Robert, Christian P and George Casella. 2004. Monte Carlo statistical methods. Vol. 319 Springer New

York.

42

Roberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis,
Shana Kushner Gadarian, Bethany Albertson and David G Rand. 2014. “Structural Topic Models
for Open-Ended Survey Responses.” American Journal of Political Science .

Ruiz, Francisco J. R., Isabel Valera, Carlos Blanco and Fernando Perez-Cruz. 2014. “Bayesian Non-
parametric Comorbidity Analysis of Psychiatric Disorders.” Journal of Machine Learning Research
15:1215–1247.
URL: http://jmlr.org/papers/v15/ruiz14a.html

Shneiderman, Ben. 1996. The eyes have it: A task by data type taxonomy for information visualizations.

In Visual Languages, 1996. Proceedings., IEEE Symposium on. IEEE pp. 336–343.

Song, Le, Eric P Xing and Ankur P Parikh. 2011. A spectral algorithm for latent tree graphical models. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11). pp. 1065–1072.

Sontag, David and Dan Roy. 2011. Complexity of inference in latent dirichlet allocation. In Advances in

Neural Information Processing Systems. pp. 1008–1016.

Srivastava, Virendera K and David EA Giles. 1987. Seemingly unrelated regression equations models:

estimation and inference. Vol. 80 CRC Press.

Teh, Yee Whye, Michael I Jordan, Matthew J Beal and David M Blei. 2006. “Hierarchical dirichlet

processes.” Journal of the american statistical association 101(476).

Tenenbaum, Joshua B, Charles Kemp, Thomas L Grifﬁths and Noah D Goodman. 2011. “How to grow

a mind: Statistics, structure, and abstraction.” science 331(6022):1279–1285.

Tibshirani, Robert. 1996. “Regression shrinkage and selection via the lasso.” Journal of the Royal Sta-

tistical Society. Series B (Methodological) pp. 267–288.

Vavasis, Stephen A. 2009. “On the complexity of nonnegative matrix factorization.” SIAM Journal on

Optimization 20(3):1364–1377.

Wallach, Hanna M, Iain Murray, Ruslan Salakhutdinov and David Mimno. 2009. Evaluation methods
for topic models. In Proceedings of the 26th Annual International Conference on Machine Learning.
ACM pp. 1105–1112.

Ward, Michael D, Brian D Greenhill and Kristin M Bakke. 2010. “The perils of policy by p-value:

Predicting civil conﬂicts.” Journal of Peace Research 47(4):363–375.

Wolpert, David H and William G Macready. 1997. “No free lunch theorems for optimization.” Evolu-

tionary Computation, IEEE Transactions on 1(1):67–82.

Yao, Limin, David Mimno and Andrew McCallum. 2009. Efﬁcient methods for topic model inference on
streaming document collections. In Proceedings of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining. ACM pp. 937–946.

Zhou, Tianyi, Jeff Bilmes and Carlos Guestrin. 2014. “Divide-and-Conquer Learning by Anchoring a

Conical Hull.” arXiv preprint arXiv:1406.5752 .

43

Figure 1: Five example topics from the reference model. These are given the labels Supreme Court,
Cheney, Global Warming, Iran/N.K. Nukes, and Wright respectively.

44

Topic 18:  law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,feder, requir, amend, protect, gun, govern, allow, appeal, citizenTopic 30:  presid, vice, cheney, offic, presidenti, first, execut, dick, decis, leader,role, histori, nation, branch, power, part, govern, order, idea, washingtonTopic 48:  global, warm, research, climat, studi, chang, scienc, scientist, gore, caus,human, scientif, earth, emiss, planet, cell, environment, report, water, greenTopic 60:  iran, nuclear, threat, weapon, iranian, program, missil, north, bomb, defens,korea, strike, sanction, intern, build, militari, intellig, capabl, pose,developTopic 71:  black, wright, white, race, church, racial, racist, pastor, jeremiah,africanamerican, racism, african, comment, reverend, king, controversi, rev,view, communiti, southFigure 2: Relation between three measures of topic similarity across all topics and modes. Plotted
surface is a kernel smoothed density estimate.

45

02468100.00.51.01.52.0L1 and Top WordsTop 10 Words in CommonL1 Distance02468100.00.51.01.52.0L1 and Top DocsTop 10 Docs in CommonL1 Distance02468100246810Words and DocsTop 10 Docs in CommonTop 10 Words in CommonFigure 3: Comparison between the approximation to the bound on the marginal likelihood (the objective
function) with similarity metrics aggregated to the model level.

46

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.40.50.60.70.80.91.0Bound and WordsBoundProportion of 10 Words in Commonlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.40.60.81.0Bound and DocsBoundProportion of 10 Docs in Commonlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.00.20.40.60.81.01.2Bound and L1BoundExpected L1 DistFigure 4: Aggregation of similarity metrics to the topic level.

47

Word Expectations over RunsExpected Common WordsFrequency2468051015CourtCheneyClimateIran/N.K.WrightDoc Expectations over RunsExpected Common DocsFrequency0246802468101214CourtCheneyClimateIran/N.K.WrightL1 Expectations over RunsExpected DistFrequency0.51.01.50246810CourtCheneyClimateIran/N.K.WrightFigure 5: Differences in topical coverage by rating (controlling for time). Effects to the right of 0
indicate a topic more heavily used by Conservatives. Lines indicate 95% conﬁdence intervals using the
“global” approximation to measurement uncertainty (Roberts et al., 2014).

48

l−0.006−0.004−0.0020.0000.0020.0040.006Partisan Rating Effects by TopicExpected Difference in Topic Proportion (Conservative minus Liberal)lWrightlIran/North Korea NukelGlobal WarminglCheneylSupreme CourtFigure 6: The ﬁrst 300 characters of the three posts most associated with the global warming topic.
Posts 1 and 3 come from American Thinker and post 2 comes from Think Progress.

49

Deathly news for the religion of Global Warming. Looks like at least oneprominent scientific group has changed its mind about the irrefutability ofevidence regarding man made climate change.The American Physical Societyrepresenting nearly 50,000 physicists "has reversed its stance on climateClimate change report forecasts global sea levels to rise up to 4 feet by 2100.According to a new report led by the U.S. Geological Survey, the U.S. â€œfacesthe possibility of much more rapid climate change by the end of the centurythan previous studies have suggested.â€(cid:157) The report,NASA has confirmed that a developing natural climate pattern will likely resultin much colder temperatures.Â(cid:160) Of course, the climate alarmists' favoritedubious data source was also quick to point out that such natural phenomenashould not confuse the issue of manmade greenhouse gas induced globalFigure 7: Distribution of the partisan rating effect across modes for the ﬁve example topics. The black
solid line shows the effect at the reference mode and the black dashed line marks an effect size of 0.

50

Supreme CourtRating EffectFrequency−0.008−0.006−0.004−0.0020.0000.002020406080No effectReference modelCheneyRating EffectFrequency−0.010−0.0050.000020406080Global WarmingRating EffectFrequency0.0000.0010.0020.0030.0040.0050.006020406080100Iran/N.K. NukesRating EffectFrequency−0.010−0.0050.0000.0050.010050100150Rev. WrightRating EffectFrequency−0.0020.0000.0020.0040.006020406080100120Figure 8: Comparison of metric based on cosine similarity.

51

02468100.00.20.40.60.81.0Cosine Sim. and Top WordsTop 10 Words in CommonCosine Similarity02468100.00.20.40.60.81.0Cosine Sim. and Top DocsTop 10 Docs in CommonCosine Similarity02468100246810Words and DocsTop 10 Docs in CommonTop 10 Words in CommonFigure 9: A comparison of initialization strategies for the K = 100 STM models.

52

Comparing Initialization StrategiesLower Bound at ConvergenceDensity−17960000−17920000−17880000−178400000e+001e−052e−053e−054e−05RandomLDAFigure 10: A comparison of the spectral initialization strategy to random and LDA for the K = 100
STM models. The green dashed denotes the result of the spectral initialized solution.

53

Comparing Initialization StrategiesLower Bound at ConvergenceDensity−17950000−17900000−17850000−17800000−177500000e+001e−052e−053e−054e−05RandomLDASpectralNavigating the Local Modes of Big Data: The Case of

Topic Models∗

Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley

This draft: June 28, 2015

∗Prepared for “Computational Social Science: Discovery and Prediction”, R. Michael Alvarez, editor. The research
assistance of Antonio Coppola, under the support of the Harvard BLISS program, was extremely valuable for this paper.

Our thanks to participants of the Harris School Political Economy workshop and Princeton University Political Method-

ology workshop for reinforcing the need to write this chapter, and Scott de Marchi, Jetson Leder-Luis, Jimmy Foulds,

Padhraic Smyth, Hanna Wallach for comments. An R package for estimating the Structural Topic Model is freely avail-

able at www.structuraltopicmodel.com.

1

Introduction

Each day humans generate massive volumes of data in a variety of different forms (Lazer et al., 2009).

For example, digitized texts provide a rich source of political content through standard media sources

such as newspapers, as well as newer forms of political discourse such as tweets and blog posts. In this

chapter we analyze a corpus of 13,246 posts that were written for 6 political blogs during the course of

the 2008 U.S. presidential election. But this is just one small example. An aggregator of nearly every

document produced by the US federal government, voxgov.com, has collected over 8 million documents

from 2010-2014 including over a million tweets from members of Congress. These data open new

possibilities for studies of all aspect of political life from public opinion (Hopkins and King, 2010) to

political control (King, Pan and Roberts, 2013) to political representation (Grimmer, 2013).

The explosion of new sources of political data has been met by the rapid development of new sta-

tistical tools for meeting the challenges of analyzing “big data.” (Council, 2013; Grimmer and Stewart,

2013; Fan, Han and Liu, 2014). A prominent example in the ﬁeld of text analysis is Latent Dirichlet

Allocation (LDA) (Blei, Ng and Jordan, 2003; Blei, 2012), a topic model which uses patterns of word

co-occurrences to discover latent themes across documents. Topic models can help us to deal with the

reality that large datasets of text are also typically unstructured. In this chapter we focus on a particular

variant of LDA, the Structural Topic Model (STM) (Roberts et al., 2014), which provides a framework to

relate the corpus structure we do have (in the form of document-level metadata) with the inferred topical

structure of the model.

Techniques for automated text analysis have been thoroughly reviewed elsewhere (Grimmer and

Stewart, 2013). We instead focus on a less often discussed feature of topic models and latent variable

models more broadly, multi-modality. That is, the models discussed here give rise to optimization prob-

lems which are non-convex. Thus, unlike workhorse tools like linear regression, the solution we ﬁnd

can be sensitive to our starting values (in technical parlance, the function we are optimizing has multiple

modes). We engage directly with this issue of multi-modality helping the reader to understand why it

arises and what can be done about it. We provide concrete ways to think about multi-modality in topic

models, as well as tools for dealing and engaging with it. For example, we enable researchers to ask: how

1

substantively different are the results of different model solutions? Is a “topic,” which heuristically can

be thought of as a collection of commonly co-occurring words, likely to appear in across many solutions

of the model? Furthermore, is our key ﬁnding between a variable (such as partisan afﬁliation) and the

prevalence of topic usage stable over multiple solutions to the model?

We also discuss initialization strategies for choosing the starting values in a model with multiple

modes. Although seldom discussed, these initialization strategies become increasingly important as

the size of the data grows and the computational cost of running the model even a single time rises.

Starting the algorithm at better starting values not only leads to improved solutions, but can also result

in dramatically faster convergence.

The outline of this chapter is as follows. In Section 2 we introduce the problem of multi-modality

and provide several examples of models with multiple modes. In Section 3 we focus on the particular

case of topic models and highlight some of the practical problems that can arise in applied research.

In Section 4 we introduce a set of tools that allow users to explore the consequences of multi-modality

in topic models by assessing stability of ﬁndings across multiple runs of the model. In Section 5 and

Section 6 we discuss procedures for carefully initializing models which may produce better solutions.

Finally Section 7 concludes by returning to the constraints and opportunities afforded by big data in light

of the statistical tools we have to analyze this data.

2

Introduction to Multi-modality

Multi-modality occurs when the function we are trying to optimize is not globally concave.1 Thus, when

we converge to a solution we are unsure whether we have converged to a point which is the global maxi-

mum or simply a local maximum. In statistical models, the function we are typically trying to maximize

is the likelihood function, and when this function is not concave the solution we arrive at can be depen-

dent on our starting values. This issue occurs in many classes of statistical models, but is particularly

relevant in those where 1) the data generating process of the data comes from a mixture of distributions

or contains latent variables, which the likelihood then reﬂects, 2) ridges (essentially ﬂat regions) in the

1In this chapter, we refer to convex optimization problems and convex models as those where the likelihood is globally
concave, and therefore has one maximum, instead of a globally convex likelihood with one minimum. Our main interest,
though, is in the number of modes the likelihood has.

2

likelihood function appear due to constraints applied to the statistical model, or 3) some parameters are

unidentiﬁed and therefore multiple solutions exist for the same model. The ability to diagnose and navi-

gate multi-modality decreases with the dimension of the parameter space, as visualizing and estimating

the likelihood becomes more difﬁcult in higher dimensions and more complicated models.

Multi-modality is particularly prevalent in the context of ‘big data’ because the same latent variable

models which are useful for analyzing largely unstructured data also lead to challenging optimization

problems. The models we employ in this setting often involve mixtures of distributions, complicated

constraints, and likelihoods that are difﬁcult to visualize because the models contain hundreds, some-

times thousands of parameters. While simple models from the exponential family with concave like-

lihoods like regression or lasso (Tibshirani, 1996) still play an important role in big-data applications

(Mullainathan, 2014; Belloni, Chernozhukov and Hansen, 2014), there is an increasing interest in the

use of more complex models for discovering latent patterns and structure (Council, 2013). While the

latent variable models can bring new insights, they also introduce a complex optimization problem with

many modes.

In this section we build up for the reader intuitions about what can lead to multi-modality. We ﬁrst

discuss a convex, univariate Gaussian maximum likelihood model that is easily optimized to provide

contrast for the non-convex models we describe later in the section. Then, we extend the univariate

Gaussian to a simple mixture of Gaussians and provide an intuition for why mixture models can be

multi-modal. Last, we connect the simple mixture of Gaussians to topic models and describe how these

models, and generally models for big data, contain latent variables (variables in the data generating

process that are not observed) which will mean they are more likely to be multi-modal.

2.1 Convex Models

To start, we present an example of a convex model, where multi-modality is not a problem. A strictly

concave function only has (at most) one maximum, and has no local maxima. This is convenient for

optimization because when the optimization procedure2 has found a maximum of a concave likelihood

function, it has clearly reached the global maximum if only one exists. The natural parameter space for

2There exist a large number of optimization procedures for ﬁnding optima of a particular function, see Boyd and Vanden-

berghe (2009) for a review.

3

regression models with a stochastic component in the exponential family are convex, and therefore are

easily optimized (Efron et al., 1978).

We begin with a simple Gaussian (Normal) model with mean µ and variance σ2.3 In the next section

we will show how we can generalize this basic setup to a more ﬂexible Gaussian mixture model.

Y ∼ N (µ, σ2)

The Normal distribution is from the exponential family, and therefore the likelihood is concave. This

is easy to see by deriving the log-likelihood:

L(µ|y) ∝ N (y|µ, σ2)

= (2πσ2)−1/2 exp

lnL(µ|y) = −n
2

ln(2πσ2) −

(cid:19)
(cid:18)−(yi − µ)2
(cid:80)n
(cid:80)n

2σ2
i=1 y2
i
2σ2 +

i=1 y2
i
σ2

(cid:18)−n

(cid:19)

2σ2

+

µ2

If we take the second derivative of the log-likelihood, we get −n

σ2 . Since n and σ2 are always positive,
the second derivative is always negative.4 For a ﬁxed σ2, in a function with only one parameter like this

one, a negative second derivative is sufﬁcient for the likelihood to be convex.5 As a result, this model

is not multi-modal. When estimated the same parameter estimates will be returned regardless of the

starting values.6

2.2 Mixture Models

Now consider a model where the stochastic component is a combination of Gaussians, instead of one

Gaussian with a mean and standard deviation. Imagine a case where the dependent variable could be

drawn from one of two different Normal distributions.

In this data generating process the Gaussian

distribution which the observation is drawn from is ﬁrst chosen with a particular probability. Then, the

value of the dependent variable is drawn from the chosen Gaussian with a particular mean and variance.

3This model is equivalent to a Normal linear regression only modeling the intercept; without regressors.
4See King (1989) for a more in-depth discussion of this example.
5For multi-dimensional likelihoods, if the Hessian is positive deﬁnite the model will be strictly convex (only has one

optimum); if it is positive semi-deﬁnite, it will be convex (two points may share a optimum, on the same plane.)

6Other normal linear regression models that are sometimes used in big data applications include lasso (Tibshirani, 1996).

4

For example, say you were trying to model the height of people within a population. Further, you

only observed the heights of the people in the population, not any other information about them. You

might assume a model where ﬁrst you draw with 0.5 probability whether the person is male or female.

Based on their gender, you would draw the height either from a distribution with a “taller” mean (if the

person were male), or from a Normal distribution with a “shorter” mean (if the person were female).

This is a simple mixture model, as the data (the heights) would be drawn from a mixture of distributions.

Formally, the data generating process for this model, a simple Gaussian mixture model is:

1. Randomly select a distribution di with probability P (di) = wi, where(cid:80) wi = 1.

2. From the selected distribution, draw y ∼ N (µi, σ2
i ).

The log likelihood for this model becomes:

lnL(y|µ1, µ2, σ2

1, σ2

2) =

N(cid:88)

(cid:32) K(cid:88)

ln

(cid:33)

wkN (yn|µk, σ2
k)

n=1

k=1

This model has more parameters to maximize than the normal regression model described in the

previous section because 1) the probability of each distribution must be estimated and 2) the mean and

variance of each distribution. Further, the model is considered a latent variable model because the latent

distribution variables di are not observed, but are rather generated as an intermediate step within the

data generating process. Because it is unknown which distribution each data point comes from (the

data do not tell us which datapoints are men and which are women), we cannot solve this problem

using the familiar tools of regression. In practice, the maximum likelihood estimate is typically solved

using heuristics such as Expectation Maximization algorithm (Dempster, Laird and Rubin, 1977) which

alternate between estimating the latent membership variable di (the unknown gender in our case) and the

parameters of the distribution (the expected height and variance for each gender).7

It is easy to see that the estimates of each distribution’s parameters will depend on the data points

assigned to it and the estimates of the latent variables will depend on distribution parameters. Because we

need one to easily estimate the other, we choose a starting value to initialize our estimator. Unfortunately,

different starting values can lead to different ﬁnal solutions when the optimization method gets stuck in

7Although see additional strategies for the lower dimensional case in Kalai, Moitra and Valiant (2012).

5

a local maximum. Despite the problems with multi-modality, mixture models are often more accurate

descriptions for data generating processes than more traditional regression models, particularly for data

that may have quite complicated underlying data generating processes (e.g., Deb and Trivedi, 2002;

DuMouchel, 1999; Fan, Han and Liu, 2014; Grimmer and Stewart, 2013; Fan, Han and Liu, 2014).

2.3 Latent Dirichlet Allocation

Later in the core sections of this chapter, we address approaches to dealing with multi-modality in models

of text data. In anticipation of this discussion, we now introduce the Latent Dirichlet Allocation (LDA)

(Blei, Ng and Jordan, 2003), one of the most popular statistical models of text. We use the intuition from

the simple mixture model described in the previous section to provide an intuition for why LDA and

similar models are multi-modal.

LDA is a mixed membership topic model, meaning that each document is assumed to be a ‘mixture’

of topics. Topics are mathematically described as a probability vector over all V words within a corpus.

For example, a topic about summer might place higher probabilities on the words “sun”, “vacation”,

and “summer”, and lower probabilities to words such as “cold” or “snow”. Each topical vector has a

probability assigned to each word within the corpus and therefore is a vector of length V . Topics are

typically described by the most probable words for that corpus. The “topic matrix” β contains K (the

number of topics estimated from the data) rows of topical vectors, each of length V .

For each document, the data generating process ﬁrst decides the number of words within the docu-

ment N. Then, it draws how much of the document will be in each topic (out of K topics), assigning

a probability to each of K topics in the vector θ ((cid:80)

K θ = 1). It then assigns each word within the
document to a topic, with probabilities θ. Last, it draws each word for the document from each of the

topic probability distributions in β.

More formally, the data generating process for each document in LDA is as follows:

1. First, the length of the document is chosen from a Poisson, with prior η: N ∼Poisson(η).

2. Next, the proportion of the document in each topic is drawn, with prior α: θ ∼Dir(α)

3. Last, for each of the N words:

6

• A topic for the word is chosen: zn ∼Multinomial(θ).
• The word is chosen from the topic matrix β, selecting the topic that was chosen zn: wn ∼

Multinomial(βzn)

The reader should already be able to note that LDA is a more complicated version of the mixture of

Gaussians described previously in this section. First, we draw from a distribution that determines the

proportion of a document within each topic and the topic assignment for each word. Then, given the

topic assignment for each word, we draw the words that we observed within the documents. While much

more complicated, this closely follows the previous section where ﬁrst we drew a ‘latent’ variable (the

distribution (male or female) of the height) and then drew the data (height itself).

Similar to the mixture of Gaussians, optimization of LDA is difﬁcult because of the ‘latent’ parame-

ters that must be drawn before the data is ﬁnally drawn. In LDA, these parameters are the proportion of a

document in each topic (θ) and the topic assignment for each word (zn) and are not observed. Similar to

the mixture model case, we can optimize the model using a variant of the EM algorithm called variational

EM.8 In the expectation step, we ﬁrst make a best guess as to the θ and zn for each individual document,

and in the maximization step, we optimize the remaining parameters (in this case β) assuming θ and zn.

We iterate between the expectation and maximization steps until convergence is reached.9

This approach maximizes the marginal likelihood (the probability of the data given β and α), which

we can use as the objective function for maximizing the model. To get an intuition for the marginal

likelihood, ﬁrst we ﬁnd the joint distribution of parameters and data:

p(θ, z, w|α, β) = p(θ|α)

p(zn|θ)p(wn|zn, β)

N(cid:89)

To ﬁnd the probability of the words marginalized over the latent parameters, we integrate over zn and

n=1

8Variational inference provides an approximation to the posterior distribution that falls within a tractable parametric family
unlike EM which provides a point estimate of the model parameters. Here we simplify some of the differences between these
approaches by referring to variational inference as optimizing the “model parameters” rather than the parameters of the
approximating posterior. For more information, see Jordan et al. (1998); Grimmer (2010b); Bishop et al. (2006).

9The posterior distribution of LDA can also be estimated using Gibbs Sampling, see Grifﬁths and Steyvers (2004) for

more information.

7

θ.

(cid:90)

N(cid:89)

(cid:88)

n=1

zn

p(θ|α)

p(w|α, β) =

p(zn|θ)p(wn|zn, β)dθ

The marginal likelihood itself is intractable in the case of LDA because of the coupling of β and θ

which leads to a an intractable integration problem. The variational EM approach uses Jensen’s Inequal-

ity to create a lower bound on the marginal likelihood which we can maximize via coordinate ascent.

That is, the algorithm is alternating between updating the content of the topics (β) and the topical makeup

of a document (θ). It is this alternating maximization strategy that leads to multiple local optima. If we

we could jointly optimize β and θ we would likely have fewer issues of local modes, but the coupling in

the marginal likelihood makes this unfeasible.

3 The Case of Topic Models

Multimodality occurs in a huge number of statistical models.10

In the rest of this chapter we focus

on unsupervised latent variable models.

In practice we use latent variable models to discover low-

dimensional latent structure that can explain high dimensional data. These models have been broadly

applied throughout the social sciences to analyze large bodies of texts (Grimmer and Stewart, 2013),

discover categories of diseases (Doshi-Velez, Ge and Kohane, 2014; Ruiz et al., 2014), study human

cognition (Tenenbaum et al., 2011), develop ontologies of political events (OConnor, Stewart and Smith,

2013), build recommendation systems (Lim and Teh, 2007) and reveal the structure of biological and

social networks (Airoldi et al., 2009; Hoff, Raftery and Handcock, 2002). As we have suggested, the

ﬂexibility of latent variable models often leads to difﬁcult statistical inference problems and standard

10For example, neural network models (Cochocki and Unbehauen, 1993), which allow for layered combinations of the
model matrix, are extremely useful for modeling more complex data generating processes (Beck, King and Zeng, 2000).
However, they too often suffer from extremely multi-modal likelihoods and rarely is the global maximum found (Bishop
et al., 2006; De Marchi, Gelpi and Grynaviski, 2004). Additional examples include Bayesian non-parametric processes (Teh
et al., 2006; Grifﬁths and Tenenbaum, 2004), hidden Markov models (Rabiner and Juang, 1986; Park, 2012), switching
time series models (Hamilton, 1989), and seemingly unrelated regression models (Srivastava and Giles, 1987; Drton and
Richardson, 2004), to name a few. The item response (IRT) model (Hambleton, 1991), popular in political science (Poole and
Rosenthal, 1997), is unidentiﬁed because solutions that are rotations of each other can exist for the same set of data (Poole
and Rosenthal, 1997; Rivers, 2003). To estimate the model, a few parameters must ﬁrst be pinned down before the rest of
the parameters can be known. In essence, there are multiple and sometimes equally likely solutions to the same problem.
While different from multi-modality in the previous examples, “multiple solutions” of an unidentiﬁed likelihood can also be
classiﬁed under models with likelihoods that have multiple modes.

8

approaches often suffer from highly multi-modal solutions.

Statistical topic models are rapidly growing in prominence within political science (Grimmer, 2010a;

Quinn et al., 2010; Lauderdale and Clark, 2014; Roberts et al., 2014) as well as in other ﬁelds (Goldstone

et al., 2014; Reich et al., N.d.). Here we focus on Latent Dirichlet Allocation (LDA) which, as discussed

in the previous section, models each document as a mixture over topics (Blei, Ng and Jordan, 2003; Blei,

2012). The mixed membership form provides a more ﬂexible representation than the single membership

mixture model, but at the cost of an optimization problem with many more local optima.11

The posterior of the LDA model cannot be computed in closed form. Two popular approximate in-

ference algorithms are collapsed Gibbs sampling (Grifﬁths and Steyvers, 2004) and variational inference

(Blei, Ng and Jordan, 2003). In this context, both methods can be seen as a form of alternating maxi-

mization; in Gibbs sampling we randomly draw from a single parameter conditional on the others and

in variational inference we update a single parameter averaging over the other parameters with respect

to the approximating distribution (Grimmer, 2010b). This process of alternating conditional updates,

necessitated by the inability to directly integrate over the posterior, leads to a sensitivity to the starting

values of the parameters. The myriad solutions which can result from different starting points is well

known amongst computer scientists but infrequently discussed.12

In fact, we can be more precise about the difﬁculty of the LDA inference problem by introducing

some terminology from theoretical computer science. Non-deterministic Polynomial-time hard (NP-

hard) problems are a class of problems which it is strongly suspected cannot be solved in polynomial

time.13 A more complete deﬁnition is beyond the scope of this article, but the classiﬁcation conveys a

sense of the difﬁculty of a problem. Maximum likelihood estimation can be shown to be NP-hard even

for LDA models with only two topics (Sontag and Roy, 2011; Arora, Ge and Moitra, 2012). These

hardness results suggest not only why local optima are a characteristic of the LDA problem but also

why they cannot easily be addressed by changes in the inference algorithm. That is, we can reasonably

11LDA, and mixture models more generally, have K! substantively identical modes arising from posterior invariance to
label switching (i.e. permutation of the order of the topics). This type of multimodality is only a nuisance as each of the
modes will yield the same inferences in an applied setting.

12For example, Blei (2012) provides an excellent overview of LDA and related models but does not mention the issue of
local optima at all. The original paper introducing LDA, mentions local optima only in passing to warn against degenerate
initializations (Blei, Ng and Jordan, 2003). Notable exceptions to this trend are Koltcov, Koltsova and Nikolenko (2014);
Lancichinetti et al. (2014) which investigate the stability more directly, as our the efforts in this chapter.

13That is, if P (cid:54)= N P then this is the case. However, there is no formal proof that P (cid:54)= N P .

9

conjecture from these results that without additional assumptions to make the problem tractable, it would

be impossible to develop a computationally practical, globally optimal inference algorithm for LDA.14

How then do we address the practical problem of multimodality in topic models? In this section, we

advocate selecting a solution using a broader set of criteria than just the value of the objective function.

In the next section we make the argument for looking beyond the objective function when evaluating

local modes. We then discuss some speciﬁc methods for choosing a single model for analysis. Finally

we consider how to assess the stability of the chosen result across many different runs. Throughout we

use LDA as a running example but the arguments are more broadly applicable. In particular we will see

how they play out in an applied example using the related STM in subsequent sections.

3.1 Evaluating Local Modes

There is a disconnect between the way we evaluate topic models and the way we use them (Blei, 2012).

The likelihood function and common evaluation metrics reward models which are predictive of unseen

words, but our interest is rarely in predicting the words in a document; we want a model which provides

a semantically coherent, substantively interesting summary of the documents (Grimmer and Stewart,

2013). This disconnect is not easily remedied; our models and evaluation metrics focus on prediction

because it is the most tractable approximation to a human judgment of utility that ultimately must be

made on a cases by case basis. This perspective informs an approach to dealing with multimodality

which emphasizes selecting a particular run not solely on the basis of which model yields the highest

value of the objective function, but also includes other external assessments of model quality.

If our sole criterion of success were to maximize the objective function, our path would be clear.

We would simply generate a large number of candidate solutions by running the model repeatedly with

different starting values and then select the one with the highest value. In variational approximations this

metric is neatly deﬁned in a single value: the lower bound on the marginal likelihood. We could simply

calculate the bound for each model and choose the largest value.

In a general sense, this procedure is both intuitive and well-supported theoretically. Not only is the

14The exact connection between NP-hard complexity and local modes is difﬁcult to concisely state. Not all convex prob-
lems can be provably solved in polynomial time(de Klerk and Pasechnik, 2002). However it is sufﬁcient for the argument
here to establish that the hardness results imply that there is something inherently difﬁcult about the nature of the problem
which makes it unlikely that a computationally practical algorithm with global convergence properties exists without adding
additional assumptions.

10

lower bound the objective function we are optimizing, but as a lower-bound on the marginal evidence it

is precisely the quantity commonly used in approaches to Bayesian model selection (Kass and Raftery,

1995; Bishop et al., 2006; Grimmer, 2010b). These methods will pick the best model, given the assump-

tions of the data generating process, and that may not be the one that is most interesting (Grimmer and

Stewart, 2013). While for the purposes of estimating the model we need to rely on our assumptions about

the data generating process, we need not maintain these commitments when making our ﬁnal selection.

This allows us to access a richer set of tools for evaluating model quality.

The implication of this argument is to say that if we found the global optimum we might not choose

to use it. This seems counter-intuitive at ﬁrst, but various forms of the argument have a long tradition

in statistics. Consider the argument that we should choose a model on the basis of cross-validation or

other forms of held-out prediction. This is the most commonly used evaluation metric for topic models

(Wallach et al., 2009; Foulds and Smyth, 2014) and also has a strong tradition in political science (Beck,

King and Zeng, 2000; De Marchi, Gelpi and Grynaviski, 2004; Ward, Greenhill and Bakke, 2010).

Selecting a model which maximizes a held-out predictive measure implies that we may not choose the

model which maximizes the in-sample objective function. In settings where forecasting is the primary

goal the ability to predict a held-out sample is the clear gold standard; however, in the case of topic

models, prediction is not the only relevant standard.

Implicit in this argument is the claim that the objective function need not directly correspond with

human judgment. In human evaluations of topic coherence, selecting model parameters to maximize

predictive log-likelihood can actually lead to a mild decrease in assessment of human interpretability

(Chang et al., 2009; Lau, Newman and Baldwin, 2014). Domain expert assessment (Mimno et al., 2011)

and alignment to reference concepts (Chuang et al., 2013) have consistently shown that selecting on the

objective function alone does not necessarily yield the same model as human selection.

This is not to say that the objective function is completely useless; we have after all chosen to op-

timize it. Rather our claim is that amongst locally optimal solutions model ﬁt statistics provide a weak

signal of model quality as judged by human analysts. Due to the nature of the optimization problem we

ﬁnd ourselves having ﬁt a number of candidate models and given that we already have them, it would be

wasteful to evaluate them only on the basis of the objective function.

11

One reaction to this situation would be to improve the objective of the model until it matched a human

perception of quality. Unfortunately, this is theoretically impossible across all possible tasks (Grimmer

and King, 2011; Wolpert and Macready, 1997). Moreover, the inference problem is already particularly

complex and modiﬁcations tend to result in even more intractable models (Mimno et al., 2011).

At the end of the day we trust the objective function enough to optimize it when ﬁtting the model,

but not enough to let it be the surrogate for the selection process. Instead, we want to explore the model

and its implications, a process which is closely related to the literature on posterior predictive checks

(Mimno and Blei, 2011; Blei, 2014; Gelman et al., 2013). In the next section we treat the question of

how to choose a particular model for analysis, which we call the reference model. In the following

section we address how to asses sensitivity to that choice.

3.2 Finding a Reference Model

Choosing a single reference model for analysis is challenging. The ideal selection criterion is the util-

ity of the model for the analyst, which is an inherently subjective and application speciﬁc assessment

(Grimmer and King, 2011; Grimmer and Stewart, 2013). There is an inherent tradeoff in selection cri-

teria between how time intensive the criterion is for the analyst and how closely it approximates the

the theoretical ideal. In this section we outline methods which span the range of high quality to highly

automated.

Manual Review The most thorough and time intensive process is a manual review and validation of

the model. This entails reading several example documents for each topic and carefully the topic-word

distributions to verify that the topics are capturing a single well-deﬁned concept. Depending on the

number of topics and the length of the documents this may be a daunting task in itself.

We may also want to consider information beyond the content of the documents themselves. In the

social sciences we often have a rich source of additional information in document metadata. Mapping

the relations between topics and a document’s author (Grimmer, 2010a) or date (Quinn et al., 2010) is an

important part of understanding if the model is functioning. When an existing typology of the documents

is available, we can evaluate how well it corresponds to the inferred topics (Chuang et al., 2013). Ideally

we hope that the model will convey some things we already know, allowing us to validate it, while also

12

providing us with some novel insights. The different types of validation criteria have been well developed

in the literature for measurement models and content analysis (Quinn et al., 2010; Grimmer and Stewart,

2013; Krippendorff, 2012, e.g). 15

Manual evaluations of this sort are essentially custom procedures designed speciﬁcally for a particu-

lar analysis and requiring a large amount of an analyst’s time. They are an important and necessary tool

for validation of the ﬁnal model but are too expensive for evaluation of each candidate model.

Semi-automated analysis A less labor intensive approach is the human analysis of automated model

summaries. The idea is to develop some generic tools for quickly evaluating a model even if some human

intervention is required to make a decision. For topic models we can summarize a topic by looking at

the most probable or distinctive words. These word lists can be supplemented by focused reading of

documents highly associated with a particular topic. These types of summaries arise naturally from the

parameters of the model in the case of LDA and most latent variable models have some approximate

equivalents.

Recent work in information visualization has moved towards the development of automatically gen-

erated topic model browsers (Chuang, Manning and Heer, 2012; Gardner et al., 2010; Chaney and Blei,

2012). Similar approaches have been used to provide browsers which focus on the exploration of co-

variate effects on word use (O’Connor, 2014). The best of these approaches embody the information

visualization mantra of “overview ﬁrst, zoom and ﬁlter, details on demand” (Shneiderman, 1996) which

encapsulates the goal of a system that can seamlessly move from high level model summaries such as

word lists all the way down to the document reading experience. Some systems can even incorporate user

feedback in order to allow for an interactive topic modeling experience (Hu et al., 2014). Visualization

of topic models is an active area of research which promises to vastly improve the analyst’s interaction

with the model.

Complete Automated Approaches The fastest evaluation metrics are those which are completely au-

tomated. The most natural metric is the objective function which is generally either a bound or an

approximation to the marginal likelihood (Grimmer, 2010b). The default standard within the computer

15Quinn et al. (2010) present ﬁve types of validity for topic models: external, semantic, discriminant, predictive and

hypothesis.

13

science literature is held-out likelihood which provides a measure of how predictive the model is on

unseen documents (Wallach et al., 2009; Foulds and Smyth, 2014). Evaluating how well the model pre-

dicts new data is appealing in its simplicity, but a predictive model need not be the most semantically

interpretable.

Automated metrics can also be useful for narrowing the selection of candidate models which we eval-

uate with more labor intensive approaches. In Roberts et al. (2014) we consider two summary measures:

semantic coherence (Mimno et al., 2011), which captures the tendency of a topic’s high probability words

to co-occur in the same document, and exclusivity, which captures whether those high probability words

are speciﬁc to a single topic. We use these summaries as a coarse ﬁlter to focus our attention on a subset

of promising candidate models.

Choosing a Balance This provides only a coarse overview of some of the strategies for choosing a

model. Necessarily model choice will be dictated by the particular problem at hand. Once a model is

chosen there is always a subjective process of assigning a label to the topic which implicitly involves

arguing that the model representation (a distribution over words) is a good proxy for some theoretical

concept represented by the label. Regardless of how the model is chosen, careful validation of the topic

to ensure it ﬁts with the theoretical concept is key (Grimmer and Stewart, 2013).

3.3 Assessing Stability

Once we have committed to a particular model and unpacked the publishable ﬁndings, we may want to

know how stable the ﬁnding is across different initializations (i.e., starting values of the optimization

algorithm). This serves two distinct purposes: ﬁrst, we get a sense of how improbable it is that we found

the particular local mode we are analyzing and second, we learn how sensitive the ﬁnding is to other

arrangements of the parameters.

The ﬁrst purpose is the most straightforward. We want to build conﬁdence in our readers and in

ourselves that we did not stumble across the result completely by chance. The instability across individ-

ual runs of LDA has been criticized as unsettling by applied users across ﬁelds (Koltcov, Koltsova and

Nikolenko, 2014; Lancichinetti et al., 2014). Understanding how topics map on to the results across runs

builds trust in the results (Chuang et al., 2013).

14

We can also use stability to assess how sensitive our ﬁnding is to other conﬁgurations of the topics.

If a researcher identiﬁes a topic as about “economics” is there some other version of that topic which

looks substantially similar but yields contradictory results? These situations can arise when a particular

topic or group of topics is of interest, but the model is sensitive to the way the remainder of the topics are

allocated. Careful examination of the topic may conﬁrm that it is about “economics” but fail to reveal

similar content outside the topic that might reasonably be included. Examining the “economics” topic

across a large set of models provides a sense of the different representations of the topic supported by

the data.

4 Similarity Between Topics Across Modes

In this section we develop tools for assessing the stability of ﬁndings of interest across local modes. We

start by setting up a running example which uses STM to analyze a corpus of political blogs. We then

illustrate several approaches to assessing how similar a pair of topics are to each other. We then show

how these metrics can be aggregated to the topic level, model level or across covariates.

The methods we present here serve two related purposes. First, we provide some intuition for the

variety of solutions that arise from local modes. Especially for those primarily familiar with globally

convex models, this provides a sense of what to expect when using or reading about latent variable

models. The methods themselves can also be useful as diagnostics for practictioners. Indeed we show

through examples how examination of stability can lead to useful insights about the data and model.

4.1 Political Blogs

In order to make our discussion concrete we turn to a speciﬁc data set. We use a collection of 13,246

blog posts from American political blogs written during the 2008 presidential election (Eisenstein and

Xing, 2010).16 Six different blogs, American Thinker, Digby, Hot Air, Michelle Malkin, Think Progress,

and Talking Points Memo, were used to construct the corpus. Each blog is given a rating: liberal or

conservative. For each blog post the day of the post is recorded. We stemmed, removed a standard list of

stopwords and words which appeared in fewer than 1% of the documents. This results in a vocabulary

16The CMU Poliblog corpus is available at http://sailing.cs.cmu.edu/socialmedia/blog2008.html
and documentation on the blogs is available at http://www.sailing.cs.cmu.edu/socialmedia/blog2008.
pdf. A sample of 5000 posts is also available in the stm package.

15

of 2653 words.

To analyze these texts we use STM (Roberts et al., 2014). STM is a mixed-membership topic model

in the style of LDA which allows for the inclusion of document-level covariates, in this case rating (lib-

eral/conservative) and time (day of the post). We use the stm package in R which uses a fast variational

EM algorithm. We specify topic prevalence as a function of the partisan rating and a smooth function of

time. We estimated the model 685 times initializing with a short run of LDA (we return to this in Section

5).17 We note that this set of runs holds a number of things constant including choices in pre-processing

(e.g. stop word removal, stemming) and speciﬁcation of the model (e.g. the STM prevalence formula,

number of topics) which could also lead to differences in model ﬁt.

We brieﬂy deﬁne a minimal amount of notation for use in later sections. Let K = 100 be the user-

selected number of topics, V = 2653 be the size of the vocabulary and D = 13246 be the number

of documents. Mixed membership topic models including LDA and STM can be summarized by two

matrices of parameters. β is a row-normalized K-by-V matrix of topic-word distributions. The entry

βk,v can be interpreted as the probability of observing the v-th word in topic k. θ is a row-normalized

D-by-K matrix of the document-topic distributions. The entry θd,k can be interpreted as the proportion

of words in document d which arise from topic k. Both LDA and STM can be framed as a factorization
of the row-normalized D-by-V empirical word count matrix W , such that W ≈ θβ. We will use the θ
and β matrices to compare the models.

In order to simplify the resulting discussion, we choose as our reference model the sample maximum

of the variational bound. We note that we do not recommend using the sample maximum in general as

the selection criteria (for reasons discussed in previous section), but it allows us to proceed more quickly

to the comparison of results.

The hundred topics estimated in the model cover a huge range of issues spanning the political di-

mensions of the 2008 presidential election. We select ﬁve topics which illustrate different properties of

stability to use as running examples.

[Figure 1 about here.]

Figure 1 shows the top 20 most probable words for each of the example topics. The topics cover Supreme

17Each model is run to convergence (a relative change of less than 10−5 in the objective).

16

Court rulings, Vice President Cheney, Global Warming Research, Nuclear Weapons issues in Iran and

North Korea and the controversy surrounding Barack Obama’s former pastor, Jeremiah Wright.

4.2 Comparing Topics

Our ﬁrst step is to ask whether there are any differences between the different runs of the model at all. If

each run is equivalent up to numerical precision the question of multimodality would be moot. In order

to answer this question we need a way to measure whether two topics generated across different runs are

in fact comparable.

We can compare the similarity of two models by comparing the topic word distribution β or the

document-topic distribution θ. Using β implies that two topics are considered similar if they generate

similar observed words. Using θ assesses two topics as similar if they load in the same patterns across

the corpus. While both approaches are useful, β will tend to contract on the true posterior faster than θ

resulting in a less noisy measure. This is because the number of documents will tend to grow faster than

the number of unique words in the vocabulary. Before proceeding to pairwise similarity metrics we need

to align topics across runs.

Alignment Consider a simple case where we have two runs of the model. We ﬁrst need to establish

which two topics from each run to compare. The topic numbers are arbitrary across each run, which

on its own is unproblematic but means that we need to do something additional in order to compare

topics to each other across runs. We call the process of deciding which topics to compare, the process of

alignment. The alignment itself is determined by some metric of similarity typically on the topic-word

distribution. Here we use the inner product between the rows of β.

Given the similarity metric there are at least two reasonable approaches to aligning topics, both of

which will yield the same result when the topics are in fact identical up to permutation of the topic

numbers. First, we can let each topic in one run of the model choose its favorite in another run of the

model, even if that involves a topic being chosen multiple times. We call this process “local alignment”

because each topic in the reference model is making a local choice which is independent of the choices

of all other topics. A second approach is to choose a one-to-one matching which maximizes the sum

of similarities across all the topic pairs. We call this the “global alignment” because each topic’s match

17

is contingent on the selection of all other topics. Although this formulation results in a combinatorial

optimization problem, it can be solved efﬁciently using the Hungarian algorithm (Kuhn, 1955).18 The

global alignment is used below. The local alignment produced essentially the same relative trends.

Pairwise Similarity Once we have a candidate alignment we can calculate distance metrics between

two topics across model runs. An intuitive measure of distance is the L1 norm, which is the sum of the

absolute value of the difference. It is deﬁned as

(cid:88)

L1 =

k,v − βcand
|βref
k,v |

v

and has a range: [0,2]. We discuss alternate metrics , but we use L1 as the result is easy to conceptualize.

We discuss the implications of alternative distance metrics in Section 4.5.

We need not constrain ourselves to distance metrics on the parameter space. As an alternative, we

compare the number of the top ten most probable words shared by the reference topic and its match. The
result ranges from {0, . . . , 10} indicating the number of words matched.

We can establish the comparable metric for documents. Ranking documents by their use of a partic-

ular topic, we can count the overlap in the number of the ten documents most strongly associated with a
topic. This metric ranges from {0, . . . , 10} with 10 indicating complete agreement in the two sets.

[Figure 2 about here.]

Figure 2 plots the relations between each of these three metrics across the aligned topics. Each pair

of metrics is strongly correlated in the theoretically anticipated direction. Also as expected, the measure

based on the documents is somewhat noisier than the corresponding measure based on the words.

The ﬁgure also provides us with some insight on the similarities across solutions. Topics range from

nearly perfectly aligned to having almost no correspondence. This suggests that there are substantial

semantic differences across local modes which could lead to signiﬁcant differences in interpretation.

18The Hungarian algorithm is a polynomial time algorithm for solving the linear sum assignment problem. Given a K by K
matrix, where entry i, j gives the cost of matching row i to columns j, the Hungarian algorithm ﬁnds the optimal assignment
of rows to columns such that the cost is minimized. The Hungarian algorithm guarantees that this can be solved in O(K 3)
time (Papadimitriou and Steiglitz, 1998). We use the implementation in the clue package in R (Hornik, 2005).

18

4.3 Aggregations

The pairwise similarities shown in Figure 2 are useful for contextualizing the full range of topic pairs;

however, to make these metrics more interpretable it is helpful to aggregate up to either the model level or

the topic level. Aggregation at the model level gives us a sense of how well the local modes approximate

the reference model by taking the average over each topic. Aggregation to the topic level gives us

information about how stable a given topic in the reference model is across runs.

Model Level Aggregations We start with aggregations to the model level. In this case we have a nat-

ural summary metric of the complete model, the approximation to the bound on the marginal likelihood.

[Figure 3 about here.]

In Figure 3 we plot each of the three similarity metrics on the Y-axis against the approximate bound

on the X-axis. The outlier (upper right corner of the ﬁrst two plots, and lower right of the third) is

the reference model which is, by deﬁnition, an exact match for itself. The dashed line marks a natural

reference point (5 of 10 words or documents in the left two plots, and an L1 distance in the middle of the

range for the third). The blue line gives a simple linear trend line.

The trend between the lower bound and the other three similarity metrics suggest that the objective

function can be useful as a coarse measure of similarity. That is, as the bound of each of the runs

approaches the reference model, all three metrics reveal similarity increasing on average. However, it

is only a coarse metric because of the large variance relative to the size of the trend. The high variance

around the trend reinforces the observation that among candidate models with comparable levels of

model ﬁt (as measured by the objective function) there is considerable semantic variety in the discovered

topics.

Topic Level Aggregations Aggregation to the topic level provides us with a measure of how stable a

topic within the reference model is across different runs. This helps address the applied situation where

a researcher has identiﬁed a topic of interest, but wants some understanding of how frequent it is across

multiple runs of the model.

[Figure 4 about here.]

19

The distribution over topics is plotted in Figure 4 where each topic is represented by the average value of

the statistic over the different model runs. The ﬁve example topics are each denoted by the dashed lines

and a label.

In each plot the distribution varies over essentially the full range of the metric, indicating that some

topics are extremely stable across all of the runs while others are essentially unique to the reference

model.

The example topics help to explain where some of this variance is coming from. The climate change

topic is one of the most stable across all three of the metrics. This reﬂects the rather specialized language

in these blog posts. In a political context, words such as “climate” are very exclusive to a particular topic.

These specialized words help to pin down the topic resulting in fewer distinct locally optimal solutions.

[Table 1 about here.]

One of the least stable topics across runs is the Cheney topic.

In the reference model the topic

is primarily about Vice President Cheney whereas other models include broader coverage of the Bush

presidency. As an example we chose the local model which is furthest away from the reference model

in L1 distance. In Table 1 we compare the two version of the topic by comparing the topic-speciﬁc

probabilities of observing eighteen terms. These terms deﬁne the set of words which have probability of

at least 0.01 in one of the two models. We can see that while both topics discuss Cheney, the local model

discusses President Bush using words such as Bush, Bush’s, George which have negligible probability

under the reference model version of the topic.

Topic level stability analysis focuses the analyst’s attention on the semantic content covered by a

topic. As an analyst, our responsibility is to choose a label for a topic which clearly communicates to the

reader what semantic content is included in a topic. We emphasize that an unstable topic is not inferior or

less substantively interesting. Depending on the question, a topic which combines discussion of Cheney

and the Bush Presidency may be more interesting than a topic which just covers the Vice President.

However, the instability in the topic alerts us that the topic in the reference model is speciﬁc to Cheney

with discussion of the Bush Presidency being included in a separate topic.

20

4.4 Covariate Effect Stability

In applied use of STM, we are often interested in the role played by covariates in driving topical preva-

lence. Indeed this is a principle advantage of the STM framework: it allows for the inclusion of covariate

information in the estimation process and facilitates the estimation of covariate effects on the resulting

model. In the Poliblog corpus, we can examine the role of partisanship in topical coverage.

We start by unpacking the partisanship effects for our example topics in the reference model. We

then show how to assess the stability of these ﬁndings across other local modes.

Unpacking Covariate Effects Figure 5 plots the expected proportion of topic use in Conservative

blogs minus the expected proportion of topic use in Liberal blogs under the reference model. Thus

topics more associated with the Conservative blogs appear to the right of zero.

[Figure 5 about here.]

We brieﬂy contextualize the partisan effects in this set of topics. Conservative attention to the

Supreme Court topic is primarily driven by the June 2008 Heller v. District of Columbia case which

struck down parts of the Firearms Control Regulations Act of 1975 on 2nd Amendment grounds. As

discussed in the previous section the Cheney topic is primarily about Dick Cheney’s legacy on the Vice

Presidency. The coverage is mainly from liberal blogs and is predominantly critical in tone.

The greater conservative attention to global warming is initially surprising given that it is typically

a more liberal issue, but it should be remembered that these blogs came from 2008 which was prior to

the more recent trend (at time of writing) in liberal assertiveness. We explore this further by examining

the posts most associated with this topic. Figure 6 shows the ﬁrst 300 characters of the three posts most

associated with the topic. The ﬁrst and third posts are critical of global warming, while the second post

describes a report warning against climate change. The ﬁrst and third are as expected from a Conservative

blog and the second is from a Liberal blog.

[Figure 6 about here.]

The Iran and North Korea Nuclear Weapons topic shows a Conservative effect consistent with in-

creased attention to security topics, consistent with conventional views that issue ownership of security

21

is much greater for Republicans. Finally the scandal involving Reverend Jeremeiah Wright, which is

critical of then Democratic Primary candidate Barack Obama, is more prevalent on Conservative blogs.

Stability Across Models How stable are these effects are across other plausible local modes. A simple

way to evaluate this is to align the topics to the reference model and then calculate the effect for each

topic.19 While this process produces a distribution over effect sizes, it is important to emphasize the

conceptual challenges in interpreting the results. Each model is estimating the effect of the partisan rating

but on a slightly different version of the topic. Thus differences arise for two reasons: the document-

topic assignments may be different, but also because the topics themselves capture different concepts.

The alignment ensures that this concept is the most similar to our reference model (given the alignment

method and the similarity metric) but they are not necessarily conceptually identical.

[Figure 7 about here.]

Figure 7 plots the distribution of effect sizes. Beginning with the ﬁrst plot on the top-left, we see that

the partisan effect for the Supreme Court topic in the reference model has one of the largest observed

values across all of the local modes. Not only is the reference model effect out in the tail, but the distri-

bution over effect sizes includes negative as well as positive values. What accounts for this difference?

Comparing the most probable words in the reference model with those in an aligned topic for one of the

models with a strong liberal effect provides an indication of the differences:

Reference Model: law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,

feder, requir, amend, protect, gun, govern, allow, appeal, citizen

Local Mode: court, tortur, law, justic, legal, rule, judg, suprem, case, interrog, detaine,

lawyer, cia, constitut, guantanamo, decis, prison, violat, prosecut, administr

The local mode includes signiﬁcant discussion of the legal issues surrounding the use of torture and the

operation of Guantanamo Bay. By contrast, our reference has a completely separate topic which captures
19This is similar to the permutation test methodology developed in Roberts et al. (2014). In Roberts et al. (2014) we
are interested in testing whether our ﬁnding on the effect of binary treatment indicator is driven by including it as a topic
prevalence covariate (that is, are we at risk of baking in our conclusion). We randomly permute the treatment indicator across
documents and rerun the model. In each case we calculate the largest treatment effect observed within the data across all
topics and compare this distribution to the observed level. If we were baking in the conclusion, the model would discover
large treatment effects despite that the the treatment indicator had been randomly assigned. In practice the observed effect
is substantially larger than the randomly permuted datasets suggesting that the model is working as expected. Here we are
aligning the topics ﬁrst and comparing effect sizes across model runs.

22

this discussion (top words: tortur, prison, cia, interrog, detaine, use, guantanamo). Thus the fact that the

effect size we found is considerably out in the tail of the histogram does not mean that the ﬁnding is not

valid, but it does suggest that the ﬁnding is very sensitive to the content of the legal cases and the way in

which relevant information about legal issues is spread across the other topics.

The second plot in Figure 7 shows the Cheney topic. Here we see a distribution with three modes

where the reference model sits directly on top of the most typical point. Following the discussion in the

previous section this reﬂects the difference between having the topic focus exclusively on Vice President

Cheney as opposed to including the broader Bush Presidency.

The global warming case (third plot) is the most clear cut with most of the solutions producing

extremely similar effect sizes. This reﬂects the relatively specialized vocabulary in discussing climate

change which allows the allocation of topics to be less ambiguous across solutions.

The Iran and North Korea topic is a case where like the Supreme Court topic there is substantial

spread across the models. However unlike the ﬁrst example, the reference model is quite close to the

majority of the solutions. Here the largest source of variation is primarily in whether both Iran and North

Korea are grouped within the same topic.

Finally, the topic on Reverend Wright shows another case where the reference model is largely con-

sistent with the local modes.

there is some distinction between topics which contain coverage of the

scandal and those which also contain elements of the positive liberal coverage that followed Barack

Obama’s speech on the matter (“A more perfect union”).

These examples highlight the value of local modes for contextualizing the ﬁnding in our reference

model. By seeing alternative models, such as a supreme court topic that focuses on either gun control

or the use of torture, we become more attuned to exactly what concepts are included within the model.

This in turn allows us to choose labels which more precisely represent the topic’s semantic content.

Differences from Alignment While most of the analyses above are insensitive to the method of align-

ing topics, we do observe signiﬁcant differences in the covariates effects. Global alignments tend to

result in more cases where there are several clusters of effect sizes. Consider for example, the Cheney

Topic (top-center of Figure 7). In the example discussed in Section 4.3 we saw that the matching topic

in another model included both discussion of the Bush Presidency and Cheney. If the global alignment

23

had assigned that topic to the Bush reference model topic, that would leave it unavailable for the Cheney

reference model topic. This tends to manifest in the covariate effect distributions as clusters of certain

covariate effect sizes. We still ﬁnd the global alignment the most useful though as it ensures that we are

not omitting any topics from the comparison models.

4.5 Additional Comparisons and Related Work

The examples provided here focused on a particular dataset with a speciﬁc number of topics. Here we

brieﬂy discuss ﬁndings from additional settings and discuss related work in the literature.

Different number of topics We ran the above set of experiments under the same dataset with K = 50

topics and observed essentially the same patterns and trends reported. Smaller experiments at K = 20

reveal higher levels of instability across runs with increased instances of topics that are very poorly

aligned. We conjecture that this is primarily a matter of how well the number of topics ﬁt the speciﬁc

dataset rather than a statement about small numbers of topics in general.20 If instability was solely a

function of the number of topics we would expect substantially poorer performance in this extreme case.

That the instability would be connected to selecting too few topics for a given dataset certainly makes

intuitive sense, but additional investigation would be necessary to make conclusive statements.

Alternative distance measures

In the results above we use two basic measures of distance between the

topic-word distributions. We align the topics using a dot product measure and we presented calculations

based on L1 distance. We also performed experiments using a cosine similarity metric (essentially the

dot product rescaled by the L2 norm of the vectors).

[Figure 8 about here.]

The results, depicted in Figure 8 show slightly less clear correlations between the similarity metric and

the top words and top documents measure. Speciﬁcally there are many cases with high cosine topic

appears with a comparatively low number of top words or documents in common. Manual examination

of topics in these settings demonstrated that this was primarily connected with topics where the majority

20In Roberts et al. (2014) we examined a small open-ended survey response dataset with K = 3 and found results to be

extremely stable even under a more demanding permutation test.

24

of the probability mass loaded onto fewer than 10 words.21

Koltcov, Koltsova and Nikolenko (2014) in a similar investigation of stability in LDA, guard against

the possibility of L1 style calculations being dominated by the long-tail of infrequently occurring words.

To guard against this we tested a version where we only calculated the distance over the minimal set

of words accounting for 75% of a topic’s probability mass within the reference model. The results are

substantially the same but with slightly less noise. We opted to maintain the versions we presented above

to allow for simpler interpretation.

Alternative Approaches The similarity metrics described here are automated approximations to se-

mantic similarity. All of the metrics equally penalize deviations from the reference model regardless of

whether it is in the direction of a semantically related word or not. One solution would be to embed

words within a vector space such that semantically related words are close together and then calculate

differences relative to this space (Mikolov et al., 2013). This has the advantage of more sharply penaliz-

ing differences between topics that involve words which are semantically unrelated. However, in order

to perform the word embeddings we need an extremely large text corpus which limits the applicability

to smaller document settings.22

Finally, our focus here has primarily been on estimating similarity across a large number of models.

Chuang et al. (2013) focus on comparing two topic models and introduce a rich typology of correspon-

dence between them including topics which are fused, repeated, junk (unmatched) or resolved (well

matched) relative to the reference model. These comparisons require a bit more technical machinery but

can elegantly handle comparisons between a reference and candidate model with different numbers of

topics.

This section has presented several approaches to comparing topics across different runs of a model.

This provides not only a measure of the reference model’s stability, but it can often provide the analyst

useful diagnostic information about the contents of the topics. The discussion though leaves open the

21Chuang et al. (2013) presented a number of different distance metrics (e.g., testing KL divergence, cosine metric and
Spearman rank coefﬁcient) against human judgments of similarity. They ﬁnd that the cosine metric most directly matches
human judgment and that it could even be further improved using a rescaled dot product measure which they introduced. The
strong ﬁndings for the cosine metric provide an interesting contrast to Figure 8 and suggest that it may be perform better in
other circumstances.

22An alternate strategy is to cast the notion of distance between topics entirely in the realm of human judgments. This is
essentially the approach of Grimmer and King (2011) which offers experimental protocols for evaluating similarity between
topics.

25

important question of whether there are ways to increase the quality of model runs at the estimation

stage. In the next section we discuss approaches to initialization that maximize the quality of the initial

runs.

5

Initialization

When the function we are optimizing is well-behaved and globally concave, any starting point will re-

sult in the same global solution. Thus initialization of the parameters becomes a trivial detail, possibly

chosen to save on computational costs.23 In the multimodal setting, our initialization inﬂuences our ﬁ-

nal solution. When the computational cost of inference in the model is extremely low, we can simply

randomly initialize the parameters and repeat until we have identiﬁed the same maximum several times.

However, in latent variable models not only may we never encounter a repeat solution, but each solu-

tion to the model may be very computationally expensive, a problem which is exacerbated in big data

settings. If ﬁtting a topic model on a million documents takes a week of computational time, rerunning

it a thousand different times is not a reasonable strategy. A well-known but little-discussed aspect of

statistical optimization is that careful initialization can be an incredibly powerful tool (McLachlan and

Peel, 2004; Murphy, 2012, e.g. )

Before returning to the case of topic models, we consider the simpler case of k-means, a central

algorithm in the clustering literature closely related to the Normal mixture model discussed in Section

2.2. The k-means example helps to provide some intuition about the role of “smart” initialization. In

Section 5.2, we return to the case of topic models and discuss how simpler models such as LDA can

be used to initialize more complex models such as STM. In Section 5.3, we provide a simulation study

which shows that the LDA based initialization yields higher values of the approximate evidence lower

bound than random initialization.

The initialization approaches we consider in this section are stochastic and so each time the procedure

is repeated we may obtain a different solution. Thus our goal is to initialize such that we produce better

solutions in expectation. In special cases such as k-means, we may even be able to obtain provable

guarantees on the number of trials necessary to come within a certain tolerance of the global solution.

23We clarify well-behaved because in practice even globally convex problems can be sensitive to starting values due to

practical issues in numerical optimization.

26

An alternative approach is to explore deterministic approaches to initialization. In Section 6 we will

outline very recent research which yields deterministic initializations with excellent performance.

5.1 k-means

k-means is arguably the central algorithm of the clustering literature. Not only is it important in its own

right as a problem in clustering and computational geometry, it is also a common component of larger

systems. Because algorithms for k-means are extremely fast and easily parallelized, it has wide spread

applications in big data settings (Bishop et al., 2006).24

k-means use an alternating optimization strategy to ﬁnd a partition of units into k distinct clusters

such that Euclidean distance between the units and their nearest center is minimized. Finding the optimal

partition of units under the k-means objective function is a combinatorial optimization problem which is

known to be NP-hard (Mahajan, Nimbhorkar and Varadarajan, 2009). This manifests itself in a tendency

of k-means algorithms to get stuck in local optima. Nevertheless, it is the most widely used clustering

algorithm in practice.

Under the most popular heuristic, cluster centers are chosen randomly from the data points (Lloyd,

1982). Estimation then proceeds by iterating between assigning data points to their closest center, and

recomputing the location of the cluster center given those points. The result is an incredibly fast pro-

cedure, but one which can produce arbitrarily bad partitions relative to the global optimum (Arthur and

Vassilvitskii, 2007).

A substantial advance in the literature on the problem came with the development of the k-means++

algorithm (Arthur and Vassilvitskii, 2007). The idea is extremely simple: by using a careful seeding of

the initial centers we can make probabilistic guarantees on recovery relative to the optimal solution. The

seeding strategy is based on selecting the ﬁrst center uniformly at random from the data points and then

choosing subsequent centers at random but re-weighting to prioritize data points which are not near a

previously chosen center.

The k-means++ algorithm highlights an important general point: carefully considering the initializa-

tion procedure can be an important tool for dealing with multimodality in practice. This is an important

24By easily parallelized, we mean that it can be easily ﬁt into the Map-Reduce paradigm (Dean and Ghemawat, 2008). The

algorithm is still serial in the iterations, but the expensive calculations within each iteration can be performed in parallel.

27

distinction from problems which are globally convex, where starting values are important only for in-

creasing speed or avoiding numerical instability. It is interesting to note that despite being both simple

conceptually and incredibly effective in practice, the k-means++ heuristic was not discovered until 25

years after Lloyd’s algorithm. Heuristics for solving this problem continue to be an active area of re-

search (Bahmani et al., 2012; Nielsen and Nock, 2014).

5.2 What makes a good initialization?

A good initialization strategy needs to balance the cost of solving for the initial state with the expected

improvement in the objective. If the cost of ﬁnding the initial values of the parameters is high relative

to the model ﬁtting process then you might as well use that computational time to randomly restart the

original algorithm. Thus the art to initializing a model is ﬁnding a procedure that places the model in

the right region of the parameter space with as few calculations as possible. k-means++ is an excellent

example of an incredibly low cost initialization.

In cases where the the model itself is straightforward and the cost of of inference rises rapidly with

the number of units, a simple but powerful strategy is to run the model itself on a small subsample of the

data. This is generally a good default, particularly in the big data regime where the computation is costly

solely due to scale.

Another steadfast default approach is to initialize a complicated model with a simpler model or

algorithm for which inference is easy. The simpler algorithm can often put you into a good region of the

parameter space without expending the higher costs of the more complex method. Indeed, this is why

k-means is often used to initialize more complex mixture models (McLachlan and Peel, 2004; Bishop

et al., 2006).

In the case of STM, there is a natural simpler model, LDA. Due to the Dirichlet-Multinomial con-

jugacy in LDA we can perform inference using a fast collapsed Gibbs sampler (Grifﬁths and Steyvers,

2004). They key here is that the conjugacy of the model allows for all parameters except the token-level

topic latent variables to be integrated out. The result is a very fast sampler which has been heavily opti-

mized (Yao, Mimno and McCallum, 2009). The cost of inference is linear in the number of individual

words (tokens) in the text.25

25Also crucially the collapsed sampler mixes dramatically faster than an uncollapsed version (Carpenter, 2010; Asuncion Jr,

28

Because LDA is itself multimodal the result is an initialization which is different each time. Thus

like k-means++ this approach places STM in a good region of the parameter space but still allows for

variation across runs. The initialization for the LDA algorithm itself is just a random assignment of the

tokens, so we don’t have a problem of inﬁnite regress.

5.3 The effects of initialization

Unlike the case of k-means++ we cannot make theoretical guarantees on the quality of LDA as a method

for initializing STM.26 This naturally leads us to ask about how it performs as an initialization in practice.

To investigate this issue we compared the objective function values in the 685 model runs initialized with

LDA to a set of 50 runs initialized from random starting values.27 Figure 9 plots the resulting distributions

over the ﬁnal level of the objective function.

[Figure 9 about here.]

These substantial gains come at a very low computational cost courtesy of the efﬁcient Gibbs sampler

in the lda package (Chang, 2012). The initialization process takes only a few seconds to complete 50

iterations of the 2.6 million tokens in the Poliblog data. Indeed this is why initializing with LDA is the

current default method in the stm package in R. Furthermore, not only do the LDA initialized models

perform uniformly better they also converged signiﬁcantly more quickly. Most of the LDA models

took between 60-120 iterations to converge whereas the randomly initialized versions took close to 200

iterations. Interestingly, we were not able to increase the average quality by running the sampler for

longer, suggesting that without considerable further effort this may be close to the optimal strategy for

this type of initialization.

2011). By integrating out the topic-word distribution β we are implicitly updating the global parameters every time we take
a new sample at the document level. As a result we only need a few passes through the data to reach a good region of the
parameter space.

26Such a theoretical analysis is likely possible under a certain set of assumptions but would lead to a lengthy and technical

digression here.

27Speciﬁcally we initialize topic-word distributions with random draws from a Dirichlet distribution and set the document-
topic proportion prior mean to zero. This is the commonly used initialization procedure in many variational algorithms for
LDA.

29

6 Global Solutions

In the previous sections we discussed how non-convex models can lead to inference algorithms that

exhibit multi-modality. For the important case of topic models we provided a series of tools both for

exploring a set of local modes and for improving the average quality of our solutions through careful

initialization. These approaches work well in settings where it is feasible to run the model many times.

However, in the truly big data setting, every single optimization of the model may be so costly that we

want to strictly limit the number of times we run the model.

In this section we introduce recent innovations in theoretical computer science which allow for global

optimization of non-convex models using spectral learning. As we will show these algorithms introduce

additional assumptions into the model in order to achieve tractable inference with provable guarantees

of recovering the globally optimal parameters. Following the logic of Section 5, we use an algorithm

for LDA as an initialization to the STM. Our results suggest that this hybrid strategy can be a useful

technique for tackling big data problems.

We remind the reader that these techniques are very much “on the frontier” and so the substantive

implications for applied projects have not been charted out, something that is beyond the scope of this

chapter. Furthermore, we emphasize that these initialization strategies do not “solve” the multimodality

problem. These techniques do not yield a correct answer, and even though they do very well at maxi-

mizing the approximate evidence lower bound, this does not mean the solution is optimal with respect to

other criteria (as discussed above). The types of robustness exercises discussed above should continue to

be an important part of the research process. Nevertheless, we ﬁnd that these deterministic initialization

procedures are a promising contribution to the topic modeling toolkit.

6.1 Introduction to Spectral learning

When we deﬁne an inference procedure we would like to be able to prove that the algorithm will converge

to the global optimum. For the types of problems that we discuss here, we generally settle for heuristics

such as Expectation-Maximization, which has provable convergence to a local optimum (Dempster, Laird

and Rubin, 1977), or MCMC algorithms, which have no ﬁnite sample guarantees but will asymptotically

recover the posterior (Robert and Casella, 2004). In practice both approaches get stuck in local optima.

30

Here we describe a class of spectral learning algorithms for estimating the parameters of latent vari-

able models while retaining guarantees of globally optimal convergence.28 The key insight is that by

using matrix (or array) decomposition techniques we can recover the parameters from low order mo-

ments of the data. This approach relies on a method of moments inferential framework, as opposed

to the likelihood based framework we have adopted thus far (Pearson, 1894; King, 1989; Anandkumar,

Ge, Hsu, Kakade and Telgarsky, 2012). In models with certain structures this can lead to procedures

with provable theoretical guarantees of recovering the true parameters as well as algorithms which are

naturally scalable.

Spectral algorithms have been applied to a wide array of models include: Gaussian mixture models

(Hsu and Kakade, 2013), Hidden Markov Models (Anandkumar, Hsu and Kakade, 2012), latent tree

models (Song, Xing and Parikh, 2011), community detection on a graph (Anandkumar, Ge, Hsu and

Kakade, 2013), dictionary learning (Arora, Ge and Moitra, 2013) and many others (Anandkumar, Ge,

Hsu, Kakade and Telgarsky, 2012). Of particular interest for our purposes is the development of spectral

approaches to estimating topic models (Arora, Ge and Moitra, 2012; Anandkumar, Liu, Hsu, Foster

and Kakade, 2012). There are two basic approaches to spectral learning in LDA, which differ in their

assumptions and methods. For clarity we focus on a simple and scalable algorithm developed in Arora,

Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013).

The discussion of these methods is unavoidably more technical than the previous material. However,

the common theme is straightforward: we are making stronger assumptions about the model in order

to obtain an algorithm that does not suffer from problems of local modes. Importantly for our case we

use the spectral algorithm as an initialization rather than as a procedure to ﬁt the model. In doing so

we weaken our reliance on the assumptions in the spectral algorithm while still achieving its desirable

properties.

In this sense the spectral learning algorithms are complementary to the likelihood based

approach we have considered here (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012).

28Spectral methods derive their name from the use of tools from linear algebra which are connected to the spectral the-
orem. Here we use an inclusive deﬁnition of spectral learning which includes methods using a variety of matrix and array
decomposition techniques beyond the canonical Singular Value Decomposition.

31

6.2 An Algorithm for LDA

Here we brieﬂy describe the intuition behind the inference algorithm of Arora, Ge, Halpern, Mimno,

Moitra, Sontag, Wu and Zhu (2013) which uses a non-negative matrix factorization (NMF)29 to recover

the model parameters from the word co-occurrence matrix, as we show below, to separate the β parameter

(the topic distributions) from the data. The main input to the algorithm is a matrix of word-word co-

occurrences which is of size V -by-V where V is the number of the words in the vocabulary. Normalizing

this matrix so all entries sum to 1, we get the matrix Q. If we assume that Q is constructed from an

inﬁnite number of documents then it is the second order moment matrix and the element Qi,j has the

interpretation as the probability of observing word i and word j in the same document. We can write the

Q matrix in terms of the model parameters as,

Q = E(cid:2)βT θT θβ(cid:3)
= βT E(cid:2)θT θ(cid:3) β

(1)

(2)

where the second line follows by treating the parameters as ﬁxed but unknown. Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu (2013) show that we can recover βT from the rest of the parameters

using a non-negative Matrix Factorization.

The NMF problem is also NP-hard in general (Vavasis, 2009) and suffers from the same local mode

problems as LDA in practice (Gillis, 2014). However recent work by Arora, Ge, Kannan and Moitra

(2012) showed that we can provably compute the NMF for the class of matrices that satisfy the separa-

bility condition (Donoho and Stodden, 2003). In this context, separability assumes that for each topic

there is at least one word, called an anchor word, which is assigned only to that topic. The anchor word

for topic k does not need to be in every document about topic k, but if a document contains the anchor

word, we know that it is at least partially about topic k. Separability implies that all non-anchor word

rows of the Q matrix can be recovered as a convex combination of the anchor rows (Arora, Ge, Halpern,

Mimno, Moitra, Sontag, Wu and Zhu, 2013). Thus if we can identify the anchors, we can solve for β

29NMF is similar to a Singular Value Decomposition except that all elements of the decomposition are constrained to be

non-negative.

32

using convex optimization methods.

Thus the algorithm of Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) proceeds in

two parts. First we identify the anchors, and then given the anchors we uncover the model parameters β.

Crucially these steps do not need to be iterated and are not sensitive to the starting values of the algorithm.

There are many different approaches to these two steps that differ in computational complexity and

robustness to noise (Kumar, Sindhwani and Kambadur, 2012; Recht et al., 2012; Gillis and Luce, 2013;

Ding, Rohban, Ishwar and Saligrama, 2013).30

Advantages The main advantage of the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu

(2013) algorithm is that we can give theoretical guarantees that it will recover the optimal parameters

(given the model and separability assumption). In practice this means that we completely side-step the

multi-modality concerns described in this chapter. The second crucial advantage is that the method is

extremely scalable. Note that Q is V -by-V and thus the algorithm does not increase in complexity with

the number of documents. This means that for a ﬁxed vocabulary size, the cost of doing inference on

a million documents is essentially the same as inference for a hundred. This is an incredibly useful

property for the big data setting. Many of the algorithms cited above for other models are similarly

scalable.31

Disadvantages Naturally there are practical drawbacks to spectral algorithms. Because we are sub-

stituting the observed sample moments for the population moments, spectral methods require a lot of

data to perform well. In experiments on synthetic data reported in Arora, Ge, Halpern, Mimno, Moitra,

Sontag, Wu and Zhu (2013), spectral methods only approach the accuracy of Gibbs sampling at around

40,000 documents. This is particularly troubling as the power-law distribution of natural language in-

sures that we will need an incredibly large number of documents to estimate co-occurrences of highly

infrequent words. In practice this is addressed by ﬁltering out low frequency words before performing

30Anchor selection methods use either a sparse regression framework (Recht et al., 2012) or appeal to geometric properties
of the anchors (Kumar, Sindhwani and Kambadur, 2012). See Gillis (2014) for a summary of these approaches. For our
experiments here we focus the approach deﬁned in Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) which
falls into the geometric properties camp. They use a combinatorial search based on a modiﬁed Gram Schmidt orthogonaliza-
tion process for the anchor selection. Parameter recovery then uses an exponentiated gradient descent algorithm (Kivinen and
Warmuth, 1997) with an L2 norm loss.

31A good example is the mixed membership stochastic blockmodel, which is loosely speaking LDA for community detec-
tion on a network (Airoldi et al., 2009). Huang et al. (2013) give a spectral algorithm which learns hundreds of communities
in a network of millions of nodes in under 10 minutes.

33

anchor selection.

The second major concern is that spectral methods lean more heavily on the model assumptions

which can lead to somewhat less interpretable models in real data (Nguyen, Hu and Boyd-Graber, 2014).

Finally, as a practical matter the spectral method only recovers the topic word distributions β so addi-

tional methods are still required to infer the document-topic proportions. These can be obtained by a

single pass of Gibbs sampling or variational inference (Roberts et al., 2014).

6.3 Spectral Learning as Initialization

Here we apply the Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu and Zhu (2013) algorithm as an

initialization for the structural topic model. Using the spectral method as an initialization weakens our

reliance on the assumptions of the methods. For example, our initialization will have anchor words,

but once we begin variational inference of STM those anchor words are free to move some of their

probability mass onto other topics. Thus we simply use the spectral algorithm to place us into an optimal

region of the space. Because the spectral method is deterministic we also only need to run the model

once.

We apply the algorithm as an initialization for the same 100 topic model of the Poliblog corpus used

previously. Note that the approximately thirteen thousand document corpus is smaller than previous

ﬁndings would suggest are necessary to match the quality of Gibbs sampling.

[Figure 10 about here.]

Figure 10 shows the results of the model with the spectral initialization. Not only is the result dra-

matically better with respect to the lower bound than the random and LDA initializations but the model

converged considerably faster as well.32 Because our focus here is on introducing this class of algorithms

we do not go through the process of reinterpreting the 100 topics model.

6.4 Future Directions

Spectral algorithms are a very active area of current research. Here we have focused on a particular

algorithm which leverages nonnegative matrix factorization under a separability assumption. There have

32It took 25 iterations to converge after the spectral initialization compared to 60 iterations for LDA initialization and close

to 200 iterations for random initialization.

34

been several algorithmic improvements since Arora, Ge, Kannan and Moitra (2012) introduced the an-

chor based method (Recht et al., 2012; Kumar, Sindhwani and Kambadur, 2012; Ding, Rohban, Ishwar

and Saligrama, 2013; Gillis and Luce, 2013; Gillis, 2014; Zhou, Bilmes and Guestrin, 2014). There

has also been substantial work applying the approach to other problem domains (Arora, Ge, Moitra and

Sachdeva, 2012; Arora, Ge and Moitra, 2013; Arora, Bhaskara, Ge and Ma, 2013; Zhou, Bilmes and

Guestrin, 2014).

A separate line of work uses higher order moments of the data along with tools for array (tensor)

decomposition (Anandkumar, Ge, Hsu, Kakade and Telgarsky, 2012). These methods have also resulted

in algorithms for an incredibly rich set of applications and models. Importantly we can also use this

framework to develop algorithms for LDA with provable global convergence guarantees (Anandkumar,

Liu, Hsu, Foster and Kakade, 2012; Anandkumar, Hsu, Javanmard and Kakade, 2013).33 This work

differs in both the assumptions and methods used. Crucially the tensor method of moments approach

uses the third moments of the data which may require an even higher sample size to accurately estimate.34

7 Conclusion

Alongside rapid increases in data and processing power has been the development and deployment of

a range of new data analysis tools. All of these tools enable new insights and new ways of looking

at data than even a decade ago would have been difﬁcult. In this chapter, we focus on the problem of

multi-modality that affects many of these tools, with a speciﬁc focus on topic models for textual data.

The purpose of this chapter has been to convey an understanding of where this multimodality comes and

then engage in a sustained discussion about what to do about multimodality from an applied perspective

when analyzing text data.

Any modeling approach requires transparency about both process and guiding principles. The topic

models we focus on in this paper are no different in this respect from more traditional statistical tools.

33Technically the work in Anandkumar, Liu, Hsu, Foster and Kakade (2012) uses an approach called Excess Correlation
Analysis which involves two singular value decompositions on the second and third moments of the data. The approach based
on the tensor method of moments strategy is described in Anandkumar, Ge, Hsu, Kakade and Telgarsky (2012) and applies to
a wider class of models. We collect them together here because they emerged from the same research group and use similar
techniques.

34An excellent discussion of differing assumptions of spectral methods is given in Ding, Ishwar, Rohban and Saligrama

(2013).

35

Even in traditional general linear models, there is also always the choice of model speciﬁcation in both

variables and functional form. Although multimodality brings new issues to the table, the responsibility

of the researcher to carefully validate the chosen model is fundamentally the same. This is true regardless

of whether the choice between competing models arises due to a non-convex latent variable model or

due to the selection of an important model tuning parameter in a globally convex problem. Thus even if

multimodality is an unfamiliar problem, social scientists can draw on the same set of best practices that

they employ throughout their research.

An important practical contribution of this chapter is that it extends the set of tools available to

scholars using topic models in applied research. While we have focused on STM, many of the procedures

we use will be helpful for a broader class of latent variable models. For instance, the approaches to

aligning topics and calculating stability across runs can all be applied directly to the broader class of

statistical topic models and with minor modiﬁcations to most latent variable models.

We see great potential for the analysis of “big” data in the social sciences, but rather than focus on

the data we have taken a more methodological focus. We think this has important implications for both

methodological development but also could structure the types of questions we ask, and the types of data

sets we seek to build. Methodologically, we think that there will be important advances in areas such

as optimal initialization strategies, which is especially important as our data sets grow in size. From an

applied perspective, users will be unlikely to want to wait for extended periods of time in order to get

even a single set of results. Advances in computational power needs to be matched with smart ways to

leverage that power. From a research design perspective, we think more focus should be put on bringing

greater structure to so called “unstructured” data. In the STM we focus on the inclusion of metadata

for modeling and hypothesis testing, but this is only one possible use. Can more direct supervision help

us with issues of multimodality? Of course, in the end, big data will be at its best when there is active

dialogue between those who pose the big question and those who might provide the big answers.

References
Airoldi, Edoardo M, David M Blei, Stephen E Fienberg and Eric P Xing. 2009. Mixed membership

stochastic blockmodels. In Advances in Neural Information Processing Systems. pp. 33–40.

Anandkumar, Anima, Rong Ge, Daniel Hsu and Sham M Kakade. 2013. “A tensor spectral approach to

learning mixed membership community models.” arXiv preprint arXiv:1302.2684 .

36

Anandkumar, Anima, Rong Ge, Daniel Hsu, Sham M Kakade and Matus Telgarsky. 2012. “Tensor

decompositions for learning latent variable models.” arXiv preprint arXiv:1210.7559 .

Anandkumar, Anima, Yi-kai Liu, Daniel J Hsu, Dean P Foster and Sham M Kakade. 2012. A spec-
tral algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems.
pp. 917–925.

Anandkumar, Animashree, Daniel Hsu, Adel Javanmard and Sham Kakade. 2013. Learning linear
bayesian networks with latent variables. In Proceedings of The 30th International Conference on Ma-
chine Learning. pp. 249–257.

Anandkumar, Animashree, Daniel Hsu and Sham M Kakade. 2012. “A method of moments for mixture

models and hidden Markov models.” arXiv preprint arXiv:1203.0683 .

Arora, Sanjeev, Aditya Bhaskara, Rong Ge and Tengyu Ma. 2013. “Provable bounds for learning some

deep representations.” arXiv preprint .

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2012. Learning topic models–going beyond svd. In Foun-

dations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE pp. 1–10.

Arora, Sanjeev, Rong Ge and Ankur Moitra. 2013. “New algorithms for learning incoherent and over-

complete dictionaries.” arXiv preprint arXiv:1308.6273 .

Arora, Sanjeev, Rong Ge, Ankur Moitra and Sushant Sachdeva. 2012. Provable ICA with unknown
Gaussian noise, with implications for Gaussian mixtures and autoencoders. In Advances in Neural
Information Processing Systems. pp. 2375–2383.

Arora, Sanjeev, Rong Ge, Ravindran Kannan and Ankur Moitra. 2012. Computing a nonnegative ma-
trix factorization–provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of
computing. ACM pp. 145–162.

Arora, Sanjeev, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu
and Michael Zhu. 2013. A Practical Algorithm for Topic Modeling with Provable Guarantees. In
Proceedings of The 30th International Conference on Machine Learning. pp. 280–288.

Arthur, David and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proceed-
ings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial
and Applied Mathematics pp. 1027–1035.

Asuncion Jr, Arthur Uy. 2011. Distributed and accelerated inference algorithms for probabilistic graph-

ical models. Technical report California State University at Long Beach.

Bahmani, Bahman, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei Vassilvitskii. 2012.

“Scalable k-means++.” Proceedings of the VLDB Endowment 5(7):622–633.

Beck, Nathaniel, Gary King and Langche Zeng. 2000. “Improving quantitative studies of international

conﬂict: A conjecture.” American Political Science Review pp. 21–35.

Belloni, Alexandre, Victor Chernozhukov and Christian Hansen. 2014. “High-Dimensional Methods and

Inference on Structural and Treatment Effects.” The Journal of Economic Perspectives 28(2):29–50.

37

Bishop, Christopher M et al. 2006. Pattern recognition and machine learning. Vol. 1 springer New York.

Blei, David M. 2012. “Probabilistic topic models.” Communications of the ACM 55(4):77–84.

Blei, David M. 2014. “Build, compute, critique, repeat: data analysis with latent variable models.”

Annual Review of Statistics and Its Application 1:203–232.

Blei, David M, Andrew Y Ng and Michael I Jordan. 2003. “Latent dirichlet allocation.” the Journal of

machine Learning research 3:993–1022.

Boyd, Stephen and Lieven Vandenberghe. 2009. Convex optimization. Cambridge university press.

Carpenter, Bob. 2010. Integrating out multinomial parameters in latent Dirichlet allocation and naive

Bayes for collapsed Gibbs sampling. Technical report Technical report, LingPipe.

Chaney, Allison June-Barlow and David M Blei. 2012. Visualizing Topic Models. In ICWSM.

Chang, Jonathan. 2012. lda: Collapsed Gibbs sampling methods for topic models. R package version

1.3.2.
URL: http://CRAN.R-project.org/package=lda

Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-graber and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In Advances in neural information processing systems.
pp. 288–296.

Chuang, Jason, Christopher D Manning and Jeffrey Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of the International Working Conference on Advanced
Visual Interfaces. ACM pp. 74–77.

Chuang, Jason, Sonal Gupta, Christopher Manning and Jeffrey Heer. 2013. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In Proceedings of the 30th International Conference
on Machine Learning (ICML-13). pp. 612–620.

Cochocki, A and Rolf Unbehauen. 1993. Neural networks for optimization and signal processing. John

Wiley & Sons, Inc.

Council, National Research. 2013. Frontiers in Massive Data Analysis. The National Academies Press.

de Klerk, Etienne and Dmitrii V Pasechnik. 2002. “Approximation of the stability number of a graph via

copositive programming.” SIAM Journal on Optimization 12(4):875–892.

De Marchi, Scott, Christopher Gelpi and Jeffrey D Grynaviski. 2004. “Untangling neural nets.” American

Political Science Review 98(02):371–378.

Dean, Jeffrey and Sanjay Ghemawat. 2008. “MapReduce: simpliﬁed data processing on large clusters.”

Communications of the ACM 51(1):107–113.

Deb, Partha and Pravin K Trivedi. 2002. “The structure of demand for health care: latent class versus

two-part models.” Journal of health economics 21(4):601–625.

Dempster, Arthur P, Nan M Laird and Donald B Rubin. 1977. “Maximum likelihood from incomplete
data via the EM algorithm.” Journal of the Royal Statistical Society. Series B (Methodological) pp. 1–
38.

38

Ding, Weicong, Mohammad H Rohban, Prakash Ishwar and Venkatesh Saligrama. 2013. “Topic discov-

ery through data dependent and random projections.” arXiv preprint arXiv:1303.3664 .

Ding, Weicong, Prakash Ishwar, Mohammad H Rohban and Venkatesh Saligrama. 2013. “Necessary
and Sufﬁcient Conditions for Novel Word Detection in Separable Topic Models.” arXiv preprint
arXiv:1310.7994 .

Donoho, David and Victoria Stodden. 2003. When does non-negative matrix factorization give a correct

decomposition into parts? In Advances in neural information processing systems. p. None.

Doshi-Velez, Finale, Yaorong Ge and Isaac Kohane. 2014. “Comorbidity clusters in autism spectrum

disorders: an electronic health record time-series analysis.” Pediatrics 133(1):e54–e63.

Drton, Mathias and Thomas S Richardson. 2004. “Multimodality of the likelihood in the bivariate

seemingly unrelated regressions model.” Biometrika 91(2):383–392.

DuMouchel, William. 1999. “Bayesian data mining in large frequency tables, with an application to the

FDA spontaneous reporting system.” The American Statistician 53(3):177–190.

Efron, Bradley et al. 1978. “The geometry of exponential families.” The Annals of Statistics 6(2):362–

376.

Eisenstein, Jacob and Eric Xing. 2010. “The CMU 2008 Political Blog Corpus.”.

Fan, Jianqing, Fang Han and Han Liu. 2014. “Challenges of Big Data analysis.” National Science Review

p. nwt032.

Foulds, J. R. and P. Smyth. 2014. Annealing Paths for the Evaluation of Topic Models. In Proceedings

of the Thirtieth Conference Conference on Uncertainty in Artiﬁcial Intelligence.

Gardner, Matthew J, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric Ringger and Kevin Seppi.
2010. The topic browser: An interactive tool for browsing topic models. In NIPS Workshop on Chal-
lenges of Data Visualization.

Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari and Donald B Rubin. 2013.

Bayesian data analysis. CRC press.

Gillis, Nicolas. 2014.

arXiv:1401.5226 .

“The why and how of nonnegative matrix factorization.” arXiv preprint

Gillis, Nicolas and Robert Luce. 2013. “Robust near-separable nonnegative matrix factorization using

linear optimization.” arXiv preprint arXiv:1302.4385 .

Goldstone, AndrewUnderwood et al. 2014. “The Quiet Transformations of Literary Studies: What

Thirteen Thousand Scholars Could Tell Us.”.

Grifﬁths, DMBTL and MIJJB Tenenbaum. 2004. “Hierarchical topic models and the nested Chinese

restaurant process.” Advances in neural information processing systems 16:17.

Grifﬁths, Thomas L and Mark Steyvers. 2004. “Finding scientiﬁc topics.” Proceedings of the National

academy of Sciences of the United States of America 101(Suppl 1):5228–5235.

39

Grimmer, Justin. 2010a. “A Bayesian hierarchical topic model for political texts: Measuring expressed

agendas in Senate press releases.” Political Analysis 18(1):1–35.

Grimmer, Justin. 2010b. “An introduction to Bayesian inference via variational approximations.” Politi-

cal Analysis p. mpq027.

Grimmer, Justin. 2013. Representational Style in Congress: What Legislators Say and why it Matters.

Cambridge University Press.

Grimmer, Justin and Brandon M Stewart. 2013. “Text as data: The promise and pitfalls of automatic

content analysis methods for political texts.” Political Analysis p. mps028.

Grimmer, Justin and Gary King. 2011. “General purpose computer-assisted clustering and conceptual-

ization.” Proceedings of the National Academy of Sciences 108(7):2643–2650.

Hambleton, Ronald K. 1991. Fundamentals of item response theory. Vol. 2 Sage publications.

Hamilton, James D. 1989. “A new approach to the economic analysis of nonstationary time series and

the business cycle.” Econometrica: Journal of the Econometric Society pp. 357–384.

Hoff, Peter D, Adrian E Raftery and Mark S Handcock. 2002. “Latent space approaches to social network

analysis.” Journal of the american Statistical association 97(460):1090–1098.

Hopkins, Daniel J and Gary King. 2010. “A method of automated nonparametric content analysis for

social science.” American Journal of Political Science 54(1):229–247.

Hornik, Kurt. 2005. “A clue for cluster ensembles.” Journal of Statistical Software 14(12).

Hsu, Daniel and Sham M Kakade. 2013. Learning mixtures of spherical gaussians: moment methods and
spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer
Science. ACM pp. 11–20.

Hu, Yuening, Jordan Boyd-Graber, Brianna Satinoff and Alison Smith. 2014. “Interactive topic model-

ing.” Machine Learning 95(3):423–469.

Huang, Furong, UN Niranjan, M Hakeem and Animashree Anandkumar. 2013. “Fast Detection of

Overlapping Communities via Online Tensor Methods.”.

Jordan, Michael I, Zoubin Ghahramani, Tommi S Jaakkola and Lawrence K Saul. 1998. An introduction

to variational methods for graphical models. Springer.

Kalai, Adam Tauman, Ankur Moitra and Gregory Valiant. 2012. “Disentangling gaussians.” Communi-

cations of the ACM 55(2):113–120.

Kass, Robert E and Adrian E Raftery. 1995. “Bayes factors.” Journal of the american statistical associ-

ation 90(430):773–795.

King, Gary. 1989. “Unifying political methodology.” New York: Cambridge .

King, Gary, Jennifer Pan and Margaret E Roberts. 2013. “How censorship in China allows government

criticism but silences collective expression.” American Political Science Review 107(02):326–343.

40

Kivinen, Jyrki and Manfred K Warmuth. 1997. “Exponentiated gradient versus gradient descent for

linear predictors.” Information and Computation 132(1):1–63.

Koltcov, Sergei, Olessia Koltsova and Sergey Nikolenko. 2014. Latent dirichlet allocation: stability and
applications to studies of user-generated content. In Proceedings of the 2014 ACM conference on Web
science. ACM pp. 161–165.

Krippendorff, Klaus. 2012. Content analysis: An introduction to its methodology. Sage.

Kuhn, Harold W. 1955. “The Hungarian method for the assignment problem.” Naval research logistics

quarterly 2(1-2):83–97.

Kumar, Abhishek, Vikas Sindhwani and Prabhanjan Kambadur. 2012. “Fast conical hull algorithms for

near-separable non-negative matrix factorization.” arXiv preprint arXiv:1210.1190 .

Lancichinetti, Andrea, M Irmak Sirer, Jane X Wang, Daniel Acuna, Konrad K¨ording and Lu´ıs A Nunes
Amaral. 2014. “A high-reproducibility and high-accuracy method for automated topic classiﬁcation.”
arXiv preprint arXiv:1402.0422 .

Lau, Jey Han, David Newman and Timothy Baldwin. 2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality. In Proceedings of the European Chapter of the
Association for Computational Linguistics.

Lauderdale, Benjamin E and Tom S Clark. 2014. “Scaling politically meaningful dimensions using texts

and votes.” American Journal of Political Science .

Lazer, David, Alex Sandy Pentland, Lada Adamic, Sinan Aral, Albert Laszlo Barabasi, Devon Brewer,
Nicholas Christakis, Noshir Contractor, James Fowler, Myron Gutmann et al. 2009. “Life in the
network: the coming age of computational social science.” Science (New York, NY) 323(5915):721.

Lim, Yew Jin and Yee Whye Teh. 2007. Variational Bayesian approach to movie rating prediction. In

Proceedings of KDD Cup and Workshop. Vol. 7 Citeseer pp. 15–21.

Lloyd, Stuart. 1982. “Least squares quantization in PCM.” Information Theory, IEEE Transactions on

28(2):129–137.

Mahajan, Meena, Prajakta Nimbhorkar and Kasturi Varadarajan. 2009. The planar k-means problem is

NP-hard. In WALCOM: Algorithms and Computation. Springer pp. 274–285.

McLachlan, Geoffrey and David Peel. 2004. Finite mixture models. John Wiley & Sons.

Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality. In Advances in Neural Information Processing
Systems. pp. 3111–3119.

Mimno, David and David Blei. 2011. Bayesian checking for topic models. In Proceedings of the Con-
ference on Empirical Methods in Natural Language Processing. Association for Computational Lin-
guistics pp. 227–237.

Mimno, David, Hanna M Wallach, Edmund Talley, Miriam Leenders and Andrew McCallum. 2011. Op-
timizing semantic coherence in topic models. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing. Association for Computational Linguistics pp. 262–272.

41

Mullainathan, Sendhil. 2014. “What Big Data Means for Social Science.” Behavioral and Experimental

Seminar.

Murphy, Kevin P. 2012. Machine learning: a probabilistic perspective. MIT press.

Nguyen, Thang, Yuening Hu and Jordan Boyd-Graber. 2014. Anchors Regularized: Adding Robustness
and Extensibility to Scalable Topic-Modeling Algorithms. In Association for Computational Linguis-
tics.

Nielsen, Frank and Richard Nock. 2014. “Further heuristics for k-means: The merge-and-split heuristic

and the (k, l)-means.” arXiv preprint arXiv:1406.6314 .

O’Connor, Brendan. 2014. MiTextExplorer: Linked brushing and mutual information for exploratory
text data analysis. In Proceedings of the Workshop on Interactive Language Learning, Visualization,
and Interfaces. Association for Computational Linguistics pp. 1–13.

OConnor, Brendan, Brandon M Stewart and Noah A Smith. 2013. Learning to Extract International

Relations from Political Context. In Proceedings of ACL.

Papadimitriou, Christos H and Kenneth Steiglitz. 1998. Combinatorial optimization: algorithms and

complexity. Courier Dover Publications.

Park, Jong Hee. 2012. “A Uniﬁed Method for Dynamic and Cross-Sectional Heterogeneity: Introducing

Hidden Markov Panel Models.” American Journal of Political Science 56(4):1040–1054.

Pearson, Karl. 1894. “Contributions to the mathematical theory of evolution.” Philosophical Transac-

tions of the Royal Society of London. A pp. 71–110.

Poole, Keith T and Howard Rosenthal. 1997. Congress: A political-economic history of roll call voting.

Oxford University Press.

Quinn, Kevin M, Burt L Monroe, Michael Colaresi, Michael H Crespin and Dragomir R Radev. 2010.
“How to analyze political attention with minimal assumptions and costs.” American Journal of Politi-
cal Science 54(1):209–228.

Rabiner, Lawrence and Biing-Hwang Juang. 1986. “An introduction to hidden Markov models.” ASSP

Magazine, IEEE 3(1):4–16.

Recht, Ben, Christopher Re, Joel Tropp and Victor Bittorf. 2012. Factoring nonnegative matrices with

linear programs. In Advances in Neural Information Processing Systems. pp. 1214–1222.

Reich, Justin, Dustin Tingley, Jetson Leder-Luis, Margaret E Roberts and Brandon M Stewart. N.d.
Computer Assisted Reading and Discovery for Student Generated Text. Technical report Working
Paper. Export.

Rivers, Douglas. 2003. “Identiﬁcation of multidimensional spatial voting models.” Typescript. Stanford

University .

Robert, Christian P and George Casella. 2004. Monte Carlo statistical methods. Vol. 319 Springer New

York.

42

Roberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis,
Shana Kushner Gadarian, Bethany Albertson and David G Rand. 2014. “Structural Topic Models
for Open-Ended Survey Responses.” American Journal of Political Science .

Ruiz, Francisco J. R., Isabel Valera, Carlos Blanco and Fernando Perez-Cruz. 2014. “Bayesian Non-
parametric Comorbidity Analysis of Psychiatric Disorders.” Journal of Machine Learning Research
15:1215–1247.
URL: http://jmlr.org/papers/v15/ruiz14a.html

Shneiderman, Ben. 1996. The eyes have it: A task by data type taxonomy for information visualizations.

In Visual Languages, 1996. Proceedings., IEEE Symposium on. IEEE pp. 336–343.

Song, Le, Eric P Xing and Ankur P Parikh. 2011. A spectral algorithm for latent tree graphical models. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11). pp. 1065–1072.

Sontag, David and Dan Roy. 2011. Complexity of inference in latent dirichlet allocation. In Advances in

Neural Information Processing Systems. pp. 1008–1016.

Srivastava, Virendera K and David EA Giles. 1987. Seemingly unrelated regression equations models:

estimation and inference. Vol. 80 CRC Press.

Teh, Yee Whye, Michael I Jordan, Matthew J Beal and David M Blei. 2006. “Hierarchical dirichlet

processes.” Journal of the american statistical association 101(476).

Tenenbaum, Joshua B, Charles Kemp, Thomas L Grifﬁths and Noah D Goodman. 2011. “How to grow

a mind: Statistics, structure, and abstraction.” science 331(6022):1279–1285.

Tibshirani, Robert. 1996. “Regression shrinkage and selection via the lasso.” Journal of the Royal Sta-

tistical Society. Series B (Methodological) pp. 267–288.

Vavasis, Stephen A. 2009. “On the complexity of nonnegative matrix factorization.” SIAM Journal on

Optimization 20(3):1364–1377.

Wallach, Hanna M, Iain Murray, Ruslan Salakhutdinov and David Mimno. 2009. Evaluation methods
for topic models. In Proceedings of the 26th Annual International Conference on Machine Learning.
ACM pp. 1105–1112.

Ward, Michael D, Brian D Greenhill and Kristin M Bakke. 2010. “The perils of policy by p-value:

Predicting civil conﬂicts.” Journal of Peace Research 47(4):363–375.

Wolpert, David H and William G Macready. 1997. “No free lunch theorems for optimization.” Evolu-

tionary Computation, IEEE Transactions on 1(1):67–82.

Yao, Limin, David Mimno and Andrew McCallum. 2009. Efﬁcient methods for topic model inference on
streaming document collections. In Proceedings of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining. ACM pp. 937–946.

Zhou, Tianyi, Jeff Bilmes and Carlos Guestrin. 2014. “Divide-and-Conquer Learning by Anchoring a

Conical Hull.” arXiv preprint arXiv:1406.5752 .

43

Figure 1: Five example topics from the reference model. These are given the labels Supreme Court,
Cheney, Global Warming, Iran/N.K. Nukes, and Wright respectively.

44

Topic 18:  law, court, rule, constitut, right, judg, decis, suprem, legal, justic, case,feder, requir, amend, protect, gun, govern, allow, appeal, citizenTopic 30:  presid, vice, cheney, offic, presidenti, first, execut, dick, decis, leader,role, histori, nation, branch, power, part, govern, order, idea, washingtonTopic 48:  global, warm, research, climat, studi, chang, scienc, scientist, gore, caus,human, scientif, earth, emiss, planet, cell, environment, report, water, greenTopic 60:  iran, nuclear, threat, weapon, iranian, program, missil, north, bomb, defens,korea, strike, sanction, intern, build, militari, intellig, capabl, pose,developTopic 71:  black, wright, white, race, church, racial, racist, pastor, jeremiah,africanamerican, racism, african, comment, reverend, king, controversi, rev,view, communiti, southFigure 2: Relation between three measures of topic similarity across all topics and modes. Plotted
surface is a kernel smoothed density estimate.

45

02468100.00.51.01.52.0L1 and Top WordsTop 10 Words in CommonL1 Distance02468100.00.51.01.52.0L1 and Top DocsTop 10 Docs in CommonL1 Distance02468100246810Words and DocsTop 10 Docs in CommonTop 10 Words in CommonFigure 3: Comparison between the approximation to the bound on the marginal likelihood (the objective
function) with similarity metrics aggregated to the model level.

46

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.40.50.60.70.80.91.0Bound and WordsBoundProportion of 10 Words in Commonlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.40.60.81.0Bound and DocsBoundProportion of 10 Docs in Commonlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−17870000−17850000−178300000.00.20.40.60.81.01.2Bound and L1BoundExpected L1 DistFigure 4: Aggregation of similarity metrics to the topic level.

47

Word Expectations over RunsExpected Common WordsFrequency2468051015CourtCheneyClimateIran/N.K.WrightDoc Expectations over RunsExpected Common DocsFrequency0246802468101214CourtCheneyClimateIran/N.K.WrightL1 Expectations over RunsExpected DistFrequency0.51.01.50246810CourtCheneyClimateIran/N.K.WrightFigure 5: Differences in topical coverage by rating (controlling for time). Effects to the right of 0
indicate a topic more heavily used by Conservatives. Lines indicate 95% conﬁdence intervals using the
“global” approximation to measurement uncertainty (Roberts et al., 2014).

48

l−0.006−0.004−0.0020.0000.0020.0040.006Partisan Rating Effects by TopicExpected Difference in Topic Proportion (Conservative minus Liberal)lWrightlIran/North Korea NukelGlobal WarminglCheneylSupreme CourtFigure 6: The ﬁrst 300 characters of the three posts most associated with the global warming topic.
Posts 1 and 3 come from American Thinker and post 2 comes from Think Progress.

49

Deathly news for the religion of Global Warming. Looks like at least oneprominent scientific group has changed its mind about the irrefutability ofevidence regarding man made climate change.The American Physical Societyrepresenting nearly 50,000 physicists "has reversed its stance on climateClimate change report forecasts global sea levels to rise up to 4 feet by 2100.According to a new report led by the U.S. Geological Survey, the U.S. â€œfacesthe possibility of much more rapid climate change by the end of the centurythan previous studies have suggested.â€(cid:157) The report,NASA has confirmed that a developing natural climate pattern will likely resultin much colder temperatures.Â(cid:160) Of course, the climate alarmists' favoritedubious data source was also quick to point out that such natural phenomenashould not confuse the issue of manmade greenhouse gas induced globalFigure 7: Distribution of the partisan rating effect across modes for the ﬁve example topics. The black
solid line shows the effect at the reference mode and the black dashed line marks an effect size of 0.

50

Supreme CourtRating EffectFrequency−0.008−0.006−0.004−0.0020.0000.002020406080No effectReference modelCheneyRating EffectFrequency−0.010−0.0050.000020406080Global WarmingRating EffectFrequency0.0000.0010.0020.0030.0040.0050.006020406080100Iran/N.K. NukesRating EffectFrequency−0.010−0.0050.0000.0050.010050100150Rev. WrightRating EffectFrequency−0.0020.0000.0020.0040.006020406080100120Figure 8: Comparison of metric based on cosine similarity.

51

02468100.00.20.40.60.81.0Cosine Sim. and Top WordsTop 10 Words in CommonCosine Similarity02468100.00.20.40.60.81.0Cosine Sim. and Top DocsTop 10 Docs in CommonCosine Similarity02468100246810Words and DocsTop 10 Docs in CommonTop 10 Words in CommonFigure 9: A comparison of initialization strategies for the K = 100 STM models.

52

Comparing Initialization StrategiesLower Bound at ConvergenceDensity−17960000−17920000−17880000−178400000e+001e−052e−053e−054e−05RandomLDAFigure 10: A comparison of the spectral initialization strategy to random and LDA for the K = 100
STM models. The green dashed denotes the result of the spectral initialized solution.

53

Comparing Initialization StrategiesLower Bound at ConvergenceDensity−17950000−17900000−17850000−17800000−177500000e+001e−052e−053e−054e−05RandomLDASpectralTerm Ref. Model Local Model

execut
ﬁrst

administr < .0005
bush < .0005
bush’ < .0005
cheney
0.0464
decis
0.0178
dick 0.0195
0.0226
0.0253
georg < .0005
histori
0.0104
leader
0.0134
nation 0.0102
0.0414
presid 0.5302
presidenti
0.0254
role
0.0129
term 0.0025
vice
0.0512

ofﬁc

0.104
0.275
0.0191
0.0279
0.0060
0.0109
0.0022
0.0001
0.0480
0.0099
< .0005
< .0005
0.0209
0.2868
0.0003
0.0001
0.0130
0.0251

Table 1: The topic-speciﬁc probabilities of observing 18 words in the Cheney topic in both the reference
model and a local solution far away from it. Included words have a probability of at least 0.01 under
one of the two versions of the topics. The reference model topic is focused primarily on Vice President
Cheney whereas the local mode includes broader coverage of the Bush presidency.

54

