Classiﬁcation of photographic images based on

perceived aesthetic quality

Jeff Hwang
Department of Electrical Engineering, Stanford University

Sean Shi
Department of Electrical Engineering, Stanford University

JEFF.HWANG@STANFORD.EDU

SSHI11@STANFORD.EDU

Abstract

In this paper, we explore automated aes-
thetic evaluation of photographs using ma-
chine learning and image processing tech-
niques. We theorize that the spatial dis-
tribution of certain visual elements within
a given image correlates with its aesthetic
quality. To this end, we present a novel ap-
proach wherein we model each photograph
as a set of tiles, extract visual features from
each tile, and train a classiﬁer on the re-
sulting features along with the images’ aes-
thetics ratings. Our model achieves a 10-
fold cross-validation classiﬁcation success
rate of 83.60%, corroborating the efﬁcacy
of our methodology and therefore showing
promise for future development.

1. Introduction
Aesthetics in photography are highly subjective. The
average individual may judge the quality of a photo-
graph simply by gut feeling; in contrast, a photogra-
pher might evaluate a photograph he or she captures
vis-a-vis technical criteria such as composition, con-
trast, and sharpness. Towards fulﬁlling these crite-
ria, photographers follow many rules of thumb. The
actual and relative visual impact of doing so for the
general public, however, remains unclear.
In our project, we show that the existence of cer-
tain characteristics does indeed make an image more
aesthetically-pleasing in general. We achieve this by
building a machine learning pipeline that trains a hy-
pothesis capable of classifying images as either ex-
hibiting high levels of aesthetic quality or not.
The potential impact of building a system to solve
this problem is broad. For example, by implement-

ing such a system, websites with community-sourced
images can programmatically ﬁlter out bad images
to maintain the desired quality of content. Cam-
eras can provide real-time visual feedback to help
users improve their photographic skills. Moreover,
from a cognitive standpoint, solving this problem
may lend interesting insight towards how humans
perceive beauty.
We begin by identifying visual features that we be-
lieve correlate with the aesthetic quality of a photo-
graph. We then build a learning pipeline that ex-
tracts these features from images on a per-tile basis
and uses them along with the images’ aesthetics rat-
ings to train a classiﬁer. In this manner, we endow the
classiﬁer with the freedom to infer spatial relation-
ships amongst features that correlate with an image’s
aesthetics.

2. Related work
There have been several efforts to tackle this problem
from different angles within the past decade. Pogac-
nik et al [1] believed that the features depended heav-
ily on identiﬁcation of the subject of the photograph.
Datta et al [2] evaluated the performance of different
machine learning models (support vector machines,
decision trees) on the problem. Ke et al [3] focused
on extracting perceptual factors important to profes-
sional photographers, such as color, noise, blur, and
spatial distribution of edges.
Also, in contrast to our approach, it is interesting to
note that these studies have focused primarily on ex-
tracting features that attempt to capture prior beliefs
on the spatial orientation of visual elements within
the image. For example, Datta et al attempted to
model rule-of-thirds composition by computing the
average hue, saturation, and luminance of the inner
thirds rectangle, and Pogacnik et al deﬁned features

Classiﬁcation of photographic images based on

perceived aesthetic quality

Jeff Hwang
Department of Electrical Engineering, Stanford University

Sean Shi
Department of Electrical Engineering, Stanford University

JEFF.HWANG@STANFORD.EDU

SSHI11@STANFORD.EDU

Abstract

In this paper, we explore automated aes-
thetic evaluation of photographs using ma-
chine learning and image processing tech-
niques. We theorize that the spatial dis-
tribution of certain visual elements within
a given image correlates with its aesthetic
quality. To this end, we present a novel ap-
proach wherein we model each photograph
as a set of tiles, extract visual features from
each tile, and train a classiﬁer on the re-
sulting features along with the images’ aes-
thetics ratings. Our model achieves a 10-
fold cross-validation classiﬁcation success
rate of 83.60%, corroborating the efﬁcacy
of our methodology and therefore showing
promise for future development.

1. Introduction
Aesthetics in photography are highly subjective. The
average individual may judge the quality of a photo-
graph simply by gut feeling; in contrast, a photogra-
pher might evaluate a photograph he or she captures
vis-a-vis technical criteria such as composition, con-
trast, and sharpness. Towards fulﬁlling these crite-
ria, photographers follow many rules of thumb. The
actual and relative visual impact of doing so for the
general public, however, remains unclear.
In our project, we show that the existence of cer-
tain characteristics does indeed make an image more
aesthetically-pleasing in general. We achieve this by
building a machine learning pipeline that trains a hy-
pothesis capable of classifying images as either ex-
hibiting high levels of aesthetic quality or not.
The potential impact of building a system to solve
this problem is broad. For example, by implement-

ing such a system, websites with community-sourced
images can programmatically ﬁlter out bad images
to maintain the desired quality of content. Cam-
eras can provide real-time visual feedback to help
users improve their photographic skills. Moreover,
from a cognitive standpoint, solving this problem
may lend interesting insight towards how humans
perceive beauty.
We begin by identifying visual features that we be-
lieve correlate with the aesthetic quality of a photo-
graph. We then build a learning pipeline that ex-
tracts these features from images on a per-tile basis
and uses them along with the images’ aesthetics rat-
ings to train a classiﬁer. In this manner, we endow the
classiﬁer with the freedom to infer spatial relation-
ships amongst features that correlate with an image’s
aesthetics.

2. Related work
There have been several efforts to tackle this problem
from different angles within the past decade. Pogac-
nik et al [1] believed that the features depended heav-
ily on identiﬁcation of the subject of the photograph.
Datta et al [2] evaluated the performance of different
machine learning models (support vector machines,
decision trees) on the problem. Ke et al [3] focused
on extracting perceptual factors important to profes-
sional photographers, such as color, noise, blur, and
spatial distribution of edges.
Also, in contrast to our approach, it is interesting to
note that these studies have focused primarily on ex-
tracting features that attempt to capture prior beliefs
on the spatial orientation of visual elements within
the image. For example, Datta et al attempted to
model rule-of-thirds composition by computing the
average hue, saturation, and luminance of the inner
thirds rectangle, and Pogacnik et al deﬁned features

gorithm would be able to discern the well-composed
framing of the pier from the features extracted from
its containing tiles with respect to those extracted
from the surrounding tiles.
Below, we describe the features we extract from each
image tile.
Subject detection: Strong edges distinguish the sub-
ject from the image’s background. To quantify the
degree of subject-background separation, we apply a
Sobel ﬁlter to each image tile, binarize the result via
Otsu’s method, and compute the proportion of pixels
in the tile that are edge pixels:

fsd =

(x,y)∈Tile

(cid:88)

1{I(x, y) = Edge}

Color palette: A photograph’s color composition can
dramatically inﬂuence how a person perceives a pho-
tograph. We capture the color diversity of a pho-
tograph using a color histogram that subdivides the
three dimensional RGB color space into 64 equally
sized bins. Since each pixel can take on one of 256 dis-
crete values in each color channel, this results in each
bin being a cube with 16 possible values in each di-
mension. We normalize each bin’s count by the total
pixel count so that it is invariant to image dimensions.
Detail: Higher levels of detail are generally desirable
for photographs, particularly for its subject. To ap-
proximate the amount of detail, we compare the num-
ber of edge pixels of a Gaussian ﬁltered version of
the image tile to the number of edge pixels within the
original image tile, i.e.

(cid:80)
(cid:80)
(x,y)∈Tile 1{If iltered(x, y) = Edge}
(x,y)∈Tile 1{I(x, y) = Edge}

fd =

For an image tile that is exceptionally detailed, many
of the higher-frequency edges in the region would be
removed by the Gaussian ﬁlter. Consequently, we
would expect fd to be closer to 0. Conversely, for a
tile that lacks detail, since few edges exist in the re-
gion, applying the Gaussian ﬁlter would impart little
change to the number of edges. In this case, we would
expect fd to be closer to 1.
Hue: To the human eye, certain color combinations
are more appealing than others. To capture this, for
each image tile, we compute the proportion of pix-
els that correspond to a particular hue. We discretize
hues into ﬁve regions, corresponding to red, yellow,
green, blue, and purple.
Saturation: Saturation measures the intensity of a
color. We extract the average saturation value for each
image tile.

Figure 1. Tiling scheme applied to image by learning
pipeline.

that assessed adherence to a multitude of composi-
tional rules as well as the positioning of the subject
relative to the image’s frame.

3. Dataset
Our learning pipeline downloads images and their
average aesthetic ratings from two separate datasets.
The ﬁrst is an image database hosted by photo.net, a
photo sharing website for photographers. The index
ﬁle we use to locate images was generated by Datta
et al. Members of photo.net can upload and critique
each others photographs and rate each photograph
with a number between 1 and 7, with 7 being the best
possible rating. Due to the wide range and subjectiv-
ity of ratings, we choose to only use photographs with
ratings above 6 or below 4.2, which yields a dataset
containing 1700 images split evenly between positive
labels and negative labels.
The second comprises images scraped from DPChal-
lenge, another photo sharing website for photogra-
phers. The index ﬁle we used to locate images was
generated by Murray et al [4]. Following guidelines
from prior work, we choose to use photographs with
ratings above 7.2 or below 3.4, resulting in a dataset
containing 2000 images split evenly between positive
and negative labels.

4. Feature extraction
Prior to extracting features, we partition each image
into ﬁve-by-ﬁve equally-sized tiles (Figure 1). By ex-
tracting features on a per-tile basis, the learning algo-
rithm can identify regions of interest and infer rela-
tionships between feature-tile pairs that indicate aes-
thetic quality. For example, in the case of the image
depicted in Figure 1, we surmise that the learning al-

Classiﬁcation of photographic images based on

perceived aesthetic quality

Jeff Hwang
Department of Electrical Engineering, Stanford University

Sean Shi
Department of Electrical Engineering, Stanford University

JEFF.HWANG@STANFORD.EDU

SSHI11@STANFORD.EDU

Abstract

In this paper, we explore automated aes-
thetic evaluation of photographs using ma-
chine learning and image processing tech-
niques. We theorize that the spatial dis-
tribution of certain visual elements within
a given image correlates with its aesthetic
quality. To this end, we present a novel ap-
proach wherein we model each photograph
as a set of tiles, extract visual features from
each tile, and train a classiﬁer on the re-
sulting features along with the images’ aes-
thetics ratings. Our model achieves a 10-
fold cross-validation classiﬁcation success
rate of 83.60%, corroborating the efﬁcacy
of our methodology and therefore showing
promise for future development.

1. Introduction
Aesthetics in photography are highly subjective. The
average individual may judge the quality of a photo-
graph simply by gut feeling; in contrast, a photogra-
pher might evaluate a photograph he or she captures
vis-a-vis technical criteria such as composition, con-
trast, and sharpness. Towards fulﬁlling these crite-
ria, photographers follow many rules of thumb. The
actual and relative visual impact of doing so for the
general public, however, remains unclear.
In our project, we show that the existence of cer-
tain characteristics does indeed make an image more
aesthetically-pleasing in general. We achieve this by
building a machine learning pipeline that trains a hy-
pothesis capable of classifying images as either ex-
hibiting high levels of aesthetic quality or not.
The potential impact of building a system to solve
this problem is broad. For example, by implement-

ing such a system, websites with community-sourced
images can programmatically ﬁlter out bad images
to maintain the desired quality of content. Cam-
eras can provide real-time visual feedback to help
users improve their photographic skills. Moreover,
from a cognitive standpoint, solving this problem
may lend interesting insight towards how humans
perceive beauty.
We begin by identifying visual features that we be-
lieve correlate with the aesthetic quality of a photo-
graph. We then build a learning pipeline that ex-
tracts these features from images on a per-tile basis
and uses them along with the images’ aesthetics rat-
ings to train a classiﬁer. In this manner, we endow the
classiﬁer with the freedom to infer spatial relation-
ships amongst features that correlate with an image’s
aesthetics.

2. Related work
There have been several efforts to tackle this problem
from different angles within the past decade. Pogac-
nik et al [1] believed that the features depended heav-
ily on identiﬁcation of the subject of the photograph.
Datta et al [2] evaluated the performance of different
machine learning models (support vector machines,
decision trees) on the problem. Ke et al [3] focused
on extracting perceptual factors important to profes-
sional photographers, such as color, noise, blur, and
spatial distribution of edges.
Also, in contrast to our approach, it is interesting to
note that these studies have focused primarily on ex-
tracting features that attempt to capture prior beliefs
on the spatial orientation of visual elements within
the image. For example, Datta et al attempted to
model rule-of-thirds composition by computing the
average hue, saturation, and luminance of the inner
thirds rectangle, and Pogacnik et al deﬁned features

gorithm would be able to discern the well-composed
framing of the pier from the features extracted from
its containing tiles with respect to those extracted
from the surrounding tiles.
Below, we describe the features we extract from each
image tile.
Subject detection: Strong edges distinguish the sub-
ject from the image’s background. To quantify the
degree of subject-background separation, we apply a
Sobel ﬁlter to each image tile, binarize the result via
Otsu’s method, and compute the proportion of pixels
in the tile that are edge pixels:

fsd =

(x,y)∈Tile

(cid:88)

1{I(x, y) = Edge}

Color palette: A photograph’s color composition can
dramatically inﬂuence how a person perceives a pho-
tograph. We capture the color diversity of a pho-
tograph using a color histogram that subdivides the
three dimensional RGB color space into 64 equally
sized bins. Since each pixel can take on one of 256 dis-
crete values in each color channel, this results in each
bin being a cube with 16 possible values in each di-
mension. We normalize each bin’s count by the total
pixel count so that it is invariant to image dimensions.
Detail: Higher levels of detail are generally desirable
for photographs, particularly for its subject. To ap-
proximate the amount of detail, we compare the num-
ber of edge pixels of a Gaussian ﬁltered version of
the image tile to the number of edge pixels within the
original image tile, i.e.

(cid:80)
(cid:80)
(x,y)∈Tile 1{If iltered(x, y) = Edge}
(x,y)∈Tile 1{I(x, y) = Edge}

fd =

For an image tile that is exceptionally detailed, many
of the higher-frequency edges in the region would be
removed by the Gaussian ﬁlter. Consequently, we
would expect fd to be closer to 0. Conversely, for a
tile that lacks detail, since few edges exist in the re-
gion, applying the Gaussian ﬁlter would impart little
change to the number of edges. In this case, we would
expect fd to be closer to 1.
Hue: To the human eye, certain color combinations
are more appealing than others. To capture this, for
each image tile, we compute the proportion of pix-
els that correspond to a particular hue. We discretize
hues into ﬁve regions, corresponding to red, yellow,
green, blue, and purple.
Saturation: Saturation measures the intensity of a
color. We extract the average saturation value for each
image tile.

Figure 1. Tiling scheme applied to image by learning
pipeline.

that assessed adherence to a multitude of composi-
tional rules as well as the positioning of the subject
relative to the image’s frame.

3. Dataset
Our learning pipeline downloads images and their
average aesthetic ratings from two separate datasets.
The ﬁrst is an image database hosted by photo.net, a
photo sharing website for photographers. The index
ﬁle we use to locate images was generated by Datta
et al. Members of photo.net can upload and critique
each others photographs and rate each photograph
with a number between 1 and 7, with 7 being the best
possible rating. Due to the wide range and subjectiv-
ity of ratings, we choose to only use photographs with
ratings above 6 or below 4.2, which yields a dataset
containing 1700 images split evenly between positive
labels and negative labels.
The second comprises images scraped from DPChal-
lenge, another photo sharing website for photogra-
phers. The index ﬁle we used to locate images was
generated by Murray et al [4]. Following guidelines
from prior work, we choose to use photographs with
ratings above 7.2 or below 3.4, resulting in a dataset
containing 2000 images split evenly between positive
and negative labels.

4. Feature extraction
Prior to extracting features, we partition each image
into ﬁve-by-ﬁve equally-sized tiles (Figure 1). By ex-
tracting features on a per-tile basis, the learning algo-
rithm can identify regions of interest and infer rela-
tionships between feature-tile pairs that indicate aes-
thetic quality. For example, in the case of the image
depicted in Figure 1, we surmise that the learning al-

For each of the features we describe in Section 3, there
exists a feature extractor function that accepts an im-
age as an input, calculates the feature value, and in-
serts the feature-value mapping into a sparse feature
vector allocated for the image. We rely on image pro-
cessing algorithms implemented in the scikit-image
library for many of these functions.
After the pipeline generates feature vectors for all im-
ages in the training set, it uses them to train a clas-
siﬁer. For the learning algorithm, we experimented
with support vector machines (SVM), random forests,
and gradient tree boosting.
SVM: The SVM learning algorithm with (cid:96)1 regu-
larization involves solving the primal optimization
problem

m(cid:88)

min
γ,w,b

subject to

||w||2 + C

1
2
y(i)(wT x(i) + b) ≥ 1 − ξi, i = 1, ..., m

i=1

ξi

, the dual of which is

m(cid:88)

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105)

max

α

subject to

αi − 1
2

i,j=1

0 ≤ αi ≤ C, i = 1, ..., m

αiy(i) = 0

m(cid:88)
m(cid:88)

i=1

i=1

Accordingly, provided that we ﬁnd the values of α
that maximize the dual optimization problem, the hy-
pothesis can be formulated as

if (cid:80)m

(cid:26) 1

−1

h(x) =

i=1 αiy(i)(cid:104)x(i), x(cid:105) + b ≥ 0

otherwise

Note that since the dual optimization problem and
hypothesis can be expressed as inner products be-
tween input feature vectors, we can replace each in-
ner product with a kernel applied to the two in-
put vectors, which allows us to train our classiﬁer
and perform classiﬁcation in a higher-dimensional
feature space. This characteristic of SVMs makes
them well-suited for our problem since we speculate
that non-linear relationships amongst multiple fea-
tures inﬂuence image aesthetic quality. For our sys-
tem, we choose to use the Gaussian kernel K(x, y) =

(cid:1), which corresponds to an inﬁnite-

exp(cid:0)γ||x − y||2

dimensional feature mapping.
Random forest: Random forests comprise collections
of decision trees. Each decision tree is grown by se-
lecting a random subset of input variables to serve as
candidates for splitting at a particular node. Predic-

2

Figure 2. Block diagram of learning pipeline.

Contrast: Contrast is the difference in color or bright-
ness amongst regions in an image. Generally, the
higher the contrast, the more distinguishable objects
are from one another. We approximate the contrast
within each image tile by calculating the standard de-
viation of the grayscale intensities.
Blur: Depending on the image region, blurriness may
or may not be desirable. Poor technique or camera
shake tends to yield images that are blurry across the
entire frame, which is generally undesirable. On the
other hand, low depth-of-ﬁeld images with blurred
out-of-focus highlights (“bokeh”) that complement
sharp subjects are often regarded as being pleasing.
To efﬁciently estimate the amount of blur within an
image, we calculate the variance of the Laplacian of
the image. Low variance corresponds to blurrier im-
ages, and high variance to sharper images.
Noise: The desirability of visual noise is contextual.
For most modern images and for images that con-
vey positive emotions, noise is generally undesirable.
For images that convey negative semantics, however,
noise may be desirable to accentuate their visual im-
pact. We measure noise by calculating the image’s en-
tropy.
Saliency: The saliency of the subject within a photo-
graph has a signiﬁcant impact on the perceived aes-
thetic quality of the photograph. We post-process
each image to separate the salient region from the
background using a center-vs-surround approach de-
scribed in Achanta et al [5]. We then sum the number
of salient pixels per image tile and normalize by the
tile size.

5. Methods
Figure 2 depicts a high-level block diagram of the
learning pipeline we built. The pipeline comprises
three main components: an image scraper, a bank of
feature extractors, and a learning algorithm.

Classiﬁcation of photographic images based on

perceived aesthetic quality

Jeff Hwang
Department of Electrical Engineering, Stanford University

Sean Shi
Department of Electrical Engineering, Stanford University

JEFF.HWANG@STANFORD.EDU

SSHI11@STANFORD.EDU

Abstract

In this paper, we explore automated aes-
thetic evaluation of photographs using ma-
chine learning and image processing tech-
niques. We theorize that the spatial dis-
tribution of certain visual elements within
a given image correlates with its aesthetic
quality. To this end, we present a novel ap-
proach wherein we model each photograph
as a set of tiles, extract visual features from
each tile, and train a classiﬁer on the re-
sulting features along with the images’ aes-
thetics ratings. Our model achieves a 10-
fold cross-validation classiﬁcation success
rate of 83.60%, corroborating the efﬁcacy
of our methodology and therefore showing
promise for future development.

1. Introduction
Aesthetics in photography are highly subjective. The
average individual may judge the quality of a photo-
graph simply by gut feeling; in contrast, a photogra-
pher might evaluate a photograph he or she captures
vis-a-vis technical criteria such as composition, con-
trast, and sharpness. Towards fulﬁlling these crite-
ria, photographers follow many rules of thumb. The
actual and relative visual impact of doing so for the
general public, however, remains unclear.
In our project, we show that the existence of cer-
tain characteristics does indeed make an image more
aesthetically-pleasing in general. We achieve this by
building a machine learning pipeline that trains a hy-
pothesis capable of classifying images as either ex-
hibiting high levels of aesthetic quality or not.
The potential impact of building a system to solve
this problem is broad. For example, by implement-

ing such a system, websites with community-sourced
images can programmatically ﬁlter out bad images
to maintain the desired quality of content. Cam-
eras can provide real-time visual feedback to help
users improve their photographic skills. Moreover,
from a cognitive standpoint, solving this problem
may lend interesting insight towards how humans
perceive beauty.
We begin by identifying visual features that we be-
lieve correlate with the aesthetic quality of a photo-
graph. We then build a learning pipeline that ex-
tracts these features from images on a per-tile basis
and uses them along with the images’ aesthetics rat-
ings to train a classiﬁer. In this manner, we endow the
classiﬁer with the freedom to infer spatial relation-
ships amongst features that correlate with an image’s
aesthetics.

2. Related work
There have been several efforts to tackle this problem
from different angles within the past decade. Pogac-
nik et al [1] believed that the features depended heav-
ily on identiﬁcation of the subject of the photograph.
Datta et al [2] evaluated the performance of different
machine learning models (support vector machines,
decision trees) on the problem. Ke et al [3] focused
on extracting perceptual factors important to profes-
sional photographers, such as color, noise, blur, and
spatial distribution of edges.
Also, in contrast to our approach, it is interesting to
note that these studies have focused primarily on ex-
tracting features that attempt to capture prior beliefs
on the spatial orientation of visual elements within
the image. For example, Datta et al attempted to
model rule-of-thirds composition by computing the
average hue, saturation, and luminance of the inner
thirds rectangle, and Pogacnik et al deﬁned features

gorithm would be able to discern the well-composed
framing of the pier from the features extracted from
its containing tiles with respect to those extracted
from the surrounding tiles.
Below, we describe the features we extract from each
image tile.
Subject detection: Strong edges distinguish the sub-
ject from the image’s background. To quantify the
degree of subject-background separation, we apply a
Sobel ﬁlter to each image tile, binarize the result via
Otsu’s method, and compute the proportion of pixels
in the tile that are edge pixels:

fsd =

(x,y)∈Tile

(cid:88)

1{I(x, y) = Edge}

Color palette: A photograph’s color composition can
dramatically inﬂuence how a person perceives a pho-
tograph. We capture the color diversity of a pho-
tograph using a color histogram that subdivides the
three dimensional RGB color space into 64 equally
sized bins. Since each pixel can take on one of 256 dis-
crete values in each color channel, this results in each
bin being a cube with 16 possible values in each di-
mension. We normalize each bin’s count by the total
pixel count so that it is invariant to image dimensions.
Detail: Higher levels of detail are generally desirable
for photographs, particularly for its subject. To ap-
proximate the amount of detail, we compare the num-
ber of edge pixels of a Gaussian ﬁltered version of
the image tile to the number of edge pixels within the
original image tile, i.e.

(cid:80)
(cid:80)
(x,y)∈Tile 1{If iltered(x, y) = Edge}
(x,y)∈Tile 1{I(x, y) = Edge}

fd =

For an image tile that is exceptionally detailed, many
of the higher-frequency edges in the region would be
removed by the Gaussian ﬁlter. Consequently, we
would expect fd to be closer to 0. Conversely, for a
tile that lacks detail, since few edges exist in the re-
gion, applying the Gaussian ﬁlter would impart little
change to the number of edges. In this case, we would
expect fd to be closer to 1.
Hue: To the human eye, certain color combinations
are more appealing than others. To capture this, for
each image tile, we compute the proportion of pix-
els that correspond to a particular hue. We discretize
hues into ﬁve regions, corresponding to red, yellow,
green, blue, and purple.
Saturation: Saturation measures the intensity of a
color. We extract the average saturation value for each
image tile.

Figure 1. Tiling scheme applied to image by learning
pipeline.

that assessed adherence to a multitude of composi-
tional rules as well as the positioning of the subject
relative to the image’s frame.

3. Dataset
Our learning pipeline downloads images and their
average aesthetic ratings from two separate datasets.
The ﬁrst is an image database hosted by photo.net, a
photo sharing website for photographers. The index
ﬁle we use to locate images was generated by Datta
et al. Members of photo.net can upload and critique
each others photographs and rate each photograph
with a number between 1 and 7, with 7 being the best
possible rating. Due to the wide range and subjectiv-
ity of ratings, we choose to only use photographs with
ratings above 6 or below 4.2, which yields a dataset
containing 1700 images split evenly between positive
labels and negative labels.
The second comprises images scraped from DPChal-
lenge, another photo sharing website for photogra-
phers. The index ﬁle we used to locate images was
generated by Murray et al [4]. Following guidelines
from prior work, we choose to use photographs with
ratings above 7.2 or below 3.4, resulting in a dataset
containing 2000 images split evenly between positive
and negative labels.

4. Feature extraction
Prior to extracting features, we partition each image
into ﬁve-by-ﬁve equally-sized tiles (Figure 1). By ex-
tracting features on a per-tile basis, the learning algo-
rithm can identify regions of interest and infer rela-
tionships between feature-tile pairs that indicate aes-
thetic quality. For example, in the case of the image
depicted in Figure 1, we surmise that the learning al-

For each of the features we describe in Section 3, there
exists a feature extractor function that accepts an im-
age as an input, calculates the feature value, and in-
serts the feature-value mapping into a sparse feature
vector allocated for the image. We rely on image pro-
cessing algorithms implemented in the scikit-image
library for many of these functions.
After the pipeline generates feature vectors for all im-
ages in the training set, it uses them to train a clas-
siﬁer. For the learning algorithm, we experimented
with support vector machines (SVM), random forests,
and gradient tree boosting.
SVM: The SVM learning algorithm with (cid:96)1 regu-
larization involves solving the primal optimization
problem

m(cid:88)

min
γ,w,b

subject to

||w||2 + C

1
2
y(i)(wT x(i) + b) ≥ 1 − ξi, i = 1, ..., m

i=1

ξi

, the dual of which is

m(cid:88)

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105)

max

α

subject to

αi − 1
2

i,j=1

0 ≤ αi ≤ C, i = 1, ..., m

αiy(i) = 0

m(cid:88)
m(cid:88)

i=1

i=1

Accordingly, provided that we ﬁnd the values of α
that maximize the dual optimization problem, the hy-
pothesis can be formulated as

if (cid:80)m

(cid:26) 1

−1

h(x) =

i=1 αiy(i)(cid:104)x(i), x(cid:105) + b ≥ 0

otherwise

Note that since the dual optimization problem and
hypothesis can be expressed as inner products be-
tween input feature vectors, we can replace each in-
ner product with a kernel applied to the two in-
put vectors, which allows us to train our classiﬁer
and perform classiﬁcation in a higher-dimensional
feature space. This characteristic of SVMs makes
them well-suited for our problem since we speculate
that non-linear relationships amongst multiple fea-
tures inﬂuence image aesthetic quality. For our sys-
tem, we choose to use the Gaussian kernel K(x, y) =

(cid:1), which corresponds to an inﬁnite-

exp(cid:0)γ||x − y||2

dimensional feature mapping.
Random forest: Random forests comprise collections
of decision trees. Each decision tree is grown by se-
lecting a random subset of input variables to serve as
candidates for splitting at a particular node. Predic-

2

Figure 2. Block diagram of learning pipeline.

Contrast: Contrast is the difference in color or bright-
ness amongst regions in an image. Generally, the
higher the contrast, the more distinguishable objects
are from one another. We approximate the contrast
within each image tile by calculating the standard de-
viation of the grayscale intensities.
Blur: Depending on the image region, blurriness may
or may not be desirable. Poor technique or camera
shake tends to yield images that are blurry across the
entire frame, which is generally undesirable. On the
other hand, low depth-of-ﬁeld images with blurred
out-of-focus highlights (“bokeh”) that complement
sharp subjects are often regarded as being pleasing.
To efﬁciently estimate the amount of blur within an
image, we calculate the variance of the Laplacian of
the image. Low variance corresponds to blurrier im-
ages, and high variance to sharper images.
Noise: The desirability of visual noise is contextual.
For most modern images and for images that con-
vey positive emotions, noise is generally undesirable.
For images that convey negative semantics, however,
noise may be desirable to accentuate their visual im-
pact. We measure noise by calculating the image’s en-
tropy.
Saliency: The saliency of the subject within a photo-
graph has a signiﬁcant impact on the perceived aes-
thetic quality of the photograph. We post-process
each image to separate the salient region from the
background using a center-vs-surround approach de-
scribed in Achanta et al [5]. We then sum the number
of salient pixels per image tile and normalize by the
tile size.

5. Methods
Figure 2 depicts a high-level block diagram of the
learning pipeline we built. The pipeline comprises
three main components: an image scraper, a bank of
feature extractors, and a learning algorithm.

tion then involves taking the average of the predic-
tions of all the constituent trees:

(cid:33)

(cid:32)

m(cid:88)

i=1

1
m

h(x) = sign

Ti(x)

Because of the way each decision tree is constructed,
the variance of the average prediction is less than that
of any individual prediction.
It is this characteris-
tic that makes random forests more resistant to over-
ﬁtting than decision trees, and, thus, generally have
much higher performance.
Gradient tree boosting: Boosting is a powerful learn-
ing method that sequentially applies weak classiﬁca-
tion algorithms to reweighted versions of the training
data, with the reweighting done in such a way that,
between every pair of classiﬁers in the sequence, the
examples that were misclassiﬁed by the previous clas-
siﬁer are weighted higher for the next classiﬁer.
In
this manner, each subsequent classiﬁer in the ensem-
ble is forced to concentrate on correctly classifying the
examples that were previously misclassiﬁed.
In gradient tree boosting, or gradient-boosted regres-
sion trees (GBRT), our weak classiﬁers are decision
trees. After ﬁtting the trees, the predictions from all
the decision trees are weighted and combined to form
the ﬁnal prediction:

(cid:32) m(cid:88)

(cid:33)

h(x) = sign

αiTi(x)

i=1

In literature, tree boosting has been identiﬁed as be-
ing one of the best learning algorithms available [6].

6. Experimental results and analysis
For each learning algorithm, we measure the per-
formance of our classiﬁer using 10-fold cross valida-
tion on the photo.net dataset and the DPChallenge
dataset. We run backward feature selection to elimi-
nate ineffective features to improve classiﬁcation per-
formance.
For SVM, we tuned our parameters using grid search,
which ultimately led us to use C = 1 and γ = 0.1. For
random forest, we used 300 decision trees. We deter-
mined this value by empirically ﬁnding the asymp-
totic limit to the generalization error with respect
to the number of decision trees used. For gradient
tree boosting, we used 200 decision trees and a sub-
sampling coefﬁcient of 0.9. Using a sub-sampling co-
efﬁcient smaller than 1 allows us to trade off vari-
ance for bias, which thereby mitigates overﬁtting and
hence improves generalization performance.

photo.net
DPChallenge

RF

GBRT
SVM
78.71% 78.58% 80.88%
82.62% 82.85% 83.60%

Table 1. 10-fold classiﬁcation accuracy

Predicted label

1
1 TP

Actual
label

80.12%

0 FP

18.35%

0

FN
19.88%

TN
81.65%

Figure 3. Confusion matrix for 10-fold cross validation with
GBRT on photo.net dataset.

Table 1 shows our 10-fold cross-validation accuracy
for each of the learning algorithms. For both datasets,
we got the highest performance with GBRTs, with ac-
curacies of 80.88% and 83.60%. That we see similar
quality of results for both datasets signiﬁes that our
methodology is sound.
Figure 3 shows the confusion matrix for 10-fold cross-
validation using GBRTs on the photo.net dataset.
The true positive and false negative rates are ap-
proximately symmetric with the true negative and
false positive rates, respectively, which signiﬁes that
our classiﬁer is not biased towards predicting a cer-
tain class. This also holds true for the DPChallenge
dataset.
To analyze the deﬁciencies of our methodology, we
examine images that our classiﬁer misclassiﬁed.
Figure 4 shows an example of a negative image from
the photo.net dataset that the classiﬁer mispredicted
as being positive. Note that the image is compo-
sitionally sound – the subject is clearly distinguish-
able from the background, ﬁlls most of the frame, is
well-balanced in the frame, and has components that
lie along the rule-of-thirds axes. The hot-pink back-
ground, however, is incredibly jarring, and the subject
matter is mundane and lacks signiﬁcance. Unfortu-
nately, because it discretizes color features so coarsely,
the classiﬁer is likely not able to effectively differen-
tiate between different shades of colors, such as the
artiﬁcial pink shade of this image’s background and
the warm red shade of a beautiful sunset. More-
over, it has no way of gleaning meaning from images.
We therefore believe that it is primarily due to these
shortcomings that our classiﬁer misclassiﬁed this par-
ticular image.

Classiﬁcation of photographic images based on

perceived aesthetic quality

Jeff Hwang
Department of Electrical Engineering, Stanford University

Sean Shi
Department of Electrical Engineering, Stanford University

JEFF.HWANG@STANFORD.EDU

SSHI11@STANFORD.EDU

Abstract

In this paper, we explore automated aes-
thetic evaluation of photographs using ma-
chine learning and image processing tech-
niques. We theorize that the spatial dis-
tribution of certain visual elements within
a given image correlates with its aesthetic
quality. To this end, we present a novel ap-
proach wherein we model each photograph
as a set of tiles, extract visual features from
each tile, and train a classiﬁer on the re-
sulting features along with the images’ aes-
thetics ratings. Our model achieves a 10-
fold cross-validation classiﬁcation success
rate of 83.60%, corroborating the efﬁcacy
of our methodology and therefore showing
promise for future development.

1. Introduction
Aesthetics in photography are highly subjective. The
average individual may judge the quality of a photo-
graph simply by gut feeling; in contrast, a photogra-
pher might evaluate a photograph he or she captures
vis-a-vis technical criteria such as composition, con-
trast, and sharpness. Towards fulﬁlling these crite-
ria, photographers follow many rules of thumb. The
actual and relative visual impact of doing so for the
general public, however, remains unclear.
In our project, we show that the existence of cer-
tain characteristics does indeed make an image more
aesthetically-pleasing in general. We achieve this by
building a machine learning pipeline that trains a hy-
pothesis capable of classifying images as either ex-
hibiting high levels of aesthetic quality or not.
The potential impact of building a system to solve
this problem is broad. For example, by implement-

ing such a system, websites with community-sourced
images can programmatically ﬁlter out bad images
to maintain the desired quality of content. Cam-
eras can provide real-time visual feedback to help
users improve their photographic skills. Moreover,
from a cognitive standpoint, solving this problem
may lend interesting insight towards how humans
perceive beauty.
We begin by identifying visual features that we be-
lieve correlate with the aesthetic quality of a photo-
graph. We then build a learning pipeline that ex-
tracts these features from images on a per-tile basis
and uses them along with the images’ aesthetics rat-
ings to train a classiﬁer. In this manner, we endow the
classiﬁer with the freedom to infer spatial relation-
ships amongst features that correlate with an image’s
aesthetics.

2. Related work
There have been several efforts to tackle this problem
from different angles within the past decade. Pogac-
nik et al [1] believed that the features depended heav-
ily on identiﬁcation of the subject of the photograph.
Datta et al [2] evaluated the performance of different
machine learning models (support vector machines,
decision trees) on the problem. Ke et al [3] focused
on extracting perceptual factors important to profes-
sional photographers, such as color, noise, blur, and
spatial distribution of edges.
Also, in contrast to our approach, it is interesting to
note that these studies have focused primarily on ex-
tracting features that attempt to capture prior beliefs
on the spatial orientation of visual elements within
the image. For example, Datta et al attempted to
model rule-of-thirds composition by computing the
average hue, saturation, and luminance of the inner
thirds rectangle, and Pogacnik et al deﬁned features

gorithm would be able to discern the well-composed
framing of the pier from the features extracted from
its containing tiles with respect to those extracted
from the surrounding tiles.
Below, we describe the features we extract from each
image tile.
Subject detection: Strong edges distinguish the sub-
ject from the image’s background. To quantify the
degree of subject-background separation, we apply a
Sobel ﬁlter to each image tile, binarize the result via
Otsu’s method, and compute the proportion of pixels
in the tile that are edge pixels:

fsd =

(x,y)∈Tile

(cid:88)

1{I(x, y) = Edge}

Color palette: A photograph’s color composition can
dramatically inﬂuence how a person perceives a pho-
tograph. We capture the color diversity of a pho-
tograph using a color histogram that subdivides the
three dimensional RGB color space into 64 equally
sized bins. Since each pixel can take on one of 256 dis-
crete values in each color channel, this results in each
bin being a cube with 16 possible values in each di-
mension. We normalize each bin’s count by the total
pixel count so that it is invariant to image dimensions.
Detail: Higher levels of detail are generally desirable
for photographs, particularly for its subject. To ap-
proximate the amount of detail, we compare the num-
ber of edge pixels of a Gaussian ﬁltered version of
the image tile to the number of edge pixels within the
original image tile, i.e.

(cid:80)
(cid:80)
(x,y)∈Tile 1{If iltered(x, y) = Edge}
(x,y)∈Tile 1{I(x, y) = Edge}

fd =

For an image tile that is exceptionally detailed, many
of the higher-frequency edges in the region would be
removed by the Gaussian ﬁlter. Consequently, we
would expect fd to be closer to 0. Conversely, for a
tile that lacks detail, since few edges exist in the re-
gion, applying the Gaussian ﬁlter would impart little
change to the number of edges. In this case, we would
expect fd to be closer to 1.
Hue: To the human eye, certain color combinations
are more appealing than others. To capture this, for
each image tile, we compute the proportion of pix-
els that correspond to a particular hue. We discretize
hues into ﬁve regions, corresponding to red, yellow,
green, blue, and purple.
Saturation: Saturation measures the intensity of a
color. We extract the average saturation value for each
image tile.

Figure 1. Tiling scheme applied to image by learning
pipeline.

that assessed adherence to a multitude of composi-
tional rules as well as the positioning of the subject
relative to the image’s frame.

3. Dataset
Our learning pipeline downloads images and their
average aesthetic ratings from two separate datasets.
The ﬁrst is an image database hosted by photo.net, a
photo sharing website for photographers. The index
ﬁle we use to locate images was generated by Datta
et al. Members of photo.net can upload and critique
each others photographs and rate each photograph
with a number between 1 and 7, with 7 being the best
possible rating. Due to the wide range and subjectiv-
ity of ratings, we choose to only use photographs with
ratings above 6 or below 4.2, which yields a dataset
containing 1700 images split evenly between positive
labels and negative labels.
The second comprises images scraped from DPChal-
lenge, another photo sharing website for photogra-
phers. The index ﬁle we used to locate images was
generated by Murray et al [4]. Following guidelines
from prior work, we choose to use photographs with
ratings above 7.2 or below 3.4, resulting in a dataset
containing 2000 images split evenly between positive
and negative labels.

4. Feature extraction
Prior to extracting features, we partition each image
into ﬁve-by-ﬁve equally-sized tiles (Figure 1). By ex-
tracting features on a per-tile basis, the learning algo-
rithm can identify regions of interest and infer rela-
tionships between feature-tile pairs that indicate aes-
thetic quality. For example, in the case of the image
depicted in Figure 1, we surmise that the learning al-

For each of the features we describe in Section 3, there
exists a feature extractor function that accepts an im-
age as an input, calculates the feature value, and in-
serts the feature-value mapping into a sparse feature
vector allocated for the image. We rely on image pro-
cessing algorithms implemented in the scikit-image
library for many of these functions.
After the pipeline generates feature vectors for all im-
ages in the training set, it uses them to train a clas-
siﬁer. For the learning algorithm, we experimented
with support vector machines (SVM), random forests,
and gradient tree boosting.
SVM: The SVM learning algorithm with (cid:96)1 regu-
larization involves solving the primal optimization
problem

m(cid:88)

min
γ,w,b

subject to

||w||2 + C

1
2
y(i)(wT x(i) + b) ≥ 1 − ξi, i = 1, ..., m

i=1

ξi

, the dual of which is

m(cid:88)

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105)

max

α

subject to

αi − 1
2

i,j=1

0 ≤ αi ≤ C, i = 1, ..., m

αiy(i) = 0

m(cid:88)
m(cid:88)

i=1

i=1

Accordingly, provided that we ﬁnd the values of α
that maximize the dual optimization problem, the hy-
pothesis can be formulated as

if (cid:80)m

(cid:26) 1

−1

h(x) =

i=1 αiy(i)(cid:104)x(i), x(cid:105) + b ≥ 0

otherwise

Note that since the dual optimization problem and
hypothesis can be expressed as inner products be-
tween input feature vectors, we can replace each in-
ner product with a kernel applied to the two in-
put vectors, which allows us to train our classiﬁer
and perform classiﬁcation in a higher-dimensional
feature space. This characteristic of SVMs makes
them well-suited for our problem since we speculate
that non-linear relationships amongst multiple fea-
tures inﬂuence image aesthetic quality. For our sys-
tem, we choose to use the Gaussian kernel K(x, y) =

(cid:1), which corresponds to an inﬁnite-

exp(cid:0)γ||x − y||2

dimensional feature mapping.
Random forest: Random forests comprise collections
of decision trees. Each decision tree is grown by se-
lecting a random subset of input variables to serve as
candidates for splitting at a particular node. Predic-

2

Figure 2. Block diagram of learning pipeline.

Contrast: Contrast is the difference in color or bright-
ness amongst regions in an image. Generally, the
higher the contrast, the more distinguishable objects
are from one another. We approximate the contrast
within each image tile by calculating the standard de-
viation of the grayscale intensities.
Blur: Depending on the image region, blurriness may
or may not be desirable. Poor technique or camera
shake tends to yield images that are blurry across the
entire frame, which is generally undesirable. On the
other hand, low depth-of-ﬁeld images with blurred
out-of-focus highlights (“bokeh”) that complement
sharp subjects are often regarded as being pleasing.
To efﬁciently estimate the amount of blur within an
image, we calculate the variance of the Laplacian of
the image. Low variance corresponds to blurrier im-
ages, and high variance to sharper images.
Noise: The desirability of visual noise is contextual.
For most modern images and for images that con-
vey positive emotions, noise is generally undesirable.
For images that convey negative semantics, however,
noise may be desirable to accentuate their visual im-
pact. We measure noise by calculating the image’s en-
tropy.
Saliency: The saliency of the subject within a photo-
graph has a signiﬁcant impact on the perceived aes-
thetic quality of the photograph. We post-process
each image to separate the salient region from the
background using a center-vs-surround approach de-
scribed in Achanta et al [5]. We then sum the number
of salient pixels per image tile and normalize by the
tile size.

5. Methods
Figure 2 depicts a high-level block diagram of the
learning pipeline we built. The pipeline comprises
three main components: an image scraper, a bank of
feature extractors, and a learning algorithm.

tion then involves taking the average of the predic-
tions of all the constituent trees:

(cid:33)

(cid:32)

m(cid:88)

i=1

1
m

h(x) = sign

Ti(x)

Because of the way each decision tree is constructed,
the variance of the average prediction is less than that
of any individual prediction.
It is this characteris-
tic that makes random forests more resistant to over-
ﬁtting than decision trees, and, thus, generally have
much higher performance.
Gradient tree boosting: Boosting is a powerful learn-
ing method that sequentially applies weak classiﬁca-
tion algorithms to reweighted versions of the training
data, with the reweighting done in such a way that,
between every pair of classiﬁers in the sequence, the
examples that were misclassiﬁed by the previous clas-
siﬁer are weighted higher for the next classiﬁer.
In
this manner, each subsequent classiﬁer in the ensem-
ble is forced to concentrate on correctly classifying the
examples that were previously misclassiﬁed.
In gradient tree boosting, or gradient-boosted regres-
sion trees (GBRT), our weak classiﬁers are decision
trees. After ﬁtting the trees, the predictions from all
the decision trees are weighted and combined to form
the ﬁnal prediction:

(cid:32) m(cid:88)

(cid:33)

h(x) = sign

αiTi(x)

i=1

In literature, tree boosting has been identiﬁed as be-
ing one of the best learning algorithms available [6].

6. Experimental results and analysis
For each learning algorithm, we measure the per-
formance of our classiﬁer using 10-fold cross valida-
tion on the photo.net dataset and the DPChallenge
dataset. We run backward feature selection to elimi-
nate ineffective features to improve classiﬁcation per-
formance.
For SVM, we tuned our parameters using grid search,
which ultimately led us to use C = 1 and γ = 0.1. For
random forest, we used 300 decision trees. We deter-
mined this value by empirically ﬁnding the asymp-
totic limit to the generalization error with respect
to the number of decision trees used. For gradient
tree boosting, we used 200 decision trees and a sub-
sampling coefﬁcient of 0.9. Using a sub-sampling co-
efﬁcient smaller than 1 allows us to trade off vari-
ance for bias, which thereby mitigates overﬁtting and
hence improves generalization performance.

photo.net
DPChallenge

RF

GBRT
SVM
78.71% 78.58% 80.88%
82.62% 82.85% 83.60%

Table 1. 10-fold classiﬁcation accuracy

Predicted label

1
1 TP

Actual
label

80.12%

0 FP

18.35%

0

FN
19.88%

TN
81.65%

Figure 3. Confusion matrix for 10-fold cross validation with
GBRT on photo.net dataset.

Table 1 shows our 10-fold cross-validation accuracy
for each of the learning algorithms. For both datasets,
we got the highest performance with GBRTs, with ac-
curacies of 80.88% and 83.60%. That we see similar
quality of results for both datasets signiﬁes that our
methodology is sound.
Figure 3 shows the confusion matrix for 10-fold cross-
validation using GBRTs on the photo.net dataset.
The true positive and false negative rates are ap-
proximately symmetric with the true negative and
false positive rates, respectively, which signiﬁes that
our classiﬁer is not biased towards predicting a cer-
tain class. This also holds true for the DPChallenge
dataset.
To analyze the deﬁciencies of our methodology, we
examine images that our classiﬁer misclassiﬁed.
Figure 4 shows an example of a negative image from
the photo.net dataset that the classiﬁer mispredicted
as being positive. Note that the image is compo-
sitionally sound – the subject is clearly distinguish-
able from the background, ﬁlls most of the frame, is
well-balanced in the frame, and has components that
lie along the rule-of-thirds axes. The hot-pink back-
ground, however, is incredibly jarring, and the subject
matter is mundane and lacks signiﬁcance. Unfortu-
nately, because it discretizes color features so coarsely,
the classiﬁer is likely not able to effectively differen-
tiate between different shades of colors, such as the
artiﬁcial pink shade of this image’s background and
the warm red shade of a beautiful sunset. More-
over, it has no way of gleaning meaning from images.
We therefore believe that it is primarily due to these
shortcomings that our classiﬁer misclassiﬁed this par-
ticular image.

Figure 6. Positive image classiﬁed as negative by the model.

7. Future work and conclusions
We have demonstrated that modeling an image as
a set of tiles, extracting certain visual features from
each tile, and training a learning algorithm to infer
relationships between tiles yields a high-performing
system that adapts well to different datasets. Thus,
our methodology lays a sound foundation for future
development.
In particular, we believe we can fur-
ther improve the accuracy of our system by deriv-
ing global visual features and parsing semantics from
photographs. Our model should also apply to regres-
sion for use cases where numerical ratings are de-
sired. Finally, augmenting the system with the abil-
ity to choose a classiﬁer depending on the identiﬁed
mode of a photograph, e.g. portrait or landscape,
may lead to more accurate classiﬁcation of aesthetic
quality.

Figure 4. Negative image classiﬁed as positive by the
model.

Figure 5. Positive image classiﬁed as negative by the model.

Figure 5 exhibits a photograph from the DPChal-
lenge dataset where our classiﬁer predicts a false neg-
ative. While the photograph follows good composi-
tion techniques, the subject has few high frequency
edges, and most of the tiles are considered blurry.
Our blur feature carries heavy weight with regards to
the prediction for the DPChallenge dataset. Further-
more, the current method of detecting the salient re-
gion is not consistently reliable, so despite this photo-
graph’s having a distinct salient region, the classiﬁer
may deemphasize the contributions of this feature.
We believe that improving our salient region detec-
tion accuracy across all images may enable the classi-
ﬁer to utilize the saliency feature more effectively, and
thus correctly classify this photograph.
Another image our classiﬁer mispredicts as being
negative is shown in Figure 6. The key visual element
of this image is the strong leading lines that draw at-
tention to the hiker – the subject of the image. Lead-
ing lines, however, are global features that are not
well-captured by our tiling methodology, and, thus,
are not considered by the classiﬁer.
In sum, although our classiﬁer performs respectably
well, examining the images it mispredicts reveals
many potential areas of improvement.

Classiﬁcation of photographic images based on

perceived aesthetic quality

Jeff Hwang
Department of Electrical Engineering, Stanford University

Sean Shi
Department of Electrical Engineering, Stanford University

JEFF.HWANG@STANFORD.EDU

SSHI11@STANFORD.EDU

Abstract

In this paper, we explore automated aes-
thetic evaluation of photographs using ma-
chine learning and image processing tech-
niques. We theorize that the spatial dis-
tribution of certain visual elements within
a given image correlates with its aesthetic
quality. To this end, we present a novel ap-
proach wherein we model each photograph
as a set of tiles, extract visual features from
each tile, and train a classiﬁer on the re-
sulting features along with the images’ aes-
thetics ratings. Our model achieves a 10-
fold cross-validation classiﬁcation success
rate of 83.60%, corroborating the efﬁcacy
of our methodology and therefore showing
promise for future development.

1. Introduction
Aesthetics in photography are highly subjective. The
average individual may judge the quality of a photo-
graph simply by gut feeling; in contrast, a photogra-
pher might evaluate a photograph he or she captures
vis-a-vis technical criteria such as composition, con-
trast, and sharpness. Towards fulﬁlling these crite-
ria, photographers follow many rules of thumb. The
actual and relative visual impact of doing so for the
general public, however, remains unclear.
In our project, we show that the existence of cer-
tain characteristics does indeed make an image more
aesthetically-pleasing in general. We achieve this by
building a machine learning pipeline that trains a hy-
pothesis capable of classifying images as either ex-
hibiting high levels of aesthetic quality or not.
The potential impact of building a system to solve
this problem is broad. For example, by implement-

ing such a system, websites with community-sourced
images can programmatically ﬁlter out bad images
to maintain the desired quality of content. Cam-
eras can provide real-time visual feedback to help
users improve their photographic skills. Moreover,
from a cognitive standpoint, solving this problem
may lend interesting insight towards how humans
perceive beauty.
We begin by identifying visual features that we be-
lieve correlate with the aesthetic quality of a photo-
graph. We then build a learning pipeline that ex-
tracts these features from images on a per-tile basis
and uses them along with the images’ aesthetics rat-
ings to train a classiﬁer. In this manner, we endow the
classiﬁer with the freedom to infer spatial relation-
ships amongst features that correlate with an image’s
aesthetics.

2. Related work
There have been several efforts to tackle this problem
from different angles within the past decade. Pogac-
nik et al [1] believed that the features depended heav-
ily on identiﬁcation of the subject of the photograph.
Datta et al [2] evaluated the performance of different
machine learning models (support vector machines,
decision trees) on the problem. Ke et al [3] focused
on extracting perceptual factors important to profes-
sional photographers, such as color, noise, blur, and
spatial distribution of edges.
Also, in contrast to our approach, it is interesting to
note that these studies have focused primarily on ex-
tracting features that attempt to capture prior beliefs
on the spatial orientation of visual elements within
the image. For example, Datta et al attempted to
model rule-of-thirds composition by computing the
average hue, saturation, and luminance of the inner
thirds rectangle, and Pogacnik et al deﬁned features

gorithm would be able to discern the well-composed
framing of the pier from the features extracted from
its containing tiles with respect to those extracted
from the surrounding tiles.
Below, we describe the features we extract from each
image tile.
Subject detection: Strong edges distinguish the sub-
ject from the image’s background. To quantify the
degree of subject-background separation, we apply a
Sobel ﬁlter to each image tile, binarize the result via
Otsu’s method, and compute the proportion of pixels
in the tile that are edge pixels:

fsd =

(x,y)∈Tile

(cid:88)

1{I(x, y) = Edge}

Color palette: A photograph’s color composition can
dramatically inﬂuence how a person perceives a pho-
tograph. We capture the color diversity of a pho-
tograph using a color histogram that subdivides the
three dimensional RGB color space into 64 equally
sized bins. Since each pixel can take on one of 256 dis-
crete values in each color channel, this results in each
bin being a cube with 16 possible values in each di-
mension. We normalize each bin’s count by the total
pixel count so that it is invariant to image dimensions.
Detail: Higher levels of detail are generally desirable
for photographs, particularly for its subject. To ap-
proximate the amount of detail, we compare the num-
ber of edge pixels of a Gaussian ﬁltered version of
the image tile to the number of edge pixels within the
original image tile, i.e.

(cid:80)
(cid:80)
(x,y)∈Tile 1{If iltered(x, y) = Edge}
(x,y)∈Tile 1{I(x, y) = Edge}

fd =

For an image tile that is exceptionally detailed, many
of the higher-frequency edges in the region would be
removed by the Gaussian ﬁlter. Consequently, we
would expect fd to be closer to 0. Conversely, for a
tile that lacks detail, since few edges exist in the re-
gion, applying the Gaussian ﬁlter would impart little
change to the number of edges. In this case, we would
expect fd to be closer to 1.
Hue: To the human eye, certain color combinations
are more appealing than others. To capture this, for
each image tile, we compute the proportion of pix-
els that correspond to a particular hue. We discretize
hues into ﬁve regions, corresponding to red, yellow,
green, blue, and purple.
Saturation: Saturation measures the intensity of a
color. We extract the average saturation value for each
image tile.

Figure 1. Tiling scheme applied to image by learning
pipeline.

that assessed adherence to a multitude of composi-
tional rules as well as the positioning of the subject
relative to the image’s frame.

3. Dataset
Our learning pipeline downloads images and their
average aesthetic ratings from two separate datasets.
The ﬁrst is an image database hosted by photo.net, a
photo sharing website for photographers. The index
ﬁle we use to locate images was generated by Datta
et al. Members of photo.net can upload and critique
each others photographs and rate each photograph
with a number between 1 and 7, with 7 being the best
possible rating. Due to the wide range and subjectiv-
ity of ratings, we choose to only use photographs with
ratings above 6 or below 4.2, which yields a dataset
containing 1700 images split evenly between positive
labels and negative labels.
The second comprises images scraped from DPChal-
lenge, another photo sharing website for photogra-
phers. The index ﬁle we used to locate images was
generated by Murray et al [4]. Following guidelines
from prior work, we choose to use photographs with
ratings above 7.2 or below 3.4, resulting in a dataset
containing 2000 images split evenly between positive
and negative labels.

4. Feature extraction
Prior to extracting features, we partition each image
into ﬁve-by-ﬁve equally-sized tiles (Figure 1). By ex-
tracting features on a per-tile basis, the learning algo-
rithm can identify regions of interest and infer rela-
tionships between feature-tile pairs that indicate aes-
thetic quality. For example, in the case of the image
depicted in Figure 1, we surmise that the learning al-

For each of the features we describe in Section 3, there
exists a feature extractor function that accepts an im-
age as an input, calculates the feature value, and in-
serts the feature-value mapping into a sparse feature
vector allocated for the image. We rely on image pro-
cessing algorithms implemented in the scikit-image
library for many of these functions.
After the pipeline generates feature vectors for all im-
ages in the training set, it uses them to train a clas-
siﬁer. For the learning algorithm, we experimented
with support vector machines (SVM), random forests,
and gradient tree boosting.
SVM: The SVM learning algorithm with (cid:96)1 regu-
larization involves solving the primal optimization
problem

m(cid:88)

min
γ,w,b

subject to

||w||2 + C

1
2
y(i)(wT x(i) + b) ≥ 1 − ξi, i = 1, ..., m

i=1

ξi

, the dual of which is

m(cid:88)

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105)

max

α

subject to

αi − 1
2

i,j=1

0 ≤ αi ≤ C, i = 1, ..., m

αiy(i) = 0

m(cid:88)
m(cid:88)

i=1

i=1

Accordingly, provided that we ﬁnd the values of α
that maximize the dual optimization problem, the hy-
pothesis can be formulated as

if (cid:80)m

(cid:26) 1

−1

h(x) =

i=1 αiy(i)(cid:104)x(i), x(cid:105) + b ≥ 0

otherwise

Note that since the dual optimization problem and
hypothesis can be expressed as inner products be-
tween input feature vectors, we can replace each in-
ner product with a kernel applied to the two in-
put vectors, which allows us to train our classiﬁer
and perform classiﬁcation in a higher-dimensional
feature space. This characteristic of SVMs makes
them well-suited for our problem since we speculate
that non-linear relationships amongst multiple fea-
tures inﬂuence image aesthetic quality. For our sys-
tem, we choose to use the Gaussian kernel K(x, y) =

(cid:1), which corresponds to an inﬁnite-

exp(cid:0)γ||x − y||2

dimensional feature mapping.
Random forest: Random forests comprise collections
of decision trees. Each decision tree is grown by se-
lecting a random subset of input variables to serve as
candidates for splitting at a particular node. Predic-

2

Figure 2. Block diagram of learning pipeline.

Contrast: Contrast is the difference in color or bright-
ness amongst regions in an image. Generally, the
higher the contrast, the more distinguishable objects
are from one another. We approximate the contrast
within each image tile by calculating the standard de-
viation of the grayscale intensities.
Blur: Depending on the image region, blurriness may
or may not be desirable. Poor technique or camera
shake tends to yield images that are blurry across the
entire frame, which is generally undesirable. On the
other hand, low depth-of-ﬁeld images with blurred
out-of-focus highlights (“bokeh”) that complement
sharp subjects are often regarded as being pleasing.
To efﬁciently estimate the amount of blur within an
image, we calculate the variance of the Laplacian of
the image. Low variance corresponds to blurrier im-
ages, and high variance to sharper images.
Noise: The desirability of visual noise is contextual.
For most modern images and for images that con-
vey positive emotions, noise is generally undesirable.
For images that convey negative semantics, however,
noise may be desirable to accentuate their visual im-
pact. We measure noise by calculating the image’s en-
tropy.
Saliency: The saliency of the subject within a photo-
graph has a signiﬁcant impact on the perceived aes-
thetic quality of the photograph. We post-process
each image to separate the salient region from the
background using a center-vs-surround approach de-
scribed in Achanta et al [5]. We then sum the number
of salient pixels per image tile and normalize by the
tile size.

5. Methods
Figure 2 depicts a high-level block diagram of the
learning pipeline we built. The pipeline comprises
three main components: an image scraper, a bank of
feature extractors, and a learning algorithm.

tion then involves taking the average of the predic-
tions of all the constituent trees:

(cid:33)

(cid:32)

m(cid:88)

i=1

1
m

h(x) = sign

Ti(x)

Because of the way each decision tree is constructed,
the variance of the average prediction is less than that
of any individual prediction.
It is this characteris-
tic that makes random forests more resistant to over-
ﬁtting than decision trees, and, thus, generally have
much higher performance.
Gradient tree boosting: Boosting is a powerful learn-
ing method that sequentially applies weak classiﬁca-
tion algorithms to reweighted versions of the training
data, with the reweighting done in such a way that,
between every pair of classiﬁers in the sequence, the
examples that were misclassiﬁed by the previous clas-
siﬁer are weighted higher for the next classiﬁer.
In
this manner, each subsequent classiﬁer in the ensem-
ble is forced to concentrate on correctly classifying the
examples that were previously misclassiﬁed.
In gradient tree boosting, or gradient-boosted regres-
sion trees (GBRT), our weak classiﬁers are decision
trees. After ﬁtting the trees, the predictions from all
the decision trees are weighted and combined to form
the ﬁnal prediction:

(cid:32) m(cid:88)

(cid:33)

h(x) = sign

αiTi(x)

i=1

In literature, tree boosting has been identiﬁed as be-
ing one of the best learning algorithms available [6].

6. Experimental results and analysis
For each learning algorithm, we measure the per-
formance of our classiﬁer using 10-fold cross valida-
tion on the photo.net dataset and the DPChallenge
dataset. We run backward feature selection to elimi-
nate ineffective features to improve classiﬁcation per-
formance.
For SVM, we tuned our parameters using grid search,
which ultimately led us to use C = 1 and γ = 0.1. For
random forest, we used 300 decision trees. We deter-
mined this value by empirically ﬁnding the asymp-
totic limit to the generalization error with respect
to the number of decision trees used. For gradient
tree boosting, we used 200 decision trees and a sub-
sampling coefﬁcient of 0.9. Using a sub-sampling co-
efﬁcient smaller than 1 allows us to trade off vari-
ance for bias, which thereby mitigates overﬁtting and
hence improves generalization performance.

photo.net
DPChallenge

RF

GBRT
SVM
78.71% 78.58% 80.88%
82.62% 82.85% 83.60%

Table 1. 10-fold classiﬁcation accuracy

Predicted label

1
1 TP

Actual
label

80.12%

0 FP

18.35%

0

FN
19.88%

TN
81.65%

Figure 3. Confusion matrix for 10-fold cross validation with
GBRT on photo.net dataset.

Table 1 shows our 10-fold cross-validation accuracy
for each of the learning algorithms. For both datasets,
we got the highest performance with GBRTs, with ac-
curacies of 80.88% and 83.60%. That we see similar
quality of results for both datasets signiﬁes that our
methodology is sound.
Figure 3 shows the confusion matrix for 10-fold cross-
validation using GBRTs on the photo.net dataset.
The true positive and false negative rates are ap-
proximately symmetric with the true negative and
false positive rates, respectively, which signiﬁes that
our classiﬁer is not biased towards predicting a cer-
tain class. This also holds true for the DPChallenge
dataset.
To analyze the deﬁciencies of our methodology, we
examine images that our classiﬁer misclassiﬁed.
Figure 4 shows an example of a negative image from
the photo.net dataset that the classiﬁer mispredicted
as being positive. Note that the image is compo-
sitionally sound – the subject is clearly distinguish-
able from the background, ﬁlls most of the frame, is
well-balanced in the frame, and has components that
lie along the rule-of-thirds axes. The hot-pink back-
ground, however, is incredibly jarring, and the subject
matter is mundane and lacks signiﬁcance. Unfortu-
nately, because it discretizes color features so coarsely,
the classiﬁer is likely not able to effectively differen-
tiate between different shades of colors, such as the
artiﬁcial pink shade of this image’s background and
the warm red shade of a beautiful sunset. More-
over, it has no way of gleaning meaning from images.
We therefore believe that it is primarily due to these
shortcomings that our classiﬁer misclassiﬁed this par-
ticular image.

Figure 6. Positive image classiﬁed as negative by the model.

7. Future work and conclusions
We have demonstrated that modeling an image as
a set of tiles, extracting certain visual features from
each tile, and training a learning algorithm to infer
relationships between tiles yields a high-performing
system that adapts well to different datasets. Thus,
our methodology lays a sound foundation for future
development.
In particular, we believe we can fur-
ther improve the accuracy of our system by deriv-
ing global visual features and parsing semantics from
photographs. Our model should also apply to regres-
sion for use cases where numerical ratings are de-
sired. Finally, augmenting the system with the abil-
ity to choose a classiﬁer depending on the identiﬁed
mode of a photograph, e.g. portrait or landscape,
may lead to more accurate classiﬁcation of aesthetic
quality.

Figure 4. Negative image classiﬁed as positive by the
model.

Figure 5. Positive image classiﬁed as negative by the model.

Figure 5 exhibits a photograph from the DPChal-
lenge dataset where our classiﬁer predicts a false neg-
ative. While the photograph follows good composi-
tion techniques, the subject has few high frequency
edges, and most of the tiles are considered blurry.
Our blur feature carries heavy weight with regards to
the prediction for the DPChallenge dataset. Further-
more, the current method of detecting the salient re-
gion is not consistently reliable, so despite this photo-
graph’s having a distinct salient region, the classiﬁer
may deemphasize the contributions of this feature.
We believe that improving our salient region detec-
tion accuracy across all images may enable the classi-
ﬁer to utilize the saliency feature more effectively, and
thus correctly classify this photograph.
Another image our classiﬁer mispredicts as being
negative is shown in Figure 6. The key visual element
of this image is the strong leading lines that draw at-
tention to the hiker – the subject of the image. Lead-
ing lines, however, are global features that are not
well-captured by our tiling methodology, and, thus,
are not considered by the classiﬁer.
In sum, although our classiﬁer performs respectably
well, examining the images it mispredicts reveals
many potential areas of improvement.

References
[1] D. Pogacnik, R. Ravnik, N. Bovcon, and F.
Solina. Evaluating Photo Aesthetics Using Ma-
chine Learning. University of Ljubljana.

[2] R. Datta, D. Joshi, J. Li, J.Z. Wang. Studying Aes-
thetics in Photographic Images Using a Compu-
tational Approach.

[3] Y. Ke, X. Tang, F. Jing. The Design of High-
Level Features for Photo Quality Assessment.
School of Computer Science, Carnegie Mellon.
Microsoft Research Asia.

[4] N. Murray, L. Marchesotti, F. Perronnin. AVA: A
Large-Scale Database for Aesthetic Visual Analy-
sis. In Proc. IEEE Conf. Computer Vision and Pat-
tern Recognition.

[5] R. Achanta, S. Hemami, F. Estrada and S.
Susstrunk, Frequency-tuned Salient Region De-
tection, IEEE International Conference on Computer
Vision and Pattern Recognition.

[6] T. Hastie, R. Tibshirani, J. Friedman (2001). The
Elements of Statistical Learning. New York, NY,
USA: Springer New York Inc..

