Generating Anaglyphs from Light Field Images

Pablo A. Vasquez Guzman 

Department of Mechanical Engineering 

Stanford University 

Stanford, CA 

pabloavg@stanford.edu 

 
 

Abstract—Light-field  imaging  systems  have  received  a  lot  of 
attention recently, especially with the release of Lytro cameras for 
consumer application. Extensive research has been conducted in 
optimizing and developing applications for light-field images [1,2]. 
To investigate the potential use of light-field imaging systems as an 
experimental  analysis  tool,  an  automated  image  processing 
algorithm was developed to generate anaglyphs images from light-
field images acquired from a Lytro Illum camera. 

Keywords—Light-field Images; Anaglyphs; Stereo Image Pairs; 

Disparity 

I.  INTRODUCTION 

Conventional  cameras  capture  2D  images,  which  are 
projections of a 3D scene. Light-field imaging systems capture 
not  only  the  projection  but  also  the  directions  of  incoming 
lighting that project onto a sensor. Specifically, Lytro cameras 
consist  of  an  array  of  microlenses  placed  in  front  of  the 
photosensor  used  to  separate  the  light  rays  striking  each 
microlens, and to focus them on different sensors according to 
their directions as shown in Fig. 1. 
Fig. 1.   Simplified diagram of a light-field imaging system [3] 

 

The  acquired  light-field  allows  for  more  flexible  image 
manipulating.  Enough  information  in  captured  that  one  can 
refocus  images  after  acquisition,  as  well  as  shift  one’s 
viewpoint. 

Lytro  offers  an 

image  processing  application,  Lytro 
Desktop, which is a powerful post processing tool for light-field 
images.  One  available  option  is  exporting  anaglyphs  from 
imported  light-field  images;  however,  no  information  is 
available on how the anaglyphs are generated. In addition, no 
prior  work  is  available  regarding  generating  anaglyphs  from 
light-field images. 

The  standard  method  for  generating  anaglyphs  requires  a 
pair of images, which have been acquired at slightly different 
perspective views, called stereo image pairs. Normally, stereo 
image pairs are acquired using a pair of conventional cameras 
separated by a distance duplicating the spacing of human eyes. 
It is possible to create an anaglyph from just one image taken 
from a conventional camera, but that requires extensive work in 
a  graphics  editing  program  such  as  Photoshop  and  it  isn’t  an 
automated or accurate process. 

The  specific  objective  of  the  work  described  here  was  to 
develop  an  automated 
image  processing  algorithm  for 
generating anaglyphs using the rich information captured in a 
light-field image acquired from a Lytro Illum camera. 

II.  METHODS 

  Two automated image processing algorithms for generating 
anaglyphs were developed. The first method is based on direct 
perspective views, while the second method is based on depth-
field information. 

A.  Method 1 

Lytro uses a hexagonal arrangement of a microlens in front 
of  an  image  sensor  to  efficiently  capture  4D  light  field 
information on a two dimensional plane [2]. The sensor pixels 
behind  the  microlens  array  only  records  light  intensity. 
Directional information of light is recorded in the location of 
sensor pixels relative to the microlens array [4]. Stereo image 
pair can be obtained directly from the raw data to generated an 
anaglyph.  The general process is described below: 

 
1)  Calibration  and  Decoding:  Align,  color-correct,  and 
rectify raw 2D light-field image to convert the into standard 4D 
light-field data format as illustrated in Fig. 2.  
 
Fig. 2.  Raw 2D light-field image conversion to 4D light-field data format [5] 

𝑣 

𝑢 

𝑢⋅	𝑣 
𝑡 

𝑠 

 

Generating Anaglyphs from Light Field Images

Pablo A. Vasquez Guzman 

Department of Mechanical Engineering 

Stanford University 

Stanford, CA 

pabloavg@stanford.edu 

 
 

Abstract—Light-field  imaging  systems  have  received  a  lot  of 
attention recently, especially with the release of Lytro cameras for 
consumer application. Extensive research has been conducted in 
optimizing and developing applications for light-field images [1,2]. 
To investigate the potential use of light-field imaging systems as an 
experimental  analysis  tool,  an  automated  image  processing 
algorithm was developed to generate anaglyphs images from light-
field images acquired from a Lytro Illum camera. 

Keywords—Light-field Images; Anaglyphs; Stereo Image Pairs; 

Disparity 

I.  INTRODUCTION 

Conventional  cameras  capture  2D  images,  which  are 
projections of a 3D scene. Light-field imaging systems capture 
not  only  the  projection  but  also  the  directions  of  incoming 
lighting that project onto a sensor. Specifically, Lytro cameras 
consist  of  an  array  of  microlenses  placed  in  front  of  the 
photosensor  used  to  separate  the  light  rays  striking  each 
microlens, and to focus them on different sensors according to 
their directions as shown in Fig. 1. 
Fig. 1.   Simplified diagram of a light-field imaging system [3] 

 

The  acquired  light-field  allows  for  more  flexible  image 
manipulating.  Enough  information  in  captured  that  one  can 
refocus  images  after  acquisition,  as  well  as  shift  one’s 
viewpoint. 

Lytro  offers  an 

image  processing  application,  Lytro 
Desktop, which is a powerful post processing tool for light-field 
images.  One  available  option  is  exporting  anaglyphs  from 
imported  light-field  images;  however,  no  information  is 
available on how the anaglyphs are generated. In addition, no 
prior  work  is  available  regarding  generating  anaglyphs  from 
light-field images. 

The  standard  method  for  generating  anaglyphs  requires  a 
pair of images, which have been acquired at slightly different 
perspective views, called stereo image pairs. Normally, stereo 
image pairs are acquired using a pair of conventional cameras 
separated by a distance duplicating the spacing of human eyes. 
It is possible to create an anaglyph from just one image taken 
from a conventional camera, but that requires extensive work in 
a  graphics  editing  program  such  as  Photoshop  and  it  isn’t  an 
automated or accurate process. 

The  specific  objective  of  the  work  described  here  was  to 
develop  an  automated 
image  processing  algorithm  for 
generating anaglyphs using the rich information captured in a 
light-field image acquired from a Lytro Illum camera. 

II.  METHODS 

  Two automated image processing algorithms for generating 
anaglyphs were developed. The first method is based on direct 
perspective views, while the second method is based on depth-
field information. 

A.  Method 1 

Lytro uses a hexagonal arrangement of a microlens in front 
of  an  image  sensor  to  efficiently  capture  4D  light  field 
information on a two dimensional plane [2]. The sensor pixels 
behind  the  microlens  array  only  records  light  intensity. 
Directional information of light is recorded in the location of 
sensor pixels relative to the microlens array [4]. Stereo image 
pair can be obtained directly from the raw data to generated an 
anaglyph.  The general process is described below: 

 
1)  Calibration  and  Decoding:  Align,  color-correct,  and 
rectify raw 2D light-field image to convert the into standard 4D 
light-field data format as illustrated in Fig. 2.  
 
Fig. 2.  Raw 2D light-field image conversion to 4D light-field data format [5] 

𝑣 

𝑢 

𝑢⋅	𝑣 
𝑡 

𝑠 

 

Segmentation  of  the  depth  map  is  performed  using  an 

optimized multilevel Otsu’s method. For N+1 segments (𝑁=
1,2,...,20), the threshold values are computed and a metric is 
thresholds.  The  depth  map  is  then  segment  into 𝑁+1 
If  the  number  of  segments  is  large  (𝑁>10),  then  the 

segments  corresponding  with  metric  with 
effectiveness. 

recorded  to  measure  of  the  effectiveness  of  the  computed 

the  highest 

segmentation is refined. The greatest variability in depth values 
usually occurs in the near- and far-field regions, so first few and 
last few segments are combined to reduce variability and create 
a more uniform segmentation.  

4)  Displacement:  The  focused  image  is  segmented  based 
on the segmentation of the depth map with the average depth 
value for each segmented region computed. To obtain stereo 
pair  images  requires  generating  a  left  and  right  perspective 
view. Fig. 4 show how a typical stereo imaging system works  

A Simple Stereo System
A Simple Stereo System
Top Down View (XZ plane)

Fig. 4.   Geometric schematic explanation of simple stereo system [7]  

 

P=(X,Y,Z)
P=(X,Y,Z)

Left 
camera

xxl

Z?Z?

xxrr

Right
camera

O

TTxx

ff
xl = f.X / Z
Translated by a distance Tx along X axis
yl = f.Y / Z
(Tx is also called the stereo “baseline”)

The disparty, 𝑑, observed between the left and right view 
based on some point 𝑃 is inversly propotional to the depth.  
where 𝑓  is  the  focal  length, 𝑇7  is  the  distance  between  the 
views, and 𝑍 is the depth. Based on the computed depth values, 

𝑑=𝑓	⋅𝑇7/𝑍 

the image segments are displaced accordinly to generate a left 
and right perspective view.  

(1) 

 

 

5)  Hole  Removal:  Due  to  the  uneven  displacement  of 
segmented regions of the image and occlusion of objects in the 
image, holes are typically generated in the process of generating 
the  stereo  pair  images.  Proper  removal  of  these  holes  is 
necessary  to  minimize  artifacts.  The  hole  removal  process  is 
described below: 

a) Detection: Convert the stereo image pairs to gray scale 
and  apply  Otsu’s  method  to  binerize  image  into  indicating 
region without any information.    

b) Filling:  Dilate  binary  hole  image  to  create  a  mask, 
which include surrounding pixel values around all hole regions. 
Use surrounding average pixel intensity values to fill in hole 
region values. 

B.  Method 2  

Digital Image Processing © 2015 Stanford University  -- Panoramic Imaging 16

Translating the camera is equivalent to

keeping the camera ﬁxed and translating the world

in opposite direction

 

The 

into  𝐿𝑠,𝑡,𝑢,𝑣  
representation,  where  coordinates (𝑠,𝑡)  correspond  to  each 
microlens in the array and the coordinates (𝑢,𝑣) correspond to 

raw  2D  data 

is  converted 

image aperture [4]. The resolution of the raw 2D light-field is 
about 40-megapixels. For each microlens, the diameter is about 
14 pixels. Dividing the raw image resolution by the size of the 
microlens, the effective resolution of the light-field images is 
about 0.2-megapixels. Note that Lytro Desktop exports images 
with 4-megapixel resolution, which indicated that Lytro uses an 
algorithm to enhance the resolution of its images. 

2)  Extraction:  Extract  the  left  most  and  right  most 
perspective  view  images  from  4D  light-field  data.  The 
coordinates  (u,v)  effectively  describe  viewpoint.  Figure  3 
shows a simplified diagram of corresponding perspective views 
extracted from the raw 2D light-field microlens.  

Fig. 3.   Simplified diagram of a perspective view extraction from raw 2D light-
field image [4] 

Robert Collins
CSE486, Penn State

 

𝑢 

𝑣 

 

 

Since  the  effective  resolution  of  the  extracted  images  are 
0.2-megapixels,  the  images  are  resized  to  have  about  4-
megapixel resolution. 

3)  Generate  Anaglyph:  Use  left  and  right  extracted 

perspective views to generate anaglyph. 

Depth-field  information  can  be  computed  from  the  light-
field  image,  which  can  be  used  to  artifically  generate  stereo 
image pairs to generated an anaglyph. The general process is 
described below: 

1)  Calibration  and  Decoding:  Align,  color-correct,  and 
rectify raw 2D light-field image to convert the into standard 4D 
light-field data format. 

2)  Depth Map Estimation:  Depth  can  be  estimated  either 
from blurred images in a focal stack or from perspective view 
form  a  disparity  stack.  Jeon,  et  al.,  describe  an  effective 
algorithm  to  accurately  estimate  depth  map  from  light-field 
images based on disparity map [6]. The algorithm developed 
uses a cost-volume-based methodology.  

the  focused 

3)  Segmentation: Generating a stereo image pairs requires 
displacing 
to  depth-field 
information.  Displacing  each  individual  pixel  requires  sub-
pixel accuracy to avoid possible artifacts. To minimize artifacts, 
the depth map is optimally segmented.  

image  according 

Generating Anaglyphs from Light Field Images

Pablo A. Vasquez Guzman 

Department of Mechanical Engineering 

Stanford University 

Stanford, CA 

pabloavg@stanford.edu 

 
 

Abstract—Light-field  imaging  systems  have  received  a  lot  of 
attention recently, especially with the release of Lytro cameras for 
consumer application. Extensive research has been conducted in 
optimizing and developing applications for light-field images [1,2]. 
To investigate the potential use of light-field imaging systems as an 
experimental  analysis  tool,  an  automated  image  processing 
algorithm was developed to generate anaglyphs images from light-
field images acquired from a Lytro Illum camera. 

Keywords—Light-field Images; Anaglyphs; Stereo Image Pairs; 

Disparity 

I.  INTRODUCTION 

Conventional  cameras  capture  2D  images,  which  are 
projections of a 3D scene. Light-field imaging systems capture 
not  only  the  projection  but  also  the  directions  of  incoming 
lighting that project onto a sensor. Specifically, Lytro cameras 
consist  of  an  array  of  microlenses  placed  in  front  of  the 
photosensor  used  to  separate  the  light  rays  striking  each 
microlens, and to focus them on different sensors according to 
their directions as shown in Fig. 1. 
Fig. 1.   Simplified diagram of a light-field imaging system [3] 

 

The  acquired  light-field  allows  for  more  flexible  image 
manipulating.  Enough  information  in  captured  that  one  can 
refocus  images  after  acquisition,  as  well  as  shift  one’s 
viewpoint. 

Lytro  offers  an 

image  processing  application,  Lytro 
Desktop, which is a powerful post processing tool for light-field 
images.  One  available  option  is  exporting  anaglyphs  from 
imported  light-field  images;  however,  no  information  is 
available on how the anaglyphs are generated. In addition, no 
prior  work  is  available  regarding  generating  anaglyphs  from 
light-field images. 

The  standard  method  for  generating  anaglyphs  requires  a 
pair of images, which have been acquired at slightly different 
perspective views, called stereo image pairs. Normally, stereo 
image pairs are acquired using a pair of conventional cameras 
separated by a distance duplicating the spacing of human eyes. 
It is possible to create an anaglyph from just one image taken 
from a conventional camera, but that requires extensive work in 
a  graphics  editing  program  such  as  Photoshop  and  it  isn’t  an 
automated or accurate process. 

The  specific  objective  of  the  work  described  here  was  to 
develop  an  automated 
image  processing  algorithm  for 
generating anaglyphs using the rich information captured in a 
light-field image acquired from a Lytro Illum camera. 

II.  METHODS 

  Two automated image processing algorithms for generating 
anaglyphs were developed. The first method is based on direct 
perspective views, while the second method is based on depth-
field information. 

A.  Method 1 

Lytro uses a hexagonal arrangement of a microlens in front 
of  an  image  sensor  to  efficiently  capture  4D  light  field 
information on a two dimensional plane [2]. The sensor pixels 
behind  the  microlens  array  only  records  light  intensity. 
Directional information of light is recorded in the location of 
sensor pixels relative to the microlens array [4]. Stereo image 
pair can be obtained directly from the raw data to generated an 
anaglyph.  The general process is described below: 

 
1)  Calibration  and  Decoding:  Align,  color-correct,  and 
rectify raw 2D light-field image to convert the into standard 4D 
light-field data format as illustrated in Fig. 2.  
 
Fig. 2.  Raw 2D light-field image conversion to 4D light-field data format [5] 

𝑣 

𝑢 

𝑢⋅	𝑣 
𝑡 

𝑠 

 

Segmentation  of  the  depth  map  is  performed  using  an 

optimized multilevel Otsu’s method. For N+1 segments (𝑁=
1,2,...,20), the threshold values are computed and a metric is 
thresholds.  The  depth  map  is  then  segment  into 𝑁+1 
If  the  number  of  segments  is  large  (𝑁>10),  then  the 

segments  corresponding  with  metric  with 
effectiveness. 

recorded  to  measure  of  the  effectiveness  of  the  computed 

the  highest 

segmentation is refined. The greatest variability in depth values 
usually occurs in the near- and far-field regions, so first few and 
last few segments are combined to reduce variability and create 
a more uniform segmentation.  

4)  Displacement:  The  focused  image  is  segmented  based 
on the segmentation of the depth map with the average depth 
value for each segmented region computed. To obtain stereo 
pair  images  requires  generating  a  left  and  right  perspective 
view. Fig. 4 show how a typical stereo imaging system works  

A Simple Stereo System
A Simple Stereo System
Top Down View (XZ plane)

Fig. 4.   Geometric schematic explanation of simple stereo system [7]  

 

P=(X,Y,Z)
P=(X,Y,Z)

Left 
camera

xxl

Z?Z?

xxrr

Right
camera

O

TTxx

ff
xl = f.X / Z
Translated by a distance Tx along X axis
yl = f.Y / Z
(Tx is also called the stereo “baseline”)

The disparty, 𝑑, observed between the left and right view 
based on some point 𝑃 is inversly propotional to the depth.  
where 𝑓  is  the  focal  length, 𝑇7  is  the  distance  between  the 
views, and 𝑍 is the depth. Based on the computed depth values, 

𝑑=𝑓	⋅𝑇7/𝑍 

the image segments are displaced accordinly to generate a left 
and right perspective view.  

(1) 

 

 

5)  Hole  Removal:  Due  to  the  uneven  displacement  of 
segmented regions of the image and occlusion of objects in the 
image, holes are typically generated in the process of generating 
the  stereo  pair  images.  Proper  removal  of  these  holes  is 
necessary  to  minimize  artifacts.  The  hole  removal  process  is 
described below: 

a) Detection: Convert the stereo image pairs to gray scale 
and  apply  Otsu’s  method  to  binerize  image  into  indicating 
region without any information.    

b) Filling:  Dilate  binary  hole  image  to  create  a  mask, 
which include surrounding pixel values around all hole regions. 
Use surrounding average pixel intensity values to fill in hole 
region values. 

B.  Method 2  

Digital Image Processing © 2015 Stanford University  -- Panoramic Imaging 16

Translating the camera is equivalent to

keeping the camera ﬁxed and translating the world

in opposite direction

 

The 

into  𝐿𝑠,𝑡,𝑢,𝑣  
representation,  where  coordinates (𝑠,𝑡)  correspond  to  each 
microlens in the array and the coordinates (𝑢,𝑣) correspond to 

raw  2D  data 

is  converted 

image aperture [4]. The resolution of the raw 2D light-field is 
about 40-megapixels. For each microlens, the diameter is about 
14 pixels. Dividing the raw image resolution by the size of the 
microlens, the effective resolution of the light-field images is 
about 0.2-megapixels. Note that Lytro Desktop exports images 
with 4-megapixel resolution, which indicated that Lytro uses an 
algorithm to enhance the resolution of its images. 

2)  Extraction:  Extract  the  left  most  and  right  most 
perspective  view  images  from  4D  light-field  data.  The 
coordinates  (u,v)  effectively  describe  viewpoint.  Figure  3 
shows a simplified diagram of corresponding perspective views 
extracted from the raw 2D light-field microlens.  

Fig. 3.   Simplified diagram of a perspective view extraction from raw 2D light-
field image [4] 

Robert Collins
CSE486, Penn State

 

𝑢 

𝑣 

 

 

Since  the  effective  resolution  of  the  extracted  images  are 
0.2-megapixels,  the  images  are  resized  to  have  about  4-
megapixel resolution. 

3)  Generate  Anaglyph:  Use  left  and  right  extracted 

perspective views to generate anaglyph. 

Depth-field  information  can  be  computed  from  the  light-
field  image,  which  can  be  used  to  artifically  generate  stereo 
image pairs to generated an anaglyph. The general process is 
described below: 

1)  Calibration  and  Decoding:  Align,  color-correct,  and 
rectify raw 2D light-field image to convert the into standard 4D 
light-field data format. 

2)  Depth Map Estimation:  Depth  can  be  estimated  either 
from blurred images in a focal stack or from perspective view 
form  a  disparity  stack.  Jeon,  et  al.,  describe  an  effective 
algorithm  to  accurately  estimate  depth  map  from  light-field 
images based on disparity map [6]. The algorithm developed 
uses a cost-volume-based methodology.  

the  focused 

3)  Segmentation: Generating a stereo image pairs requires 
displacing 
to  depth-field 
information.  Displacing  each  individual  pixel  requires  sub-
pixel accuracy to avoid possible artifacts. To minimize artifacts, 
the depth map is optimally segmented.  

image  according 

Fig. 7. Intermetiate results of second image processing algorithm applied to an 
example light-field image 

1)  Calibration and Decoding… 
2)  Depth Map Estimation… 
 

   

3)  Segmentation and Refinement:  

 

 

 

 

 

 

6)  Anaglyph  Generation:  Use  left  and  right  generated 

perspective views to generate anaglyph. 

III.  RESULTS 

A.  Method 1 

The light-field images acquired from a Lytro Illum camera 
were  calibrated  and  decoded  using  Light  Field  Toolbox  for 
Matlab developed by Donald G. Dansereau. Fig. 5 shows the 
intermediate results in applying the image processing algorithm 
for an example light-field image. 
Fig. 5.   Intermetiate results of first image processing algorithm applied to an 
example light-field image 

1)  Calibration and Decoding… 
2)  Extraction: 

 

3)  Generate Anaglyphs  

  

 

4)  Displacement: 

 

 

 
Fig.  6  shows  the  final  results  of  generating  anaglyphs  for 
some of the accessible light-field images. Note that the color-
correction  applied  didn’t  properly  balance  the  colors  in  the 
images.  The  applied  color-correction  was  an  shades-of-gray 
technique,  but  a  more  robust  color-correction  technique  is 
needed to generate results similar to those generated by Lytro 
Desktop.  
Fig. 6.   Results of first automated image processing algorithm method  

5)  Fill Holes: 

a) Detection: 

b) Filling: 

   

 

 
B.  Method 2 

Unfortunately, an accurate depth map was unable to be 
computed from the light-field images using the described 
methodology. More rudimentary attempts of computing depth 
maps were attempted, but the results were unsatisfactory. The 
anaglyphs results shown below were generated using the depth 
map and focused image exported from Lytro Desktop. Fig. 7 
show intermediate result of the image processing algorithm for 
one light-field image.  

6)  Generate Anaglyph:  

   

 

   

 

   

 

   

 

 

Generating Anaglyphs from Light Field Images

Pablo A. Vasquez Guzman 

Department of Mechanical Engineering 

Stanford University 

Stanford, CA 

pabloavg@stanford.edu 

 
 

Abstract—Light-field  imaging  systems  have  received  a  lot  of 
attention recently, especially with the release of Lytro cameras for 
consumer application. Extensive research has been conducted in 
optimizing and developing applications for light-field images [1,2]. 
To investigate the potential use of light-field imaging systems as an 
experimental  analysis  tool,  an  automated  image  processing 
algorithm was developed to generate anaglyphs images from light-
field images acquired from a Lytro Illum camera. 

Keywords—Light-field Images; Anaglyphs; Stereo Image Pairs; 

Disparity 

I.  INTRODUCTION 

Conventional  cameras  capture  2D  images,  which  are 
projections of a 3D scene. Light-field imaging systems capture 
not  only  the  projection  but  also  the  directions  of  incoming 
lighting that project onto a sensor. Specifically, Lytro cameras 
consist  of  an  array  of  microlenses  placed  in  front  of  the 
photosensor  used  to  separate  the  light  rays  striking  each 
microlens, and to focus them on different sensors according to 
their directions as shown in Fig. 1. 
Fig. 1.   Simplified diagram of a light-field imaging system [3] 

 

The  acquired  light-field  allows  for  more  flexible  image 
manipulating.  Enough  information  in  captured  that  one  can 
refocus  images  after  acquisition,  as  well  as  shift  one’s 
viewpoint. 

Lytro  offers  an 

image  processing  application,  Lytro 
Desktop, which is a powerful post processing tool for light-field 
images.  One  available  option  is  exporting  anaglyphs  from 
imported  light-field  images;  however,  no  information  is 
available on how the anaglyphs are generated. In addition, no 
prior  work  is  available  regarding  generating  anaglyphs  from 
light-field images. 

The  standard  method  for  generating  anaglyphs  requires  a 
pair of images, which have been acquired at slightly different 
perspective views, called stereo image pairs. Normally, stereo 
image pairs are acquired using a pair of conventional cameras 
separated by a distance duplicating the spacing of human eyes. 
It is possible to create an anaglyph from just one image taken 
from a conventional camera, but that requires extensive work in 
a  graphics  editing  program  such  as  Photoshop  and  it  isn’t  an 
automated or accurate process. 

The  specific  objective  of  the  work  described  here  was  to 
develop  an  automated 
image  processing  algorithm  for 
generating anaglyphs using the rich information captured in a 
light-field image acquired from a Lytro Illum camera. 

II.  METHODS 

  Two automated image processing algorithms for generating 
anaglyphs were developed. The first method is based on direct 
perspective views, while the second method is based on depth-
field information. 

A.  Method 1 

Lytro uses a hexagonal arrangement of a microlens in front 
of  an  image  sensor  to  efficiently  capture  4D  light  field 
information on a two dimensional plane [2]. The sensor pixels 
behind  the  microlens  array  only  records  light  intensity. 
Directional information of light is recorded in the location of 
sensor pixels relative to the microlens array [4]. Stereo image 
pair can be obtained directly from the raw data to generated an 
anaglyph.  The general process is described below: 

 
1)  Calibration  and  Decoding:  Align,  color-correct,  and 
rectify raw 2D light-field image to convert the into standard 4D 
light-field data format as illustrated in Fig. 2.  
 
Fig. 2.  Raw 2D light-field image conversion to 4D light-field data format [5] 

𝑣 

𝑢 

𝑢⋅	𝑣 
𝑡 

𝑠 

 

Segmentation  of  the  depth  map  is  performed  using  an 

optimized multilevel Otsu’s method. For N+1 segments (𝑁=
1,2,...,20), the threshold values are computed and a metric is 
thresholds.  The  depth  map  is  then  segment  into 𝑁+1 
If  the  number  of  segments  is  large  (𝑁>10),  then  the 

segments  corresponding  with  metric  with 
effectiveness. 

recorded  to  measure  of  the  effectiveness  of  the  computed 

the  highest 

segmentation is refined. The greatest variability in depth values 
usually occurs in the near- and far-field regions, so first few and 
last few segments are combined to reduce variability and create 
a more uniform segmentation.  

4)  Displacement:  The  focused  image  is  segmented  based 
on the segmentation of the depth map with the average depth 
value for each segmented region computed. To obtain stereo 
pair  images  requires  generating  a  left  and  right  perspective 
view. Fig. 4 show how a typical stereo imaging system works  

A Simple Stereo System
A Simple Stereo System
Top Down View (XZ plane)

Fig. 4.   Geometric schematic explanation of simple stereo system [7]  

 

P=(X,Y,Z)
P=(X,Y,Z)

Left 
camera

xxl

Z?Z?

xxrr

Right
camera

O

TTxx

ff
xl = f.X / Z
Translated by a distance Tx along X axis
yl = f.Y / Z
(Tx is also called the stereo “baseline”)

The disparty, 𝑑, observed between the left and right view 
based on some point 𝑃 is inversly propotional to the depth.  
where 𝑓  is  the  focal  length, 𝑇7  is  the  distance  between  the 
views, and 𝑍 is the depth. Based on the computed depth values, 

𝑑=𝑓	⋅𝑇7/𝑍 

the image segments are displaced accordinly to generate a left 
and right perspective view.  

(1) 

 

 

5)  Hole  Removal:  Due  to  the  uneven  displacement  of 
segmented regions of the image and occlusion of objects in the 
image, holes are typically generated in the process of generating 
the  stereo  pair  images.  Proper  removal  of  these  holes  is 
necessary  to  minimize  artifacts.  The  hole  removal  process  is 
described below: 

a) Detection: Convert the stereo image pairs to gray scale 
and  apply  Otsu’s  method  to  binerize  image  into  indicating 
region without any information.    

b) Filling:  Dilate  binary  hole  image  to  create  a  mask, 
which include surrounding pixel values around all hole regions. 
Use surrounding average pixel intensity values to fill in hole 
region values. 

B.  Method 2  

Digital Image Processing © 2015 Stanford University  -- Panoramic Imaging 16

Translating the camera is equivalent to

keeping the camera ﬁxed and translating the world

in opposite direction

 

The 

into  𝐿𝑠,𝑡,𝑢,𝑣  
representation,  where  coordinates (𝑠,𝑡)  correspond  to  each 
microlens in the array and the coordinates (𝑢,𝑣) correspond to 

raw  2D  data 

is  converted 

image aperture [4]. The resolution of the raw 2D light-field is 
about 40-megapixels. For each microlens, the diameter is about 
14 pixels. Dividing the raw image resolution by the size of the 
microlens, the effective resolution of the light-field images is 
about 0.2-megapixels. Note that Lytro Desktop exports images 
with 4-megapixel resolution, which indicated that Lytro uses an 
algorithm to enhance the resolution of its images. 

2)  Extraction:  Extract  the  left  most  and  right  most 
perspective  view  images  from  4D  light-field  data.  The 
coordinates  (u,v)  effectively  describe  viewpoint.  Figure  3 
shows a simplified diagram of corresponding perspective views 
extracted from the raw 2D light-field microlens.  

Fig. 3.   Simplified diagram of a perspective view extraction from raw 2D light-
field image [4] 

Robert Collins
CSE486, Penn State

 

𝑢 

𝑣 

 

 

Since  the  effective  resolution  of  the  extracted  images  are 
0.2-megapixels,  the  images  are  resized  to  have  about  4-
megapixel resolution. 

3)  Generate  Anaglyph:  Use  left  and  right  extracted 

perspective views to generate anaglyph. 

Depth-field  information  can  be  computed  from  the  light-
field  image,  which  can  be  used  to  artifically  generate  stereo 
image pairs to generated an anaglyph. The general process is 
described below: 

1)  Calibration  and  Decoding:  Align,  color-correct,  and 
rectify raw 2D light-field image to convert the into standard 4D 
light-field data format. 

2)  Depth Map Estimation:  Depth  can  be  estimated  either 
from blurred images in a focal stack or from perspective view 
form  a  disparity  stack.  Jeon,  et  al.,  describe  an  effective 
algorithm  to  accurately  estimate  depth  map  from  light-field 
images based on disparity map [6]. The algorithm developed 
uses a cost-volume-based methodology.  

the  focused 

3)  Segmentation: Generating a stereo image pairs requires 
displacing 
to  depth-field 
information.  Displacing  each  individual  pixel  requires  sub-
pixel accuracy to avoid possible artifacts. To minimize artifacts, 
the depth map is optimally segmented.  

image  according 

Fig. 7. Intermetiate results of second image processing algorithm applied to an 
example light-field image 

1)  Calibration and Decoding… 
2)  Depth Map Estimation… 
 

   

3)  Segmentation and Refinement:  

 

 

 

 

 

 

6)  Anaglyph  Generation:  Use  left  and  right  generated 

perspective views to generate anaglyph. 

III.  RESULTS 

A.  Method 1 

The light-field images acquired from a Lytro Illum camera 
were  calibrated  and  decoded  using  Light  Field  Toolbox  for 
Matlab developed by Donald G. Dansereau. Fig. 5 shows the 
intermediate results in applying the image processing algorithm 
for an example light-field image. 
Fig. 5.   Intermetiate results of first image processing algorithm applied to an 
example light-field image 

1)  Calibration and Decoding… 
2)  Extraction: 

 

3)  Generate Anaglyphs  

  

 

4)  Displacement: 

 

 

 
Fig.  6  shows  the  final  results  of  generating  anaglyphs  for 
some of the accessible light-field images. Note that the color-
correction  applied  didn’t  properly  balance  the  colors  in  the 
images.  The  applied  color-correction  was  an  shades-of-gray 
technique,  but  a  more  robust  color-correction  technique  is 
needed to generate results similar to those generated by Lytro 
Desktop.  
Fig. 6.   Results of first automated image processing algorithm method  

5)  Fill Holes: 

a) Detection: 

b) Filling: 

   

 

 
B.  Method 2 

Unfortunately, an accurate depth map was unable to be 
computed from the light-field images using the described 
methodology. More rudimentary attempts of computing depth 
maps were attempted, but the results were unsatisfactory. The 
anaglyphs results shown below were generated using the depth 
map and focused image exported from Lytro Desktop. Fig. 7 
show intermediate result of the image processing algorithm for 
one light-field image.  

6)  Generate Anaglyph:  

   

 

   

 

   

 

   

 

 

 
Fig. 8 shows the final results of generating  anaglyphs for all 
the accesibale light-field images. Using 3D red/cyan glasses, the 
results can be seen clearly.  
Fig. 8.   Results of second automated image processing algorithm method  

Fig. 9.  Comparison of results (left) with Lytro Desktop (right) 

   

   

 

 

 

IV.  DISCUSSION  

The two automated image processing algorithms developed 
for  generating  anaglyphs  produce  desirable  results.  The 
disparity in the stereo image pairs obtain from the first method 
were  expected  to  be  too  small  to  a  generated  an  anaglyph; 
however, the observed disparity is sufficient enough to observe 
a noticeable difference and produce desirable results. While the 
first  method  provides  the  simplest  method  for  generating 
anaglyphs  note  that  there  is  degradation  in  image  quality 
applying this method.  The microlens has non-uniform effects 
which greatly degrade the extracted light-field images from the 
border sensor pixels relative to the microlens. 

While  the  second  automated  image  processing  algorithm 
method  produced  desirable  results,  it  is  very  dependent  on 
obtain an accurate depth map estimation. An inaccurate or noisy 
depth  map  estimation  would  lead  to  lots  of  artifacts  and 
discrepancies in the generated anaglyph. Generating both stereo 
image pairs rather than one relative to the other greatly helped 
minimize artifacts that would be otherwise amplified by the hole 
filling process. 
A.  Comparison  
  Lytro Desktop offers the option to export anaglyphs from 
imported light-field images. Fig. 9 shows a comparison 
between the anaglyphs generated from the second automated 
image processing algorithm method vs. those obtain from 
Lytro Desktop.  
 
 
 
 
 

     
 

     
 

     
 

     

 

 

 

 

 

 

     
 

  Beside  the  color  filtering  difference  in  generating  the 
anaglyphs,  qualitatively  the  anaglyphs  obtain  from  the  Lytro 
Desktop are very similar to the results obtain from the second 
automatic image processing algorithm method, which indicate 
that  Lytro  might  use  a  similar  algorithm  to  generate  their 
anaglyphs. 
B.  Limitations 
  Another challenge besides obtaining an accurate depth map 
estimation are the holes generated by displacing occlusions. The 
hole  filling  process  produced  desirable  results  with  very 
minimal  artifacts.  In  order  to  investigate  the  limitation  of  the 
image  processing  algorithm,  the  disparity  between  the  stereo 
image pairs were increased by a factor of 2, 4, 6, 8, 10, and 12 
the results of which for one light-field image are shown in Fig. 
10.  
 
 
 

Generating Anaglyphs from Light Field Images

Pablo A. Vasquez Guzman 

Department of Mechanical Engineering 

Stanford University 

Stanford, CA 

pabloavg@stanford.edu 

 
 

Abstract—Light-field  imaging  systems  have  received  a  lot  of 
attention recently, especially with the release of Lytro cameras for 
consumer application. Extensive research has been conducted in 
optimizing and developing applications for light-field images [1,2]. 
To investigate the potential use of light-field imaging systems as an 
experimental  analysis  tool,  an  automated  image  processing 
algorithm was developed to generate anaglyphs images from light-
field images acquired from a Lytro Illum camera. 

Keywords—Light-field Images; Anaglyphs; Stereo Image Pairs; 

Disparity 

I.  INTRODUCTION 

Conventional  cameras  capture  2D  images,  which  are 
projections of a 3D scene. Light-field imaging systems capture 
not  only  the  projection  but  also  the  directions  of  incoming 
lighting that project onto a sensor. Specifically, Lytro cameras 
consist  of  an  array  of  microlenses  placed  in  front  of  the 
photosensor  used  to  separate  the  light  rays  striking  each 
microlens, and to focus them on different sensors according to 
their directions as shown in Fig. 1. 
Fig. 1.   Simplified diagram of a light-field imaging system [3] 

 

The  acquired  light-field  allows  for  more  flexible  image 
manipulating.  Enough  information  in  captured  that  one  can 
refocus  images  after  acquisition,  as  well  as  shift  one’s 
viewpoint. 

Lytro  offers  an 

image  processing  application,  Lytro 
Desktop, which is a powerful post processing tool for light-field 
images.  One  available  option  is  exporting  anaglyphs  from 
imported  light-field  images;  however,  no  information  is 
available on how the anaglyphs are generated. In addition, no 
prior  work  is  available  regarding  generating  anaglyphs  from 
light-field images. 

The  standard  method  for  generating  anaglyphs  requires  a 
pair of images, which have been acquired at slightly different 
perspective views, called stereo image pairs. Normally, stereo 
image pairs are acquired using a pair of conventional cameras 
separated by a distance duplicating the spacing of human eyes. 
It is possible to create an anaglyph from just one image taken 
from a conventional camera, but that requires extensive work in 
a  graphics  editing  program  such  as  Photoshop  and  it  isn’t  an 
automated or accurate process. 

The  specific  objective  of  the  work  described  here  was  to 
develop  an  automated 
image  processing  algorithm  for 
generating anaglyphs using the rich information captured in a 
light-field image acquired from a Lytro Illum camera. 

II.  METHODS 

  Two automated image processing algorithms for generating 
anaglyphs were developed. The first method is based on direct 
perspective views, while the second method is based on depth-
field information. 

A.  Method 1 

Lytro uses a hexagonal arrangement of a microlens in front 
of  an  image  sensor  to  efficiently  capture  4D  light  field 
information on a two dimensional plane [2]. The sensor pixels 
behind  the  microlens  array  only  records  light  intensity. 
Directional information of light is recorded in the location of 
sensor pixels relative to the microlens array [4]. Stereo image 
pair can be obtained directly from the raw data to generated an 
anaglyph.  The general process is described below: 

 
1)  Calibration  and  Decoding:  Align,  color-correct,  and 
rectify raw 2D light-field image to convert the into standard 4D 
light-field data format as illustrated in Fig. 2.  
 
Fig. 2.  Raw 2D light-field image conversion to 4D light-field data format [5] 

𝑣 

𝑢 

𝑢⋅	𝑣 
𝑡 

𝑠 

 

Segmentation  of  the  depth  map  is  performed  using  an 

optimized multilevel Otsu’s method. For N+1 segments (𝑁=
1,2,...,20), the threshold values are computed and a metric is 
thresholds.  The  depth  map  is  then  segment  into 𝑁+1 
If  the  number  of  segments  is  large  (𝑁>10),  then  the 

segments  corresponding  with  metric  with 
effectiveness. 

recorded  to  measure  of  the  effectiveness  of  the  computed 

the  highest 

segmentation is refined. The greatest variability in depth values 
usually occurs in the near- and far-field regions, so first few and 
last few segments are combined to reduce variability and create 
a more uniform segmentation.  

4)  Displacement:  The  focused  image  is  segmented  based 
on the segmentation of the depth map with the average depth 
value for each segmented region computed. To obtain stereo 
pair  images  requires  generating  a  left  and  right  perspective 
view. Fig. 4 show how a typical stereo imaging system works  

A Simple Stereo System
A Simple Stereo System
Top Down View (XZ plane)

Fig. 4.   Geometric schematic explanation of simple stereo system [7]  

 

P=(X,Y,Z)
P=(X,Y,Z)

Left 
camera

xxl

Z?Z?

xxrr

Right
camera

O

TTxx

ff
xl = f.X / Z
Translated by a distance Tx along X axis
yl = f.Y / Z
(Tx is also called the stereo “baseline”)

The disparty, 𝑑, observed between the left and right view 
based on some point 𝑃 is inversly propotional to the depth.  
where 𝑓  is  the  focal  length, 𝑇7  is  the  distance  between  the 
views, and 𝑍 is the depth. Based on the computed depth values, 

𝑑=𝑓	⋅𝑇7/𝑍 

the image segments are displaced accordinly to generate a left 
and right perspective view.  

(1) 

 

 

5)  Hole  Removal:  Due  to  the  uneven  displacement  of 
segmented regions of the image and occlusion of objects in the 
image, holes are typically generated in the process of generating 
the  stereo  pair  images.  Proper  removal  of  these  holes  is 
necessary  to  minimize  artifacts.  The  hole  removal  process  is 
described below: 

a) Detection: Convert the stereo image pairs to gray scale 
and  apply  Otsu’s  method  to  binerize  image  into  indicating 
region without any information.    

b) Filling:  Dilate  binary  hole  image  to  create  a  mask, 
which include surrounding pixel values around all hole regions. 
Use surrounding average pixel intensity values to fill in hole 
region values. 

B.  Method 2  

Digital Image Processing © 2015 Stanford University  -- Panoramic Imaging 16

Translating the camera is equivalent to

keeping the camera ﬁxed and translating the world

in opposite direction

 

The 

into  𝐿𝑠,𝑡,𝑢,𝑣  
representation,  where  coordinates (𝑠,𝑡)  correspond  to  each 
microlens in the array and the coordinates (𝑢,𝑣) correspond to 

raw  2D  data 

is  converted 

image aperture [4]. The resolution of the raw 2D light-field is 
about 40-megapixels. For each microlens, the diameter is about 
14 pixels. Dividing the raw image resolution by the size of the 
microlens, the effective resolution of the light-field images is 
about 0.2-megapixels. Note that Lytro Desktop exports images 
with 4-megapixel resolution, which indicated that Lytro uses an 
algorithm to enhance the resolution of its images. 

2)  Extraction:  Extract  the  left  most  and  right  most 
perspective  view  images  from  4D  light-field  data.  The 
coordinates  (u,v)  effectively  describe  viewpoint.  Figure  3 
shows a simplified diagram of corresponding perspective views 
extracted from the raw 2D light-field microlens.  

Fig. 3.   Simplified diagram of a perspective view extraction from raw 2D light-
field image [4] 

Robert Collins
CSE486, Penn State

 

𝑢 

𝑣 

 

 

Since  the  effective  resolution  of  the  extracted  images  are 
0.2-megapixels,  the  images  are  resized  to  have  about  4-
megapixel resolution. 

3)  Generate  Anaglyph:  Use  left  and  right  extracted 

perspective views to generate anaglyph. 

Depth-field  information  can  be  computed  from  the  light-
field  image,  which  can  be  used  to  artifically  generate  stereo 
image pairs to generated an anaglyph. The general process is 
described below: 

1)  Calibration  and  Decoding:  Align,  color-correct,  and 
rectify raw 2D light-field image to convert the into standard 4D 
light-field data format. 

2)  Depth Map Estimation:  Depth  can  be  estimated  either 
from blurred images in a focal stack or from perspective view 
form  a  disparity  stack.  Jeon,  et  al.,  describe  an  effective 
algorithm  to  accurately  estimate  depth  map  from  light-field 
images based on disparity map [6]. The algorithm developed 
uses a cost-volume-based methodology.  

the  focused 

3)  Segmentation: Generating a stereo image pairs requires 
displacing 
to  depth-field 
information.  Displacing  each  individual  pixel  requires  sub-
pixel accuracy to avoid possible artifacts. To minimize artifacts, 
the depth map is optimally segmented.  

image  according 

Fig. 7. Intermetiate results of second image processing algorithm applied to an 
example light-field image 

1)  Calibration and Decoding… 
2)  Depth Map Estimation… 
 

   

3)  Segmentation and Refinement:  

 

 

 

 

 

 

6)  Anaglyph  Generation:  Use  left  and  right  generated 

perspective views to generate anaglyph. 

III.  RESULTS 

A.  Method 1 

The light-field images acquired from a Lytro Illum camera 
were  calibrated  and  decoded  using  Light  Field  Toolbox  for 
Matlab developed by Donald G. Dansereau. Fig. 5 shows the 
intermediate results in applying the image processing algorithm 
for an example light-field image. 
Fig. 5.   Intermetiate results of first image processing algorithm applied to an 
example light-field image 

1)  Calibration and Decoding… 
2)  Extraction: 

 

3)  Generate Anaglyphs  

  

 

4)  Displacement: 

 

 

 
Fig.  6  shows  the  final  results  of  generating  anaglyphs  for 
some of the accessible light-field images. Note that the color-
correction  applied  didn’t  properly  balance  the  colors  in  the 
images.  The  applied  color-correction  was  an  shades-of-gray 
technique,  but  a  more  robust  color-correction  technique  is 
needed to generate results similar to those generated by Lytro 
Desktop.  
Fig. 6.   Results of first automated image processing algorithm method  

5)  Fill Holes: 

a) Detection: 

b) Filling: 

   

 

 
B.  Method 2 

Unfortunately, an accurate depth map was unable to be 
computed from the light-field images using the described 
methodology. More rudimentary attempts of computing depth 
maps were attempted, but the results were unsatisfactory. The 
anaglyphs results shown below were generated using the depth 
map and focused image exported from Lytro Desktop. Fig. 7 
show intermediate result of the image processing algorithm for 
one light-field image.  

6)  Generate Anaglyph:  

   

 

   

 

   

 

   

 

 

 
Fig. 8 shows the final results of generating  anaglyphs for all 
the accesibale light-field images. Using 3D red/cyan glasses, the 
results can be seen clearly.  
Fig. 8.   Results of second automated image processing algorithm method  

Fig. 9.  Comparison of results (left) with Lytro Desktop (right) 

   

   

 

 

 

IV.  DISCUSSION  

The two automated image processing algorithms developed 
for  generating  anaglyphs  produce  desirable  results.  The 
disparity in the stereo image pairs obtain from the first method 
were  expected  to  be  too  small  to  a  generated  an  anaglyph; 
however, the observed disparity is sufficient enough to observe 
a noticeable difference and produce desirable results. While the 
first  method  provides  the  simplest  method  for  generating 
anaglyphs  note  that  there  is  degradation  in  image  quality 
applying this method.  The microlens has non-uniform effects 
which greatly degrade the extracted light-field images from the 
border sensor pixels relative to the microlens. 

While  the  second  automated  image  processing  algorithm 
method  produced  desirable  results,  it  is  very  dependent  on 
obtain an accurate depth map estimation. An inaccurate or noisy 
depth  map  estimation  would  lead  to  lots  of  artifacts  and 
discrepancies in the generated anaglyph. Generating both stereo 
image pairs rather than one relative to the other greatly helped 
minimize artifacts that would be otherwise amplified by the hole 
filling process. 
A.  Comparison  
  Lytro Desktop offers the option to export anaglyphs from 
imported light-field images. Fig. 9 shows a comparison 
between the anaglyphs generated from the second automated 
image processing algorithm method vs. those obtain from 
Lytro Desktop.  
 
 
 
 
 

     
 

     
 

     
 

     

 

 

 

 

 

 

     
 

  Beside  the  color  filtering  difference  in  generating  the 
anaglyphs,  qualitatively  the  anaglyphs  obtain  from  the  Lytro 
Desktop are very similar to the results obtain from the second 
automatic image processing algorithm method, which indicate 
that  Lytro  might  use  a  similar  algorithm  to  generate  their 
anaglyphs. 
B.  Limitations 
  Another challenge besides obtaining an accurate depth map 
estimation are the holes generated by displacing occlusions. The 
hole  filling  process  produced  desirable  results  with  very 
minimal  artifacts.  In  order  to  investigate  the  limitation  of  the 
image  processing  algorithm,  the  disparity  between  the  stereo 
image pairs were increased by a factor of 2, 4, 6, 8, 10, and 12 
the results of which for one light-field image are shown in Fig. 
10.  
 
 
 

Fig.  10.   Results from increasing disparity by factor of 2, 4, 6, 8 10 and 12 

𝑥2 

𝑥4 

𝑥6 

𝑥10 

   

   

   

𝑥8 

𝑥12 

 

 

 

  Artifacts become pronounced once the disparity is increased 
by a factor of 8, especially for objects in the image that span 
various  depth  fields.  The  holes  become  large  due  to  the 
increased displacement that they are no longer properly filled. A 
different  hole  filling  technique  can  be  implements  to  reduce 
artifacts for large displacements, but unnecessary in the interest 
of generating anaglyphs. 

V.  CONCLUSION 

The automated image processing algorithms developed are 
robust under different imaging conditions. The results obtain by 
applying  the  second  automated  image  processing  algorithm 
 

method  agrees  well  with  those  obtained  directly  from  Lytro 
Desktop. However, the results depend greatly on computing an 
accurate  depth  map  estimation.  Future  work  requires  the 
development of an accurate depth map estimator. 
  Even  though  acquired  light-field  images  allows  for  more 
flexible 
is 
significantly reduced at the cost of gaining angular resolution. 
The  use  for  light-field  imagining  systems  as  an  experimental 
analysis tool is currently limited due to the degradation of spatial 
resolution and sensitivity of depth map estimation as discovered 
from this project. 

image  manipulation, 

the  spatial 

resolution 

ACKNOWLEDGMENT 

The author would like to thank Professor Gordon Wetzstein 
for his guidence on this project and the teaching staff of EE368: 
Digital Image Processing for their contribution in the course. 

REFERENCES 

[1]  T.  Georgiev,  Z.  Yu,  A.  Lumsdaine,  and  S.  Goma,  “Lytro  camera 
technology: theory, algorithms, performance analysis,” In Proceedings of 
SPIE, Multimedia Content and Mobile Devices, 2013. 

[2]  D. Cho, M. Lee, S. Kim, and Y. Tai, “Modeling the Calibration Pipeline 
of the Lytro Camera for High Quality Light-Field Image Reconstruction”, 
International Conference on Computer Vision, Sydney, Australia, 2013. 
[3]  M. Hansen and E. Holk, “Depth map estimation for plenoptic images,” 

2011. 

[4]  H. Zhang, “3D Surface Reconstruction Based On Plenoptic Image,” M.S. 

thesis, ELCE Dept., Auburn Univ., Auburn, Alabama, 2015. 

[5]  D. Johnston, “Learning Depth in Light Field Images,” unpublished. 
[6]  H.  Jeon  et  al.,  “Accurate  Depth  Map  Estimation  from  a  Lenslet  Light 
Field Camera,”In Proceedings of International Conference on Computer 
Vision and Pattern Recognition, 2015. 

[7]  G. Wetzstein. EE 368. Class Lecture, Topic: “Panoramic Imaging – Part 
2:  Stereo  Panoramas.”  Stanford  University,  Stanford,  California,  Nov., 
2015. 

