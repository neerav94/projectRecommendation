SlideSearch: Slide-to-Lecture-Video search system on ClassX database

Mengyuan Yan

School of Engineering, Stanford University

Stanford, CA, 94305
mengyuan@stanford.edu

Abstract

A slide-to-lecture-video search system is implemented on
Android tablet. The search system gets video clips from
the SCPD database. Previous research on Fisher Vector
and Residual Enhanced Visual Vector are combined to gen-
erate compact index for each database image. An aug-
mented database of clean slide images is used to exploit
the temporal redundancy of videos. Original database of
video keyframes and augmented database of slides are com-
pared and experiments show using augmented database
can achieve signiﬁcant memory reduction and precision
increase.

1. Introduction

As the usage of computers and the internet penetrates
deeper into peoples’ lives, universities are following the
trend offering their courses online. Platforms like Cours-
era and Udacity are offering large amounts of courses from
universities as well as great tech companies. Stanford Uni-
versity also has its own online lecture video database called
ClassX. It is developed by Stanford Center for Professional
Development and it achieves lecture videos from classes in
the school of engineering. The problem in these databases is
that it is hard to search for the time when professor was talk-
ing about a particular concept, equation, etc. In this project
I developed a way to search for video chunks by taking a
photo of a slide using mobile devices. Given photo of a
slide, whether printed or displayed on a screen, the applica-
tion returns the URL and time stamp for the corresponding
video chunk in which the slide is shown.

The image-to-video search problem has been addressed
by Sivic and Zisserman [7] a decade ago. A video is rep-
resented by a sequence of keyframes at speciﬁc frame rate.
Therefore, image-to-video search is built on content based
image retrieval. Bag-of-Words model [6] and Vocabulary
Tree [4] are widely used for this task today. For large-scale
image retrieval, it is important to build compact index for
database images and reduce memory usage.
It is desir-

able to ﬁt the database indices in the RAM of a mobile
device, so that users can process queries on their own device
instead of sending their query to the server. This low-
ers the requirement for a strong server, reduces latency
due to network data transmission, and protects user pri-
vacy. Recent progress in this direction includes the Vec-
tor of Locally Aggregated Descriptors (VLAD) [3], the
Compressed Fisher Vector (CFV) [5], and the Residual
Enhanced Visual Vector (REVV) [2]. In this project, I com-
bined the thoughts in CFV and REVV to build compact
indices for database images.

An important feature for videos is their temporal redun-
dancy. Much progress is made in the direction to exploit
the temporal redundancy of videos to aggregate local fea-
tures and compress database ﬁles [1]. In the case of lecture
videos, an easy way to exploit the temporal redundancy is
to introduce an augmented database consisting of clean lec-
ture slide images converted from pdf or powerpoint ﬁles.
With the help of augmented database, the slide-to-lecture-
video search becomes a two-step process: match the photo
of a slide against the augmented database images, and link
the database image to its corresponding video and time
stamp with a preprocessed annotation ﬁle. This method
reduces the memory requirement by a signiﬁcant amount,
and boosts retrieval precision.

This report

In Section 3,

is organized as follows.

In Section 2,
I present the training of codebook and construction of
database indices, which combines the original CFV and
REVV papers [3, 5].
implementation of
the practical slide-to-lecture-video mobile search system is
introduced. Experimental results are shown in Section 4 to
characterize the performance of this search system. Sec-
tion 5 discusses some existing problems and future work to
address these problems. The project is concluded in Section
6.

2. Design of compact index for database image
Figure 1 shows the pipeline for generating a compact
index for query image and comparing it against database
indices. I will explain each block in detail in the following

1

SlideSearch: Slide-to-Lecture-Video search system on ClassX database

Mengyuan Yan

School of Engineering, Stanford University

Stanford, CA, 94305
mengyuan@stanford.edu

Abstract

A slide-to-lecture-video search system is implemented on
Android tablet. The search system gets video clips from
the SCPD database. Previous research on Fisher Vector
and Residual Enhanced Visual Vector are combined to gen-
erate compact index for each database image. An aug-
mented database of clean slide images is used to exploit
the temporal redundancy of videos. Original database of
video keyframes and augmented database of slides are com-
pared and experiments show using augmented database
can achieve signiﬁcant memory reduction and precision
increase.

1. Introduction

As the usage of computers and the internet penetrates
deeper into peoples’ lives, universities are following the
trend offering their courses online. Platforms like Cours-
era and Udacity are offering large amounts of courses from
universities as well as great tech companies. Stanford Uni-
versity also has its own online lecture video database called
ClassX. It is developed by Stanford Center for Professional
Development and it achieves lecture videos from classes in
the school of engineering. The problem in these databases is
that it is hard to search for the time when professor was talk-
ing about a particular concept, equation, etc. In this project
I developed a way to search for video chunks by taking a
photo of a slide using mobile devices. Given photo of a
slide, whether printed or displayed on a screen, the applica-
tion returns the URL and time stamp for the corresponding
video chunk in which the slide is shown.

The image-to-video search problem has been addressed
by Sivic and Zisserman [7] a decade ago. A video is rep-
resented by a sequence of keyframes at speciﬁc frame rate.
Therefore, image-to-video search is built on content based
image retrieval. Bag-of-Words model [6] and Vocabulary
Tree [4] are widely used for this task today. For large-scale
image retrieval, it is important to build compact index for
database images and reduce memory usage.
It is desir-

able to ﬁt the database indices in the RAM of a mobile
device, so that users can process queries on their own device
instead of sending their query to the server. This low-
ers the requirement for a strong server, reduces latency
due to network data transmission, and protects user pri-
vacy. Recent progress in this direction includes the Vec-
tor of Locally Aggregated Descriptors (VLAD) [3], the
Compressed Fisher Vector (CFV) [5], and the Residual
Enhanced Visual Vector (REVV) [2]. In this project, I com-
bined the thoughts in CFV and REVV to build compact
indices for database images.

An important feature for videos is their temporal redun-
dancy. Much progress is made in the direction to exploit
the temporal redundancy of videos to aggregate local fea-
tures and compress database ﬁles [1]. In the case of lecture
videos, an easy way to exploit the temporal redundancy is
to introduce an augmented database consisting of clean lec-
ture slide images converted from pdf or powerpoint ﬁles.
With the help of augmented database, the slide-to-lecture-
video search becomes a two-step process: match the photo
of a slide against the augmented database images, and link
the database image to its corresponding video and time
stamp with a preprocessed annotation ﬁle. This method
reduces the memory requirement by a signiﬁcant amount,
and boosts retrieval precision.

This report

In Section 3,

is organized as follows.

In Section 2,
I present the training of codebook and construction of
database indices, which combines the original CFV and
REVV papers [3, 5].
implementation of
the practical slide-to-lecture-video mobile search system is
introduced. Experimental results are shown in Section 4 to
characterize the performance of this search system. Sec-
tion 5 discusses some existing problems and future work to
address these problems. The project is concluded in Section
6.

2. Design of compact index for database image
Figure 1 shows the pipeline for generating a compact
index for query image and comparing it against database
indices. I will explain each block in detail in the following

1

Here the vector division (xt − µi)/σi is calculated ele-
mentwise. The aggregated vector for each visual word is
then concatenated and L2 normalized to form the index
for image. This aggregation discounts the contribution of
descriptors which are close to word centroids (with respect
to its variance) thus have small (xt − µi)/σi, or are soft
assigned with high probability to a visual word with large
weight wi. Both above cases indicate that the descriptor
is frequent in training dataset, therefore its contribution to
discriminating between images is small. This weighting is
similar to the tf-idf weighting in text retrieval, where fre-
quent words have small contributions towards representing
the document.

2.3. Power law

In order to reduce the inﬂuence of large values in the vec-
tor before L2 normalization, power law should be applied
elementwise to suppress large values. Power value α = 0.5
is used from heuristics.

2.4. Binarization

The index for

image constructed so far

is high-
dimensional and dense. It would consume much memory,
and computing L2 distance between indices would be slow.
Therefore, it is helpful to binarize the index elementwise to
1 or -1 according to their sign. As the Fisher Vector for each
visual word is 32 dimensional, it can be packed to a 32-bit
unsigned integer.

2.5. Calculate Weighted Correlation

To calculate the similarity between a pair of indices, we

calculate the correlation:

(cid:88)
(cid:84) Id

i⊂Iq

1

NqNd

C(Sq,i, Sd,i)

(3)

Here Sq,i and Sd,i are the binarized Fisher Vector at the
i th visual word for query image and database image,
respectively. C(Sq,i, Sd,i) = dP CA − 2H(Sq,i, Sd,i) and
H is the Hamming distance which can be quickly com-
puted with bitwise XOR and POPCNT. Iq and Id are the
sets of visual words that query image and database image

has visited, respectively. Nq = (cid:112)dP CA|Iq|, Nd =
(cid:112)dP CA|Id| are normalization factors. To make the cor-

relation more discriminatory between matching and non-
matching image pairs, We apply a weight to the correla-
tion for each visual word. The weight would highlight
large correlation, because larger correlation indicates higher
possibility of a match. We deﬁne the weight w(C) =
P (match|C), which can be calculated by training with
annotated matching and non-matching image pairs. The

Figure 1: ﬁgure 1. Pipeline for generating a compact index
for query image and compare it against database indices.

subsections.
2.1. Dimension Reduction

During codebook training, covariance matrix is calcu-
lated from all SIFT descriptors extracted from training
images. Each descriptor is then reduced to a 32 dimension
vector using Principle Component Analysis. When building
database image indices and query image index, the same
dimension reduction is applied to every descriptor. This
dimension reduction would speed up the training process,
and would help reduce the size of database image indices.
2.2. Word Residual Aggregation

p(x) = (cid:80)N

Let us denote the set of descriptors extracted from a
image as X = {xt, t = 1...T}. T is the number of
descriptors, with a typical value of a few hundreds or thou-
sands. We assume that the xt’s are i.i.d random vectors
generated from distribution p. The Fisher Kernel frame-
work assumes that p is a Gaussian mixture model (GMM):
i=1 wipi(x). Each Gaussian pi can be viewed
as a visual word and N is the vocabulary size. wi is the
mixture weight which reﬂects the percentage of descriptors
that are quantized to this visual word. µi is the mean vector
of the Gaussian pi, which can be viewed as the word cen-
troid. Σi is the covariance matrix of the Gaussian pi. It is
assumed that the covariance matrices are diagonal and we
denote by σ2
i the variance vector. The GMM p is trained
on a large number of images using Maximum Likelihood
Estimation (MLE).

After the GMM is obtained from training, we want to
build indices for database images. Each descriptor xt is ﬁrst
soft assigned to visual words. γt(i) is the probability that xt
belongs to Gaussian pi.

(cid:80)N

wipi(xt)
j=1 wjpj(xt)

γt(i) =

(1)

Word residuals xt − µi for one visual word pi are aggre-

gated in the following manner:

T(cid:88)

t=1

F X

i =

1√
wi

xt − µi

σi

γt(i)

(2)

2

SlideSearch: Slide-to-Lecture-Video search system on ClassX database

Mengyuan Yan

School of Engineering, Stanford University

Stanford, CA, 94305
mengyuan@stanford.edu

Abstract

A slide-to-lecture-video search system is implemented on
Android tablet. The search system gets video clips from
the SCPD database. Previous research on Fisher Vector
and Residual Enhanced Visual Vector are combined to gen-
erate compact index for each database image. An aug-
mented database of clean slide images is used to exploit
the temporal redundancy of videos. Original database of
video keyframes and augmented database of slides are com-
pared and experiments show using augmented database
can achieve signiﬁcant memory reduction and precision
increase.

1. Introduction

As the usage of computers and the internet penetrates
deeper into peoples’ lives, universities are following the
trend offering their courses online. Platforms like Cours-
era and Udacity are offering large amounts of courses from
universities as well as great tech companies. Stanford Uni-
versity also has its own online lecture video database called
ClassX. It is developed by Stanford Center for Professional
Development and it achieves lecture videos from classes in
the school of engineering. The problem in these databases is
that it is hard to search for the time when professor was talk-
ing about a particular concept, equation, etc. In this project
I developed a way to search for video chunks by taking a
photo of a slide using mobile devices. Given photo of a
slide, whether printed or displayed on a screen, the applica-
tion returns the URL and time stamp for the corresponding
video chunk in which the slide is shown.

The image-to-video search problem has been addressed
by Sivic and Zisserman [7] a decade ago. A video is rep-
resented by a sequence of keyframes at speciﬁc frame rate.
Therefore, image-to-video search is built on content based
image retrieval. Bag-of-Words model [6] and Vocabulary
Tree [4] are widely used for this task today. For large-scale
image retrieval, it is important to build compact index for
database images and reduce memory usage.
It is desir-

able to ﬁt the database indices in the RAM of a mobile
device, so that users can process queries on their own device
instead of sending their query to the server. This low-
ers the requirement for a strong server, reduces latency
due to network data transmission, and protects user pri-
vacy. Recent progress in this direction includes the Vec-
tor of Locally Aggregated Descriptors (VLAD) [3], the
Compressed Fisher Vector (CFV) [5], and the Residual
Enhanced Visual Vector (REVV) [2]. In this project, I com-
bined the thoughts in CFV and REVV to build compact
indices for database images.

An important feature for videos is their temporal redun-
dancy. Much progress is made in the direction to exploit
the temporal redundancy of videos to aggregate local fea-
tures and compress database ﬁles [1]. In the case of lecture
videos, an easy way to exploit the temporal redundancy is
to introduce an augmented database consisting of clean lec-
ture slide images converted from pdf or powerpoint ﬁles.
With the help of augmented database, the slide-to-lecture-
video search becomes a two-step process: match the photo
of a slide against the augmented database images, and link
the database image to its corresponding video and time
stamp with a preprocessed annotation ﬁle. This method
reduces the memory requirement by a signiﬁcant amount,
and boosts retrieval precision.

This report

In Section 3,

is organized as follows.

In Section 2,
I present the training of codebook and construction of
database indices, which combines the original CFV and
REVV papers [3, 5].
implementation of
the practical slide-to-lecture-video mobile search system is
introduced. Experimental results are shown in Section 4 to
characterize the performance of this search system. Sec-
tion 5 discusses some existing problems and future work to
address these problems. The project is concluded in Section
6.

2. Design of compact index for database image
Figure 1 shows the pipeline for generating a compact
index for query image and comparing it against database
indices. I will explain each block in detail in the following

1

Here the vector division (xt − µi)/σi is calculated ele-
mentwise. The aggregated vector for each visual word is
then concatenated and L2 normalized to form the index
for image. This aggregation discounts the contribution of
descriptors which are close to word centroids (with respect
to its variance) thus have small (xt − µi)/σi, or are soft
assigned with high probability to a visual word with large
weight wi. Both above cases indicate that the descriptor
is frequent in training dataset, therefore its contribution to
discriminating between images is small. This weighting is
similar to the tf-idf weighting in text retrieval, where fre-
quent words have small contributions towards representing
the document.

2.3. Power law

In order to reduce the inﬂuence of large values in the vec-
tor before L2 normalization, power law should be applied
elementwise to suppress large values. Power value α = 0.5
is used from heuristics.

2.4. Binarization

The index for

image constructed so far

is high-
dimensional and dense. It would consume much memory,
and computing L2 distance between indices would be slow.
Therefore, it is helpful to binarize the index elementwise to
1 or -1 according to their sign. As the Fisher Vector for each
visual word is 32 dimensional, it can be packed to a 32-bit
unsigned integer.

2.5. Calculate Weighted Correlation

To calculate the similarity between a pair of indices, we

calculate the correlation:

(cid:88)
(cid:84) Id

i⊂Iq

1

NqNd

C(Sq,i, Sd,i)

(3)

Here Sq,i and Sd,i are the binarized Fisher Vector at the
i th visual word for query image and database image,
respectively. C(Sq,i, Sd,i) = dP CA − 2H(Sq,i, Sd,i) and
H is the Hamming distance which can be quickly com-
puted with bitwise XOR and POPCNT. Iq and Id are the
sets of visual words that query image and database image

has visited, respectively. Nq = (cid:112)dP CA|Iq|, Nd =
(cid:112)dP CA|Id| are normalization factors. To make the cor-

relation more discriminatory between matching and non-
matching image pairs, We apply a weight to the correla-
tion for each visual word. The weight would highlight
large correlation, because larger correlation indicates higher
possibility of a match. We deﬁne the weight w(C) =
P (match|C), which can be calculated by training with
annotated matching and non-matching image pairs. The

Figure 1: ﬁgure 1. Pipeline for generating a compact index
for query image and compare it against database indices.

subsections.
2.1. Dimension Reduction

During codebook training, covariance matrix is calcu-
lated from all SIFT descriptors extracted from training
images. Each descriptor is then reduced to a 32 dimension
vector using Principle Component Analysis. When building
database image indices and query image index, the same
dimension reduction is applied to every descriptor. This
dimension reduction would speed up the training process,
and would help reduce the size of database image indices.
2.2. Word Residual Aggregation

p(x) = (cid:80)N

Let us denote the set of descriptors extracted from a
image as X = {xt, t = 1...T}. T is the number of
descriptors, with a typical value of a few hundreds or thou-
sands. We assume that the xt’s are i.i.d random vectors
generated from distribution p. The Fisher Kernel frame-
work assumes that p is a Gaussian mixture model (GMM):
i=1 wipi(x). Each Gaussian pi can be viewed
as a visual word and N is the vocabulary size. wi is the
mixture weight which reﬂects the percentage of descriptors
that are quantized to this visual word. µi is the mean vector
of the Gaussian pi, which can be viewed as the word cen-
troid. Σi is the covariance matrix of the Gaussian pi. It is
assumed that the covariance matrices are diagonal and we
denote by σ2
i the variance vector. The GMM p is trained
on a large number of images using Maximum Likelihood
Estimation (MLE).

After the GMM is obtained from training, we want to
build indices for database images. Each descriptor xt is ﬁrst
soft assigned to visual words. γt(i) is the probability that xt
belongs to Gaussian pi.

(cid:80)N

wipi(xt)
j=1 wjpj(xt)

γt(i) =

(1)

Word residuals xt − µi for one visual word pi are aggre-

gated in the following manner:

T(cid:88)

t=1

F X

i =

1√
wi

xt − µi

σi

γt(i)

(2)

2

Figure 2: Pipeline for the mobile slide-to-lecture-video
search system

similarity score changes to

(cid:88)
(cid:84) Id

i⊂Iq

1

NqNd

w(C(Sq,i, Sd,i))C(Sq,i, Sd,i)

(4)

Figure 3: Sample images from the training set.

3. Mobile search system implementation

Figure 2 shows the pipeline for the practical mobile
slide-to-lecture-video search system. The codebook train-
ing and database index generation are done on the server.
The matches from clean slides to video information (url
and time stamp) are manually annotated for 233 slides for
demonstration. It could also be obtained by pairwise match-
ing clean slides and video keyframes on the server given
enough preprocessing time. The trained codebook, database
index ﬁle, annotation ﬁle and all database image feature
ﬁles are downloaded to the tablet. The query is processed
on tablet, and the application would open a browser to show
the returned matching video.
3.1. Training

The training dataset consists of 7304 images. Some sam-
ple images from the training dataset is shown in ﬁgure 3.
They are natural images of buildings, people, natural view,
etc. 11642 matching pairs are annotated as well as 11642
non-matching pairs. The most stable 300 SIFT features are
extracted from each image to ensure that codebook train-
ing is not degraded by noisy features. 512 visual words are
trained on this dataset.
3.2. Database

Indices for 955 slide images are generated. The slides are
from CS155 class in spring quarter 2013-14. Some sample
images are shown in ﬁgure 4. Aside from text contents,
the slides also contain ﬁgures, ﬂowcharts and images. The
index ﬁle for 955 images is 2.4MB in size.

Figure 4: Sample images from the database.

3.3. On Device Processing

An Android application is developed and tested on
NVIDIA SHIELD tablet. Upon starting the application,
codebook data and database index ﬁle are loaded into mem-
ory. The openCV cameraview handler would manage and
render preview frames from the device’s camera. The user
could then move the tablet and ﬁll the cameraview with a
slide that he would like to search. When the user touches
the screen, openCV cameraview handler would pass the
preview frame from Java coded front-end to C++ coded
native functions. The native code then extract SIFT fea-
ture using the opencv2.4.9 library. After that, index for
the query image is generated and compared to database
indices to return a ranked list of candidate matches. Top
ten matching database images are then compared to the
query image using RANSAC. L1 norm is used instead
of L2 norm to match SIFT features in order to speed up
computation. Related functions are from NVIDIA tegra-
optimized opencv2.4.8 library. Finally, the database image
with the highest number of inliers is regarded as the correct

3

SlideSearch: Slide-to-Lecture-Video search system on ClassX database

Mengyuan Yan

School of Engineering, Stanford University

Stanford, CA, 94305
mengyuan@stanford.edu

Abstract

A slide-to-lecture-video search system is implemented on
Android tablet. The search system gets video clips from
the SCPD database. Previous research on Fisher Vector
and Residual Enhanced Visual Vector are combined to gen-
erate compact index for each database image. An aug-
mented database of clean slide images is used to exploit
the temporal redundancy of videos. Original database of
video keyframes and augmented database of slides are com-
pared and experiments show using augmented database
can achieve signiﬁcant memory reduction and precision
increase.

1. Introduction

As the usage of computers and the internet penetrates
deeper into peoples’ lives, universities are following the
trend offering their courses online. Platforms like Cours-
era and Udacity are offering large amounts of courses from
universities as well as great tech companies. Stanford Uni-
versity also has its own online lecture video database called
ClassX. It is developed by Stanford Center for Professional
Development and it achieves lecture videos from classes in
the school of engineering. The problem in these databases is
that it is hard to search for the time when professor was talk-
ing about a particular concept, equation, etc. In this project
I developed a way to search for video chunks by taking a
photo of a slide using mobile devices. Given photo of a
slide, whether printed or displayed on a screen, the applica-
tion returns the URL and time stamp for the corresponding
video chunk in which the slide is shown.

The image-to-video search problem has been addressed
by Sivic and Zisserman [7] a decade ago. A video is rep-
resented by a sequence of keyframes at speciﬁc frame rate.
Therefore, image-to-video search is built on content based
image retrieval. Bag-of-Words model [6] and Vocabulary
Tree [4] are widely used for this task today. For large-scale
image retrieval, it is important to build compact index for
database images and reduce memory usage.
It is desir-

able to ﬁt the database indices in the RAM of a mobile
device, so that users can process queries on their own device
instead of sending their query to the server. This low-
ers the requirement for a strong server, reduces latency
due to network data transmission, and protects user pri-
vacy. Recent progress in this direction includes the Vec-
tor of Locally Aggregated Descriptors (VLAD) [3], the
Compressed Fisher Vector (CFV) [5], and the Residual
Enhanced Visual Vector (REVV) [2]. In this project, I com-
bined the thoughts in CFV and REVV to build compact
indices for database images.

An important feature for videos is their temporal redun-
dancy. Much progress is made in the direction to exploit
the temporal redundancy of videos to aggregate local fea-
tures and compress database ﬁles [1]. In the case of lecture
videos, an easy way to exploit the temporal redundancy is
to introduce an augmented database consisting of clean lec-
ture slide images converted from pdf or powerpoint ﬁles.
With the help of augmented database, the slide-to-lecture-
video search becomes a two-step process: match the photo
of a slide against the augmented database images, and link
the database image to its corresponding video and time
stamp with a preprocessed annotation ﬁle. This method
reduces the memory requirement by a signiﬁcant amount,
and boosts retrieval precision.

This report

In Section 3,

is organized as follows.

In Section 2,
I present the training of codebook and construction of
database indices, which combines the original CFV and
REVV papers [3, 5].
implementation of
the practical slide-to-lecture-video mobile search system is
introduced. Experimental results are shown in Section 4 to
characterize the performance of this search system. Sec-
tion 5 discusses some existing problems and future work to
address these problems. The project is concluded in Section
6.

2. Design of compact index for database image
Figure 1 shows the pipeline for generating a compact
index for query image and comparing it against database
indices. I will explain each block in detail in the following

1

Here the vector division (xt − µi)/σi is calculated ele-
mentwise. The aggregated vector for each visual word is
then concatenated and L2 normalized to form the index
for image. This aggregation discounts the contribution of
descriptors which are close to word centroids (with respect
to its variance) thus have small (xt − µi)/σi, or are soft
assigned with high probability to a visual word with large
weight wi. Both above cases indicate that the descriptor
is frequent in training dataset, therefore its contribution to
discriminating between images is small. This weighting is
similar to the tf-idf weighting in text retrieval, where fre-
quent words have small contributions towards representing
the document.

2.3. Power law

In order to reduce the inﬂuence of large values in the vec-
tor before L2 normalization, power law should be applied
elementwise to suppress large values. Power value α = 0.5
is used from heuristics.

2.4. Binarization

The index for

image constructed so far

is high-
dimensional and dense. It would consume much memory,
and computing L2 distance between indices would be slow.
Therefore, it is helpful to binarize the index elementwise to
1 or -1 according to their sign. As the Fisher Vector for each
visual word is 32 dimensional, it can be packed to a 32-bit
unsigned integer.

2.5. Calculate Weighted Correlation

To calculate the similarity between a pair of indices, we

calculate the correlation:

(cid:88)
(cid:84) Id

i⊂Iq

1

NqNd

C(Sq,i, Sd,i)

(3)

Here Sq,i and Sd,i are the binarized Fisher Vector at the
i th visual word for query image and database image,
respectively. C(Sq,i, Sd,i) = dP CA − 2H(Sq,i, Sd,i) and
H is the Hamming distance which can be quickly com-
puted with bitwise XOR and POPCNT. Iq and Id are the
sets of visual words that query image and database image

has visited, respectively. Nq = (cid:112)dP CA|Iq|, Nd =
(cid:112)dP CA|Id| are normalization factors. To make the cor-

relation more discriminatory between matching and non-
matching image pairs, We apply a weight to the correla-
tion for each visual word. The weight would highlight
large correlation, because larger correlation indicates higher
possibility of a match. We deﬁne the weight w(C) =
P (match|C), which can be calculated by training with
annotated matching and non-matching image pairs. The

Figure 1: ﬁgure 1. Pipeline for generating a compact index
for query image and compare it against database indices.

subsections.
2.1. Dimension Reduction

During codebook training, covariance matrix is calcu-
lated from all SIFT descriptors extracted from training
images. Each descriptor is then reduced to a 32 dimension
vector using Principle Component Analysis. When building
database image indices and query image index, the same
dimension reduction is applied to every descriptor. This
dimension reduction would speed up the training process,
and would help reduce the size of database image indices.
2.2. Word Residual Aggregation

p(x) = (cid:80)N

Let us denote the set of descriptors extracted from a
image as X = {xt, t = 1...T}. T is the number of
descriptors, with a typical value of a few hundreds or thou-
sands. We assume that the xt’s are i.i.d random vectors
generated from distribution p. The Fisher Kernel frame-
work assumes that p is a Gaussian mixture model (GMM):
i=1 wipi(x). Each Gaussian pi can be viewed
as a visual word and N is the vocabulary size. wi is the
mixture weight which reﬂects the percentage of descriptors
that are quantized to this visual word. µi is the mean vector
of the Gaussian pi, which can be viewed as the word cen-
troid. Σi is the covariance matrix of the Gaussian pi. It is
assumed that the covariance matrices are diagonal and we
denote by σ2
i the variance vector. The GMM p is trained
on a large number of images using Maximum Likelihood
Estimation (MLE).

After the GMM is obtained from training, we want to
build indices for database images. Each descriptor xt is ﬁrst
soft assigned to visual words. γt(i) is the probability that xt
belongs to Gaussian pi.

(cid:80)N

wipi(xt)
j=1 wjpj(xt)

γt(i) =

(1)

Word residuals xt − µi for one visual word pi are aggre-

gated in the following manner:

T(cid:88)

t=1

F X

i =

1√
wi

xt − µi

σi

γt(i)

(2)

2

Figure 2: Pipeline for the mobile slide-to-lecture-video
search system

similarity score changes to

(cid:88)
(cid:84) Id

i⊂Iq

1

NqNd

w(C(Sq,i, Sd,i))C(Sq,i, Sd,i)

(4)

Figure 3: Sample images from the training set.

3. Mobile search system implementation

Figure 2 shows the pipeline for the practical mobile
slide-to-lecture-video search system. The codebook train-
ing and database index generation are done on the server.
The matches from clean slides to video information (url
and time stamp) are manually annotated for 233 slides for
demonstration. It could also be obtained by pairwise match-
ing clean slides and video keyframes on the server given
enough preprocessing time. The trained codebook, database
index ﬁle, annotation ﬁle and all database image feature
ﬁles are downloaded to the tablet. The query is processed
on tablet, and the application would open a browser to show
the returned matching video.
3.1. Training

The training dataset consists of 7304 images. Some sam-
ple images from the training dataset is shown in ﬁgure 3.
They are natural images of buildings, people, natural view,
etc. 11642 matching pairs are annotated as well as 11642
non-matching pairs. The most stable 300 SIFT features are
extracted from each image to ensure that codebook train-
ing is not degraded by noisy features. 512 visual words are
trained on this dataset.
3.2. Database

Indices for 955 slide images are generated. The slides are
from CS155 class in spring quarter 2013-14. Some sample
images are shown in ﬁgure 4. Aside from text contents,
the slides also contain ﬁgures, ﬂowcharts and images. The
index ﬁle for 955 images is 2.4MB in size.

Figure 4: Sample images from the database.

3.3. On Device Processing

An Android application is developed and tested on
NVIDIA SHIELD tablet. Upon starting the application,
codebook data and database index ﬁle are loaded into mem-
ory. The openCV cameraview handler would manage and
render preview frames from the device’s camera. The user
could then move the tablet and ﬁll the cameraview with a
slide that he would like to search. When the user touches
the screen, openCV cameraview handler would pass the
preview frame from Java coded front-end to C++ coded
native functions. The native code then extract SIFT fea-
ture using the opencv2.4.9 library. After that, index for
the query image is generated and compared to database
indices to return a ranked list of candidate matches. Top
ten matching database images are then compared to the
query image using RANSAC. L1 norm is used instead
of L2 norm to match SIFT features in order to speed up
computation. Related functions are from NVIDIA tegra-
optimized opencv2.4.8 library. Finally, the database image
with the highest number of inliers is regarded as the correct

3

match, and its corresponding video URL and time stamp is
retrieved from the annotation ﬁle. These information are
returned back to the Java front-end. Since I haven’t ﬁgure
out a way to control video player embedded in the webpage,
I choose to show the video time stamp in a dialog popped
from the application, and then open the web browser to
show the matching video clip. The user can move the time
bar to the time indicated by previous dialog.

4. Experimental Results
4.1. Effectiveness of Augmented Database

In this section, we compare the retrieval performance of
searching a photo of a slide against the original database
consisting of video keyframes, versus searching against the
augmented database consisting of clean slide images. The
query set is made up of 98 photos of slides, half of them
are taken for printed slides and the other half are taken for
slides shown on computer screen. Some photos have rota-
tion and perspective changes, but they don’t contain back-
ground clutter. A codebook of 512 visual words is used for
both experiments. In the ﬁrst experiment, database images
are video keyframes extracted at 1 frame per second. The
database has a total of 88720 images. After comparing
query index with database indices, the top 300 keyframes
are passed to RANSAC, and reranked by the number of
inliers. The retrieval is successful if the top keyframe is
within the correct video clip. Precision at one in this case
is only 18%.
In the second experiment, database images
are 955 clean slide images. After comparing query index
with database indices, the top 4 slide images are passed
to RANSAC, and reranked by the number of inliers. The
retrieval is successful if the top slide is the one shown in the
photo. We chose short list size 4 in order to have similar
ratio between short list size and database size. The preci-
sion at one is boosted to 90%. In addition, database index
ﬁle is only 2.4MB compared to the 227MB index ﬁle for
original database. It is a 100X memory saving.

4.2. Varying size of codebook

In this section, we compare the retrieval performance
with different sizes of codebooks. The query set is made
up of 98 photos of slides as mentioned in the previous sec-
tion. Codebook with 128,256,512, and 1024 visual words
are experimented. The augmented database of slide images
is used and precision at one is calculated for each case with
varying short list size. The result is shown in ﬁgure 5. From
this ﬁgure, it is obvious that as the short list size increase,
precision would increase. This is not surprising since we are
using geometric veriﬁcation to rerank candidate matches.
Because geometric veriﬁcation would almost certainly ﬁnd
the true match if it is within the short list, the possibil-
ity of ﬁnding the true match would increase with growing

4

Figure 5: Precision at one for varying number of visual
words and varying short list size.

short list. Comparing the retrieval performance for differ-
ent number of visual words, a larger vocabulary would lead
to higher precision, especially when the short list size is
small. However the performance tend to saturate when the
number of visual words increases from 512 to 1024. This
could be because the database is rather small. Considering
the fact that database index ﬁle size and search time would
grow linearly with the number of visual words, there is a
speed/time-precision trade off in choosing a proper code-
book size. Since our database is small, memory and speed
are not important problems for our mobile application, we
choose to use 512 visual words in our mobile search system.
4.3. Robustness under Background Clutter

I created another query set which has 30 images of slides
with various background clutter. Types of clutter include
other slides, other non-slide text, and non-text objects. Sam-
ples of query images can be found on the left side of ﬁgure
7. The system is tested on this dataset in the same way as
described in the previous section. Precision at one is cal-
culated for various vocabulary size and short list size. The
result is shown in ﬁgure 6.

Comparing ﬁgure 6 with ﬁgure 5,

the precision has
dropped signiﬁcantly. For short list smaller than 5 images,
the precision has dropped to below 30%. Analyzing the
result for different types of clutter, the syetem is more
robust to non-text clutter than text clutter, and there is no
obvious difference between slide text clutter and non-slide
text clutter. A possible reason for the precision decrease
is that descriptors extracted from clutter text get mixed up
with descriptors from our region of interest during residual
aggregation, and makes the index less like the index of orig-
inal slide. descriptors from non text clutter are less likely to
be assigned to the same visual word as descriptors from text
patches, thus would have less inﬂuence on the index. Some
trial into solving this problem of background clutter is dis-
cussed in section 5.

SlideSearch: Slide-to-Lecture-Video search system on ClassX database

Mengyuan Yan

School of Engineering, Stanford University

Stanford, CA, 94305
mengyuan@stanford.edu

Abstract

A slide-to-lecture-video search system is implemented on
Android tablet. The search system gets video clips from
the SCPD database. Previous research on Fisher Vector
and Residual Enhanced Visual Vector are combined to gen-
erate compact index for each database image. An aug-
mented database of clean slide images is used to exploit
the temporal redundancy of videos. Original database of
video keyframes and augmented database of slides are com-
pared and experiments show using augmented database
can achieve signiﬁcant memory reduction and precision
increase.

1. Introduction

As the usage of computers and the internet penetrates
deeper into peoples’ lives, universities are following the
trend offering their courses online. Platforms like Cours-
era and Udacity are offering large amounts of courses from
universities as well as great tech companies. Stanford Uni-
versity also has its own online lecture video database called
ClassX. It is developed by Stanford Center for Professional
Development and it achieves lecture videos from classes in
the school of engineering. The problem in these databases is
that it is hard to search for the time when professor was talk-
ing about a particular concept, equation, etc. In this project
I developed a way to search for video chunks by taking a
photo of a slide using mobile devices. Given photo of a
slide, whether printed or displayed on a screen, the applica-
tion returns the URL and time stamp for the corresponding
video chunk in which the slide is shown.

The image-to-video search problem has been addressed
by Sivic and Zisserman [7] a decade ago. A video is rep-
resented by a sequence of keyframes at speciﬁc frame rate.
Therefore, image-to-video search is built on content based
image retrieval. Bag-of-Words model [6] and Vocabulary
Tree [4] are widely used for this task today. For large-scale
image retrieval, it is important to build compact index for
database images and reduce memory usage.
It is desir-

able to ﬁt the database indices in the RAM of a mobile
device, so that users can process queries on their own device
instead of sending their query to the server. This low-
ers the requirement for a strong server, reduces latency
due to network data transmission, and protects user pri-
vacy. Recent progress in this direction includes the Vec-
tor of Locally Aggregated Descriptors (VLAD) [3], the
Compressed Fisher Vector (CFV) [5], and the Residual
Enhanced Visual Vector (REVV) [2]. In this project, I com-
bined the thoughts in CFV and REVV to build compact
indices for database images.

An important feature for videos is their temporal redun-
dancy. Much progress is made in the direction to exploit
the temporal redundancy of videos to aggregate local fea-
tures and compress database ﬁles [1]. In the case of lecture
videos, an easy way to exploit the temporal redundancy is
to introduce an augmented database consisting of clean lec-
ture slide images converted from pdf or powerpoint ﬁles.
With the help of augmented database, the slide-to-lecture-
video search becomes a two-step process: match the photo
of a slide against the augmented database images, and link
the database image to its corresponding video and time
stamp with a preprocessed annotation ﬁle. This method
reduces the memory requirement by a signiﬁcant amount,
and boosts retrieval precision.

This report

In Section 3,

is organized as follows.

In Section 2,
I present the training of codebook and construction of
database indices, which combines the original CFV and
REVV papers [3, 5].
implementation of
the practical slide-to-lecture-video mobile search system is
introduced. Experimental results are shown in Section 4 to
characterize the performance of this search system. Sec-
tion 5 discusses some existing problems and future work to
address these problems. The project is concluded in Section
6.

2. Design of compact index for database image
Figure 1 shows the pipeline for generating a compact
index for query image and comparing it against database
indices. I will explain each block in detail in the following

1

Here the vector division (xt − µi)/σi is calculated ele-
mentwise. The aggregated vector for each visual word is
then concatenated and L2 normalized to form the index
for image. This aggregation discounts the contribution of
descriptors which are close to word centroids (with respect
to its variance) thus have small (xt − µi)/σi, or are soft
assigned with high probability to a visual word with large
weight wi. Both above cases indicate that the descriptor
is frequent in training dataset, therefore its contribution to
discriminating between images is small. This weighting is
similar to the tf-idf weighting in text retrieval, where fre-
quent words have small contributions towards representing
the document.

2.3. Power law

In order to reduce the inﬂuence of large values in the vec-
tor before L2 normalization, power law should be applied
elementwise to suppress large values. Power value α = 0.5
is used from heuristics.

2.4. Binarization

The index for

image constructed so far

is high-
dimensional and dense. It would consume much memory,
and computing L2 distance between indices would be slow.
Therefore, it is helpful to binarize the index elementwise to
1 or -1 according to their sign. As the Fisher Vector for each
visual word is 32 dimensional, it can be packed to a 32-bit
unsigned integer.

2.5. Calculate Weighted Correlation

To calculate the similarity between a pair of indices, we

calculate the correlation:

(cid:88)
(cid:84) Id

i⊂Iq

1

NqNd

C(Sq,i, Sd,i)

(3)

Here Sq,i and Sd,i are the binarized Fisher Vector at the
i th visual word for query image and database image,
respectively. C(Sq,i, Sd,i) = dP CA − 2H(Sq,i, Sd,i) and
H is the Hamming distance which can be quickly com-
puted with bitwise XOR and POPCNT. Iq and Id are the
sets of visual words that query image and database image

has visited, respectively. Nq = (cid:112)dP CA|Iq|, Nd =
(cid:112)dP CA|Id| are normalization factors. To make the cor-

relation more discriminatory between matching and non-
matching image pairs, We apply a weight to the correla-
tion for each visual word. The weight would highlight
large correlation, because larger correlation indicates higher
possibility of a match. We deﬁne the weight w(C) =
P (match|C), which can be calculated by training with
annotated matching and non-matching image pairs. The

Figure 1: ﬁgure 1. Pipeline for generating a compact index
for query image and compare it against database indices.

subsections.
2.1. Dimension Reduction

During codebook training, covariance matrix is calcu-
lated from all SIFT descriptors extracted from training
images. Each descriptor is then reduced to a 32 dimension
vector using Principle Component Analysis. When building
database image indices and query image index, the same
dimension reduction is applied to every descriptor. This
dimension reduction would speed up the training process,
and would help reduce the size of database image indices.
2.2. Word Residual Aggregation

p(x) = (cid:80)N

Let us denote the set of descriptors extracted from a
image as X = {xt, t = 1...T}. T is the number of
descriptors, with a typical value of a few hundreds or thou-
sands. We assume that the xt’s are i.i.d random vectors
generated from distribution p. The Fisher Kernel frame-
work assumes that p is a Gaussian mixture model (GMM):
i=1 wipi(x). Each Gaussian pi can be viewed
as a visual word and N is the vocabulary size. wi is the
mixture weight which reﬂects the percentage of descriptors
that are quantized to this visual word. µi is the mean vector
of the Gaussian pi, which can be viewed as the word cen-
troid. Σi is the covariance matrix of the Gaussian pi. It is
assumed that the covariance matrices are diagonal and we
denote by σ2
i the variance vector. The GMM p is trained
on a large number of images using Maximum Likelihood
Estimation (MLE).

After the GMM is obtained from training, we want to
build indices for database images. Each descriptor xt is ﬁrst
soft assigned to visual words. γt(i) is the probability that xt
belongs to Gaussian pi.

(cid:80)N

wipi(xt)
j=1 wjpj(xt)

γt(i) =

(1)

Word residuals xt − µi for one visual word pi are aggre-

gated in the following manner:

T(cid:88)

t=1

F X

i =

1√
wi

xt − µi

σi

γt(i)

(2)

2

Figure 2: Pipeline for the mobile slide-to-lecture-video
search system

similarity score changes to

(cid:88)
(cid:84) Id

i⊂Iq

1

NqNd

w(C(Sq,i, Sd,i))C(Sq,i, Sd,i)

(4)

Figure 3: Sample images from the training set.

3. Mobile search system implementation

Figure 2 shows the pipeline for the practical mobile
slide-to-lecture-video search system. The codebook train-
ing and database index generation are done on the server.
The matches from clean slides to video information (url
and time stamp) are manually annotated for 233 slides for
demonstration. It could also be obtained by pairwise match-
ing clean slides and video keyframes on the server given
enough preprocessing time. The trained codebook, database
index ﬁle, annotation ﬁle and all database image feature
ﬁles are downloaded to the tablet. The query is processed
on tablet, and the application would open a browser to show
the returned matching video.
3.1. Training

The training dataset consists of 7304 images. Some sam-
ple images from the training dataset is shown in ﬁgure 3.
They are natural images of buildings, people, natural view,
etc. 11642 matching pairs are annotated as well as 11642
non-matching pairs. The most stable 300 SIFT features are
extracted from each image to ensure that codebook train-
ing is not degraded by noisy features. 512 visual words are
trained on this dataset.
3.2. Database

Indices for 955 slide images are generated. The slides are
from CS155 class in spring quarter 2013-14. Some sample
images are shown in ﬁgure 4. Aside from text contents,
the slides also contain ﬁgures, ﬂowcharts and images. The
index ﬁle for 955 images is 2.4MB in size.

Figure 4: Sample images from the database.

3.3. On Device Processing

An Android application is developed and tested on
NVIDIA SHIELD tablet. Upon starting the application,
codebook data and database index ﬁle are loaded into mem-
ory. The openCV cameraview handler would manage and
render preview frames from the device’s camera. The user
could then move the tablet and ﬁll the cameraview with a
slide that he would like to search. When the user touches
the screen, openCV cameraview handler would pass the
preview frame from Java coded front-end to C++ coded
native functions. The native code then extract SIFT fea-
ture using the opencv2.4.9 library. After that, index for
the query image is generated and compared to database
indices to return a ranked list of candidate matches. Top
ten matching database images are then compared to the
query image using RANSAC. L1 norm is used instead
of L2 norm to match SIFT features in order to speed up
computation. Related functions are from NVIDIA tegra-
optimized opencv2.4.8 library. Finally, the database image
with the highest number of inliers is regarded as the correct

3

match, and its corresponding video URL and time stamp is
retrieved from the annotation ﬁle. These information are
returned back to the Java front-end. Since I haven’t ﬁgure
out a way to control video player embedded in the webpage,
I choose to show the video time stamp in a dialog popped
from the application, and then open the web browser to
show the matching video clip. The user can move the time
bar to the time indicated by previous dialog.

4. Experimental Results
4.1. Effectiveness of Augmented Database

In this section, we compare the retrieval performance of
searching a photo of a slide against the original database
consisting of video keyframes, versus searching against the
augmented database consisting of clean slide images. The
query set is made up of 98 photos of slides, half of them
are taken for printed slides and the other half are taken for
slides shown on computer screen. Some photos have rota-
tion and perspective changes, but they don’t contain back-
ground clutter. A codebook of 512 visual words is used for
both experiments. In the ﬁrst experiment, database images
are video keyframes extracted at 1 frame per second. The
database has a total of 88720 images. After comparing
query index with database indices, the top 300 keyframes
are passed to RANSAC, and reranked by the number of
inliers. The retrieval is successful if the top keyframe is
within the correct video clip. Precision at one in this case
is only 18%.
In the second experiment, database images
are 955 clean slide images. After comparing query index
with database indices, the top 4 slide images are passed
to RANSAC, and reranked by the number of inliers. The
retrieval is successful if the top slide is the one shown in the
photo. We chose short list size 4 in order to have similar
ratio between short list size and database size. The preci-
sion at one is boosted to 90%. In addition, database index
ﬁle is only 2.4MB compared to the 227MB index ﬁle for
original database. It is a 100X memory saving.

4.2. Varying size of codebook

In this section, we compare the retrieval performance
with different sizes of codebooks. The query set is made
up of 98 photos of slides as mentioned in the previous sec-
tion. Codebook with 128,256,512, and 1024 visual words
are experimented. The augmented database of slide images
is used and precision at one is calculated for each case with
varying short list size. The result is shown in ﬁgure 5. From
this ﬁgure, it is obvious that as the short list size increase,
precision would increase. This is not surprising since we are
using geometric veriﬁcation to rerank candidate matches.
Because geometric veriﬁcation would almost certainly ﬁnd
the true match if it is within the short list, the possibil-
ity of ﬁnding the true match would increase with growing

4

Figure 5: Precision at one for varying number of visual
words and varying short list size.

short list. Comparing the retrieval performance for differ-
ent number of visual words, a larger vocabulary would lead
to higher precision, especially when the short list size is
small. However the performance tend to saturate when the
number of visual words increases from 512 to 1024. This
could be because the database is rather small. Considering
the fact that database index ﬁle size and search time would
grow linearly with the number of visual words, there is a
speed/time-precision trade off in choosing a proper code-
book size. Since our database is small, memory and speed
are not important problems for our mobile application, we
choose to use 512 visual words in our mobile search system.
4.3. Robustness under Background Clutter

I created another query set which has 30 images of slides
with various background clutter. Types of clutter include
other slides, other non-slide text, and non-text objects. Sam-
ples of query images can be found on the left side of ﬁgure
7. The system is tested on this dataset in the same way as
described in the previous section. Precision at one is cal-
culated for various vocabulary size and short list size. The
result is shown in ﬁgure 6.

Comparing ﬁgure 6 with ﬁgure 5,

the precision has
dropped signiﬁcantly. For short list smaller than 5 images,
the precision has dropped to below 30%. Analyzing the
result for different types of clutter, the syetem is more
robust to non-text clutter than text clutter, and there is no
obvious difference between slide text clutter and non-slide
text clutter. A possible reason for the precision decrease
is that descriptors extracted from clutter text get mixed up
with descriptors from our region of interest during residual
aggregation, and makes the index less like the index of orig-
inal slide. descriptors from non text clutter are less likely to
be assigned to the same visual word as descriptors from text
patches, thus would have less inﬂuence on the index. Some
trial into solving this problem of background clutter is dis-
cussed in section 5.

Figure 6: Precision at one for test images with background
clutter.

5. Discussion and Future Work

As mentioned in the previous section, the mobile slide-
to-lecture-video search system is not very robust for back-
ground clutter, especially text clutter. Investigation into this
direction is limited by the time of this project. As a ﬁrst
step, I am able to segment the printed paper or the com-
puter screen from other non-text background by detecting
white pixels in the input photo. The algorithm ﬁrst converts
color image into grayscale image by taking the minimum
of three color channel values. Taking the minimum puts
penalty on object that are bright but have hue. Then the
grayscale image is sent to detect MSER or to binarization
using threshold. Some results using threshold binarization
are shown in ﬁgure 7.

More processing is needed to segment the slide region
we want out of other texts. This can be done by ﬁrst ﬁnding
the direction of text lines by Hough transform and rectifying
the image, and then projecting the binary image to horizon-
tal and vertical direction to ﬁnd blank margins around the
slide. Due to limitation of time this is not implemented well
to achieve satisfying results.

Another possibility to improve the performance of slide-
to-lecture-video search is to train the codebook on similar
text images instead of natural images. In generating index
for images, descriptors that appear often in the training set
have lower weight and don’t contribute much towards the
index. Therefore, training the codebook on natural image
would highlight text regions in building index for database
images and query images. However, text patches are repet-
itive and not discriminatory enough, we may want to high-
light the ﬁgures and images which are more unique in iden-
tifying slides. Therefore, I should try to collect another
training dataset of text images to see if performance of the
search system can be improved.

Test is also done in taking photos of only part of a slide.
Preliminary results show that the system could retrieve the

:

:

:

Figure 7: Segmentation of paper or computer screen from
input photos.

right video when non-trivial amount of slide is shown (more
than two lines of text). Systematic test can be done to inves-
tigate the robustness and limitation in matching parts of a
slide to lecture videos.

6. Conclusion

In this project, I have combined the previous research on
Fisher Vectors and Residual Enhanced Visual Vector to gen-
erate compact indices for database images to achieve efﬁ-
cient slide-to-lecture-video search. In order to exploit the
temporal redundancy of videos, an augmented database of
clean slide images is used instead of the database of video
keyframes. Using augmented database has led to 100X
memory reduction and has boosted the retrieval precision
from 18% to 90%. An Android Application is implemented
and tested on NVIDIA SHIELD tablet.

7. Acknowledgement

Special thanks to Andre Araujo for helpful discussions
during this project and for sharing part of the code to build
compact index for images.
In addition, thanks should be
given to Huizhong Chen for helpful suggestions on seg-
menting slides and analyzing layouts.

5

SlideSearch: Slide-to-Lecture-Video search system on ClassX database

Mengyuan Yan

School of Engineering, Stanford University

Stanford, CA, 94305
mengyuan@stanford.edu

Abstract

A slide-to-lecture-video search system is implemented on
Android tablet. The search system gets video clips from
the SCPD database. Previous research on Fisher Vector
and Residual Enhanced Visual Vector are combined to gen-
erate compact index for each database image. An aug-
mented database of clean slide images is used to exploit
the temporal redundancy of videos. Original database of
video keyframes and augmented database of slides are com-
pared and experiments show using augmented database
can achieve signiﬁcant memory reduction and precision
increase.

1. Introduction

As the usage of computers and the internet penetrates
deeper into peoples’ lives, universities are following the
trend offering their courses online. Platforms like Cours-
era and Udacity are offering large amounts of courses from
universities as well as great tech companies. Stanford Uni-
versity also has its own online lecture video database called
ClassX. It is developed by Stanford Center for Professional
Development and it achieves lecture videos from classes in
the school of engineering. The problem in these databases is
that it is hard to search for the time when professor was talk-
ing about a particular concept, equation, etc. In this project
I developed a way to search for video chunks by taking a
photo of a slide using mobile devices. Given photo of a
slide, whether printed or displayed on a screen, the applica-
tion returns the URL and time stamp for the corresponding
video chunk in which the slide is shown.

The image-to-video search problem has been addressed
by Sivic and Zisserman [7] a decade ago. A video is rep-
resented by a sequence of keyframes at speciﬁc frame rate.
Therefore, image-to-video search is built on content based
image retrieval. Bag-of-Words model [6] and Vocabulary
Tree [4] are widely used for this task today. For large-scale
image retrieval, it is important to build compact index for
database images and reduce memory usage.
It is desir-

able to ﬁt the database indices in the RAM of a mobile
device, so that users can process queries on their own device
instead of sending their query to the server. This low-
ers the requirement for a strong server, reduces latency
due to network data transmission, and protects user pri-
vacy. Recent progress in this direction includes the Vec-
tor of Locally Aggregated Descriptors (VLAD) [3], the
Compressed Fisher Vector (CFV) [5], and the Residual
Enhanced Visual Vector (REVV) [2]. In this project, I com-
bined the thoughts in CFV and REVV to build compact
indices for database images.

An important feature for videos is their temporal redun-
dancy. Much progress is made in the direction to exploit
the temporal redundancy of videos to aggregate local fea-
tures and compress database ﬁles [1]. In the case of lecture
videos, an easy way to exploit the temporal redundancy is
to introduce an augmented database consisting of clean lec-
ture slide images converted from pdf or powerpoint ﬁles.
With the help of augmented database, the slide-to-lecture-
video search becomes a two-step process: match the photo
of a slide against the augmented database images, and link
the database image to its corresponding video and time
stamp with a preprocessed annotation ﬁle. This method
reduces the memory requirement by a signiﬁcant amount,
and boosts retrieval precision.

This report

In Section 3,

is organized as follows.

In Section 2,
I present the training of codebook and construction of
database indices, which combines the original CFV and
REVV papers [3, 5].
implementation of
the practical slide-to-lecture-video mobile search system is
introduced. Experimental results are shown in Section 4 to
characterize the performance of this search system. Sec-
tion 5 discusses some existing problems and future work to
address these problems. The project is concluded in Section
6.

2. Design of compact index for database image
Figure 1 shows the pipeline for generating a compact
index for query image and comparing it against database
indices. I will explain each block in detail in the following

1

Here the vector division (xt − µi)/σi is calculated ele-
mentwise. The aggregated vector for each visual word is
then concatenated and L2 normalized to form the index
for image. This aggregation discounts the contribution of
descriptors which are close to word centroids (with respect
to its variance) thus have small (xt − µi)/σi, or are soft
assigned with high probability to a visual word with large
weight wi. Both above cases indicate that the descriptor
is frequent in training dataset, therefore its contribution to
discriminating between images is small. This weighting is
similar to the tf-idf weighting in text retrieval, where fre-
quent words have small contributions towards representing
the document.

2.3. Power law

In order to reduce the inﬂuence of large values in the vec-
tor before L2 normalization, power law should be applied
elementwise to suppress large values. Power value α = 0.5
is used from heuristics.

2.4. Binarization

The index for

image constructed so far

is high-
dimensional and dense. It would consume much memory,
and computing L2 distance between indices would be slow.
Therefore, it is helpful to binarize the index elementwise to
1 or -1 according to their sign. As the Fisher Vector for each
visual word is 32 dimensional, it can be packed to a 32-bit
unsigned integer.

2.5. Calculate Weighted Correlation

To calculate the similarity between a pair of indices, we

calculate the correlation:

(cid:88)
(cid:84) Id

i⊂Iq

1

NqNd

C(Sq,i, Sd,i)

(3)

Here Sq,i and Sd,i are the binarized Fisher Vector at the
i th visual word for query image and database image,
respectively. C(Sq,i, Sd,i) = dP CA − 2H(Sq,i, Sd,i) and
H is the Hamming distance which can be quickly com-
puted with bitwise XOR and POPCNT. Iq and Id are the
sets of visual words that query image and database image

has visited, respectively. Nq = (cid:112)dP CA|Iq|, Nd =
(cid:112)dP CA|Id| are normalization factors. To make the cor-

relation more discriminatory between matching and non-
matching image pairs, We apply a weight to the correla-
tion for each visual word. The weight would highlight
large correlation, because larger correlation indicates higher
possibility of a match. We deﬁne the weight w(C) =
P (match|C), which can be calculated by training with
annotated matching and non-matching image pairs. The

Figure 1: ﬁgure 1. Pipeline for generating a compact index
for query image and compare it against database indices.

subsections.
2.1. Dimension Reduction

During codebook training, covariance matrix is calcu-
lated from all SIFT descriptors extracted from training
images. Each descriptor is then reduced to a 32 dimension
vector using Principle Component Analysis. When building
database image indices and query image index, the same
dimension reduction is applied to every descriptor. This
dimension reduction would speed up the training process,
and would help reduce the size of database image indices.
2.2. Word Residual Aggregation

p(x) = (cid:80)N

Let us denote the set of descriptors extracted from a
image as X = {xt, t = 1...T}. T is the number of
descriptors, with a typical value of a few hundreds or thou-
sands. We assume that the xt’s are i.i.d random vectors
generated from distribution p. The Fisher Kernel frame-
work assumes that p is a Gaussian mixture model (GMM):
i=1 wipi(x). Each Gaussian pi can be viewed
as a visual word and N is the vocabulary size. wi is the
mixture weight which reﬂects the percentage of descriptors
that are quantized to this visual word. µi is the mean vector
of the Gaussian pi, which can be viewed as the word cen-
troid. Σi is the covariance matrix of the Gaussian pi. It is
assumed that the covariance matrices are diagonal and we
denote by σ2
i the variance vector. The GMM p is trained
on a large number of images using Maximum Likelihood
Estimation (MLE).

After the GMM is obtained from training, we want to
build indices for database images. Each descriptor xt is ﬁrst
soft assigned to visual words. γt(i) is the probability that xt
belongs to Gaussian pi.

(cid:80)N

wipi(xt)
j=1 wjpj(xt)

γt(i) =

(1)

Word residuals xt − µi for one visual word pi are aggre-

gated in the following manner:

T(cid:88)

t=1

F X

i =

1√
wi

xt − µi

σi

γt(i)

(2)

2

Figure 2: Pipeline for the mobile slide-to-lecture-video
search system

similarity score changes to

(cid:88)
(cid:84) Id

i⊂Iq

1

NqNd

w(C(Sq,i, Sd,i))C(Sq,i, Sd,i)

(4)

Figure 3: Sample images from the training set.

3. Mobile search system implementation

Figure 2 shows the pipeline for the practical mobile
slide-to-lecture-video search system. The codebook train-
ing and database index generation are done on the server.
The matches from clean slides to video information (url
and time stamp) are manually annotated for 233 slides for
demonstration. It could also be obtained by pairwise match-
ing clean slides and video keyframes on the server given
enough preprocessing time. The trained codebook, database
index ﬁle, annotation ﬁle and all database image feature
ﬁles are downloaded to the tablet. The query is processed
on tablet, and the application would open a browser to show
the returned matching video.
3.1. Training

The training dataset consists of 7304 images. Some sam-
ple images from the training dataset is shown in ﬁgure 3.
They are natural images of buildings, people, natural view,
etc. 11642 matching pairs are annotated as well as 11642
non-matching pairs. The most stable 300 SIFT features are
extracted from each image to ensure that codebook train-
ing is not degraded by noisy features. 512 visual words are
trained on this dataset.
3.2. Database

Indices for 955 slide images are generated. The slides are
from CS155 class in spring quarter 2013-14. Some sample
images are shown in ﬁgure 4. Aside from text contents,
the slides also contain ﬁgures, ﬂowcharts and images. The
index ﬁle for 955 images is 2.4MB in size.

Figure 4: Sample images from the database.

3.3. On Device Processing

An Android application is developed and tested on
NVIDIA SHIELD tablet. Upon starting the application,
codebook data and database index ﬁle are loaded into mem-
ory. The openCV cameraview handler would manage and
render preview frames from the device’s camera. The user
could then move the tablet and ﬁll the cameraview with a
slide that he would like to search. When the user touches
the screen, openCV cameraview handler would pass the
preview frame from Java coded front-end to C++ coded
native functions. The native code then extract SIFT fea-
ture using the opencv2.4.9 library. After that, index for
the query image is generated and compared to database
indices to return a ranked list of candidate matches. Top
ten matching database images are then compared to the
query image using RANSAC. L1 norm is used instead
of L2 norm to match SIFT features in order to speed up
computation. Related functions are from NVIDIA tegra-
optimized opencv2.4.8 library. Finally, the database image
with the highest number of inliers is regarded as the correct

3

match, and its corresponding video URL and time stamp is
retrieved from the annotation ﬁle. These information are
returned back to the Java front-end. Since I haven’t ﬁgure
out a way to control video player embedded in the webpage,
I choose to show the video time stamp in a dialog popped
from the application, and then open the web browser to
show the matching video clip. The user can move the time
bar to the time indicated by previous dialog.

4. Experimental Results
4.1. Effectiveness of Augmented Database

In this section, we compare the retrieval performance of
searching a photo of a slide against the original database
consisting of video keyframes, versus searching against the
augmented database consisting of clean slide images. The
query set is made up of 98 photos of slides, half of them
are taken for printed slides and the other half are taken for
slides shown on computer screen. Some photos have rota-
tion and perspective changes, but they don’t contain back-
ground clutter. A codebook of 512 visual words is used for
both experiments. In the ﬁrst experiment, database images
are video keyframes extracted at 1 frame per second. The
database has a total of 88720 images. After comparing
query index with database indices, the top 300 keyframes
are passed to RANSAC, and reranked by the number of
inliers. The retrieval is successful if the top keyframe is
within the correct video clip. Precision at one in this case
is only 18%.
In the second experiment, database images
are 955 clean slide images. After comparing query index
with database indices, the top 4 slide images are passed
to RANSAC, and reranked by the number of inliers. The
retrieval is successful if the top slide is the one shown in the
photo. We chose short list size 4 in order to have similar
ratio between short list size and database size. The preci-
sion at one is boosted to 90%. In addition, database index
ﬁle is only 2.4MB compared to the 227MB index ﬁle for
original database. It is a 100X memory saving.

4.2. Varying size of codebook

In this section, we compare the retrieval performance
with different sizes of codebooks. The query set is made
up of 98 photos of slides as mentioned in the previous sec-
tion. Codebook with 128,256,512, and 1024 visual words
are experimented. The augmented database of slide images
is used and precision at one is calculated for each case with
varying short list size. The result is shown in ﬁgure 5. From
this ﬁgure, it is obvious that as the short list size increase,
precision would increase. This is not surprising since we are
using geometric veriﬁcation to rerank candidate matches.
Because geometric veriﬁcation would almost certainly ﬁnd
the true match if it is within the short list, the possibil-
ity of ﬁnding the true match would increase with growing

4

Figure 5: Precision at one for varying number of visual
words and varying short list size.

short list. Comparing the retrieval performance for differ-
ent number of visual words, a larger vocabulary would lead
to higher precision, especially when the short list size is
small. However the performance tend to saturate when the
number of visual words increases from 512 to 1024. This
could be because the database is rather small. Considering
the fact that database index ﬁle size and search time would
grow linearly with the number of visual words, there is a
speed/time-precision trade off in choosing a proper code-
book size. Since our database is small, memory and speed
are not important problems for our mobile application, we
choose to use 512 visual words in our mobile search system.
4.3. Robustness under Background Clutter

I created another query set which has 30 images of slides
with various background clutter. Types of clutter include
other slides, other non-slide text, and non-text objects. Sam-
ples of query images can be found on the left side of ﬁgure
7. The system is tested on this dataset in the same way as
described in the previous section. Precision at one is cal-
culated for various vocabulary size and short list size. The
result is shown in ﬁgure 6.

Comparing ﬁgure 6 with ﬁgure 5,

the precision has
dropped signiﬁcantly. For short list smaller than 5 images,
the precision has dropped to below 30%. Analyzing the
result for different types of clutter, the syetem is more
robust to non-text clutter than text clutter, and there is no
obvious difference between slide text clutter and non-slide
text clutter. A possible reason for the precision decrease
is that descriptors extracted from clutter text get mixed up
with descriptors from our region of interest during residual
aggregation, and makes the index less like the index of orig-
inal slide. descriptors from non text clutter are less likely to
be assigned to the same visual word as descriptors from text
patches, thus would have less inﬂuence on the index. Some
trial into solving this problem of background clutter is dis-
cussed in section 5.

Figure 6: Precision at one for test images with background
clutter.

5. Discussion and Future Work

As mentioned in the previous section, the mobile slide-
to-lecture-video search system is not very robust for back-
ground clutter, especially text clutter. Investigation into this
direction is limited by the time of this project. As a ﬁrst
step, I am able to segment the printed paper or the com-
puter screen from other non-text background by detecting
white pixels in the input photo. The algorithm ﬁrst converts
color image into grayscale image by taking the minimum
of three color channel values. Taking the minimum puts
penalty on object that are bright but have hue. Then the
grayscale image is sent to detect MSER or to binarization
using threshold. Some results using threshold binarization
are shown in ﬁgure 7.

More processing is needed to segment the slide region
we want out of other texts. This can be done by ﬁrst ﬁnding
the direction of text lines by Hough transform and rectifying
the image, and then projecting the binary image to horizon-
tal and vertical direction to ﬁnd blank margins around the
slide. Due to limitation of time this is not implemented well
to achieve satisfying results.

Another possibility to improve the performance of slide-
to-lecture-video search is to train the codebook on similar
text images instead of natural images. In generating index
for images, descriptors that appear often in the training set
have lower weight and don’t contribute much towards the
index. Therefore, training the codebook on natural image
would highlight text regions in building index for database
images and query images. However, text patches are repet-
itive and not discriminatory enough, we may want to high-
light the ﬁgures and images which are more unique in iden-
tifying slides. Therefore, I should try to collect another
training dataset of text images to see if performance of the
search system can be improved.

Test is also done in taking photos of only part of a slide.
Preliminary results show that the system could retrieve the

:

:

:

Figure 7: Segmentation of paper or computer screen from
input photos.

right video when non-trivial amount of slide is shown (more
than two lines of text). Systematic test can be done to inves-
tigate the robustness and limitation in matching parts of a
slide to lecture videos.

6. Conclusion

In this project, I have combined the previous research on
Fisher Vectors and Residual Enhanced Visual Vector to gen-
erate compact indices for database images to achieve efﬁ-
cient slide-to-lecture-video search. In order to exploit the
temporal redundancy of videos, an augmented database of
clean slide images is used instead of the database of video
keyframes. Using augmented database has led to 100X
memory reduction and has boosted the retrieval precision
from 18% to 90%. An Android Application is implemented
and tested on NVIDIA SHIELD tablet.

7. Acknowledgement

Special thanks to Andre Araujo for helpful discussions
during this project and for sharing part of the code to build
compact index for images.
In addition, thanks should be
given to Huizhong Chen for helpful suggestions on seg-
menting slides and analyzing layouts.

5

References
[1] A. Araujo, M. Makar, V. Chandrasekhar, D. Chen, S. Tsai,
H. Chen, R. Angst, and B. Girod. Efﬁcient video search using
image queries. In Proc. ICIP, 2014.

[2] D. Chen, S. Tsai, V. Chandrasekhar, G. Takacs, R. Vedantham,
R. Grzeszczuk, and B. Girod. Residual enhanced visual vector
as a compact signature for mobile visual search. Signal Pro-
cessing, 93(8):2316 – 2327, 2013.
Indexing of Large-Scale
Multimedia Signals.

[3] H. Jegou, M. Douze, C. Schmid, and P. Perez. Aggregat-
ing local descriptors into a compact image representation.
In Computer Vision and Pattern Recognition (CVPR), 2010
IEEE Conference on, pages 3304–3311, June 2010.

[4] D. Nister and H. Stewenius. Scalable recognition with a
In Computer Vision and Pattern Recogni-
vocabulary tree.
tion, 2006 IEEE Computer Society Conference on, volume 2,
pages 2161–2168, 2006.

[5] F. Perronnin, Y. Liu, J. Sanchez, and H. Poirier. Large-scale
image retrieval with compressed ﬁsher vectors. In Computer
Vision and Pattern Recognition (CVPR), 2010 IEEE Confer-
ence on, pages 3384–3391, June 2010.

[6] J. Sivic and A. Zisserman. Video google: a text retrieval
approach to object matching in videos. In Computer Vision,
2003. Proceedings. Ninth IEEE International Conference on,
pages 1470–1477 vol.2, Oct 2003.

[7] J. Sivic and A. Zisserman. Video google: Efﬁcient visual
In J. Ponce, M. Hebert, C. Schmid, and
search of videos.
A. Zisserman, editors, Toward Category-Level Object Recog-
nition, volume 4170 of Lecture Notes in Computer Science,
pages 127–144. Springer Berlin Heidelberg, 2006.

6

