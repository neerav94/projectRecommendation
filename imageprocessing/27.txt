Image Processing Pipeline for Facial Expression Recognition under Variable

Lighting

Ralph Ma, Amr Mohamed

ralphma@stanford.edu, amr1@stanford.edu

Abstract

Much research has been done in the ﬁeld of automated
facial expression recognition because of the importance of
facial expressions to understanding human interactions and
emotions. While several systems have achieved positive re-
sults using either facial model based classiﬁcation or fea-
ture based classiﬁcation, most of these systems have been
tested on subjects in constant lighting conditions. These
systems may thus be susceptible to lighting changes since
illumination contribute much more to image variation than
facial features. In this report, we augment the BU-4DFE
dataset by adding different lighting conditions to 3D images
of subjects performing different facial expressions. Then we
develop an image processing pipeline to rectify the effects
of illumination on the images, hoping to preserve high clas-
siﬁcation rate even in harsh lighting conditions. Then we
test our pipeline on two measurement: classiﬁcation accu-
racy based on a LDA model and SIFT keypoint repeatabil-
ity. For our results, we found that our image processing
pipeline helped improve classiﬁcation accuracy when per-
forming LDA to identify images in dark lighting conditions.
We did not ﬁnd signiﬁcant improvement in keypoint detec-
tion.

1. Introduction

Expression recognition is used in many different set-
tings. Medical researchers use expression recognition de-
vice to provide therapy to children and adults with Autism.
Companies and government can use expression detection
to gauge response to products and changes. Thus research
into this ﬁeld can have tremendous impact. Many studies
addressing this subject use images and datasets that are cre-
ated in a lab with uniform lighting conditions[1]. This is un-
derstandable because it allows for accurate evaluation of the
recognition algorithm. However, for most practical applica-
tions, the emotions recognition task is done in real-world
conditions where the lighting is diverse and far from being
uniform. In this study, we aim to study the effects of dif-

ferent lighting/shadowing on the emotions recognition task
and ﬁnd the best techniques to improve emotions recogni-
tion under variable lighting.

Illumination changes have huge effects on facial classiﬁ-
cation tasks. In fact, differences due to varying illumination
can be much larger than difference due to varying emotions
and even larger than differences among faces [2]. Thus it
is important for expression classiﬁcation systems to take
lighting into account.

This poses a problem even to the most sophisticated
computer vision techniques. For instance, convolutional
neural networks are one of the most successful classiﬁca-
tion tools so it is natural to wonder why they are not be-
ing used to solve the problem of emotion detection. One
of the main reasons is that only a small amount of labeled
training data is available. Convolutional neural networks
usually require a large amount of training data in order to
avoid overﬁtting. A common technique is to train the net-
work on a larger data set from a related domain. Once the
network parameters have converged an additional training
step is performed using the in-domain data to ﬁne-tune the
network weights. This allows convolutional networks to be
successfully applied to problems that do not have large la-
beled datasets. For our purposes, however, unavailability of
labeled data was a deterrent from using convolutional neural
networks.

We can address the problem of lighting through prepro-
cessing of our training/test sets or through feature engineer-
ing. We decide to focus on image preprocessing, because
different classiﬁcation systems vary widely in their feature
selection techniques. However, all classiﬁcation systems
can incorporate a preprocessing module into their pipelines

Image Processing Pipeline for Facial Expression Recognition under Variable

Lighting

Ralph Ma, Amr Mohamed

ralphma@stanford.edu, amr1@stanford.edu

Abstract

Much research has been done in the ﬁeld of automated
facial expression recognition because of the importance of
facial expressions to understanding human interactions and
emotions. While several systems have achieved positive re-
sults using either facial model based classiﬁcation or fea-
ture based classiﬁcation, most of these systems have been
tested on subjects in constant lighting conditions. These
systems may thus be susceptible to lighting changes since
illumination contribute much more to image variation than
facial features. In this report, we augment the BU-4DFE
dataset by adding different lighting conditions to 3D images
of subjects performing different facial expressions. Then we
develop an image processing pipeline to rectify the effects
of illumination on the images, hoping to preserve high clas-
siﬁcation rate even in harsh lighting conditions. Then we
test our pipeline on two measurement: classiﬁcation accu-
racy based on a LDA model and SIFT keypoint repeatabil-
ity. For our results, we found that our image processing
pipeline helped improve classiﬁcation accuracy when per-
forming LDA to identify images in dark lighting conditions.
We did not ﬁnd signiﬁcant improvement in keypoint detec-
tion.

1. Introduction

Expression recognition is used in many different set-
tings. Medical researchers use expression recognition de-
vice to provide therapy to children and adults with Autism.
Companies and government can use expression detection
to gauge response to products and changes. Thus research
into this ﬁeld can have tremendous impact. Many studies
addressing this subject use images and datasets that are cre-
ated in a lab with uniform lighting conditions[1]. This is un-
derstandable because it allows for accurate evaluation of the
recognition algorithm. However, for most practical applica-
tions, the emotions recognition task is done in real-world
conditions where the lighting is diverse and far from being
uniform. In this study, we aim to study the effects of dif-

ferent lighting/shadowing on the emotions recognition task
and ﬁnd the best techniques to improve emotions recogni-
tion under variable lighting.

Illumination changes have huge effects on facial classiﬁ-
cation tasks. In fact, differences due to varying illumination
can be much larger than difference due to varying emotions
and even larger than differences among faces [2]. Thus it
is important for expression classiﬁcation systems to take
lighting into account.

This poses a problem even to the most sophisticated
computer vision techniques. For instance, convolutional
neural networks are one of the most successful classiﬁca-
tion tools so it is natural to wonder why they are not be-
ing used to solve the problem of emotion detection. One
of the main reasons is that only a small amount of labeled
training data is available. Convolutional neural networks
usually require a large amount of training data in order to
avoid overﬁtting. A common technique is to train the net-
work on a larger data set from a related domain. Once the
network parameters have converged an additional training
step is performed using the in-domain data to ﬁne-tune the
network weights. This allows convolutional networks to be
successfully applied to problems that do not have large la-
beled datasets. For our purposes, however, unavailability of
labeled data was a deterrent from using convolutional neural
networks.

We can address the problem of lighting through prepro-
cessing of our training/test sets or through feature engineer-
ing. We decide to focus on image preprocessing, because
different classiﬁcation systems vary widely in their feature
selection techniques. However, all classiﬁcation systems
can incorporate a preprocessing module into their pipelines

without having to change any other modules.

3.2. Selective Filtering

2. Related Works

Most recent work on lighting normalization techniques
for facial images has been focused on the problem of im-
age recognition. These techniques range from illumination
modeling that needs training datasets to universal image
processing that can be performed without prior training. Be-
cause we lack a big dataset, we focus on image processing
techniques.

Several techniques have been proposed to remove light-
ing or to equalize the effects of lighting. Wang [1] sug-
gests the self-quotient face method which entails dividing
an image by the low-pass ﬁltered version of the same im-
age. Wang achieved ¿90% recognition accuracy on the Yale
B Dataset. Tan and Triggs [3] recommended a pipeline
that performed dynamic range correction, difference of
Gaussian ﬁltering, and contrast equalization . Using their
pipeline, the authors were able to improve a facial recogni-
tion system’s accuracy from 41.6% to 79.0%.

Both of these works suggest that low-pass ﬁlters are ef-
fective in removing light which are usually encoded in the
low frequencies of an image. However, removing too much
data from the lower frequency may remove important shad-
ing from the face. Furthermore, high-pass ﬁlter can help
eliminate noise and the effects of aliasing. However, high-
pass ﬁlters can eliminate details and ﬁner edges, such as
those around the eye and lips. Those facial features may
not be as important for facial recognition, which uses more
face structure and feature locations. However, in expression
recognition, the eyes and mouth region carry a huge amount
of information.

3. Image Processing Pipeline

Our pipeline consists of four stages. We assume that we
are given an image with the background cut out and only the
face. For our training images, we use the Viola-Jones Haar
Cascade Method implemented in OpenCV [4] to do so). We
base our pipeline on the TanTriggs preprocessing [3] with
adjustments suiting the facial expression recognition task.
Our pipeline consists of gamma correction, selective ﬁlter-
ing, and ﬁnally contrast equalization.

3.1. Gamma Adjustment

Both low lighting and high lighting conditions tend to
wash out features of the face. We found that a gamma cor-
rection of γ = 1.6 is optimal to increase the dynamic range
and thus, increase the contrast in the image. This highlights
the edges on the face which can be useful for expression
recognition.

Tan and Triggs apply a Difference of Gaussian ﬁlter in
order to remove the illumination variation in low frequen-
cies and noise/aliasing in the high frequencies. We found in
experiments that the bandpass ﬁlter does not preserve well
the details in the eye and mouth region. To counter against
this problem, we sharpen both the eye and mouth before ap-
plying the bandpass ﬁlter. For the eyes, we apply the Viola-
Jones technique [4] for eye detection and ﬁlter out false pos-
itives by only taking the two largest detected regions. For
the mouth, Viola-Jones does not work very well. Instead we
use the location heuristic that the mouth tends to be in the
lower center region of the face. For a 200 by 200 pixel im-
age, we take a 40 by 100 pixel long rectangle in the center
bottom of the image. After ﬁnding the boxes that contain
the eyes and mouth, we sharpen each region by the Unsharp
Masking technique of subtracting low-pass ﬁltered version
of the images from the image to be sharpened. A Guassian
ﬁlter with σ = 1.2 is used to create the low-pass ﬁltered
image. After sharpening only the eye and mouth, we ap-
ply a Difference of Gaussian ﬁlter to the entire image using
σ1 = 1 and σ2 = 2. The Difference of Gaussian ﬁlter helps
to remove noise and is also a good edge detector due to its
similarity to the Laplacian Filter.

3.3. Contrast Equalization

Contrast equalization is done according to methodology
suggested by Tan-Triggs. Contrast equalization is done in
two stages. For a given image I(x, y), we perform the fol-
lowing

I(x, y) ←

I(x, y) ←

I(x, y)

(mean(|I(x(cid:48), y(cid:48))|α))1/α

I(x, y)

(mean(min(γ,|I(x(cid:48), y(cid:48))|)α))1/α

(1)

(2)

α prevents the mean from being affected negatively by
large outlier values. γ is also a hard threshold used to trun-
cate large values in the normalization.

Figure 1 and Figure 2 show results of preprocessing.

4. Methods

In this section, we discuss the data generation and pre-
processing as well as the metrics we use to evaluate and
improve our pipeline. We test our pipeline on two fronts.
We measure improvements in restoring the SIFT keypoints
that are lost after changing illumination as well as improve-
ments in the classiﬁcation accuracy under varying lighting
conditions.

Image Processing Pipeline for Facial Expression Recognition under Variable

Lighting

Ralph Ma, Amr Mohamed

ralphma@stanford.edu, amr1@stanford.edu

Abstract

Much research has been done in the ﬁeld of automated
facial expression recognition because of the importance of
facial expressions to understanding human interactions and
emotions. While several systems have achieved positive re-
sults using either facial model based classiﬁcation or fea-
ture based classiﬁcation, most of these systems have been
tested on subjects in constant lighting conditions. These
systems may thus be susceptible to lighting changes since
illumination contribute much more to image variation than
facial features. In this report, we augment the BU-4DFE
dataset by adding different lighting conditions to 3D images
of subjects performing different facial expressions. Then we
develop an image processing pipeline to rectify the effects
of illumination on the images, hoping to preserve high clas-
siﬁcation rate even in harsh lighting conditions. Then we
test our pipeline on two measurement: classiﬁcation accu-
racy based on a LDA model and SIFT keypoint repeatabil-
ity. For our results, we found that our image processing
pipeline helped improve classiﬁcation accuracy when per-
forming LDA to identify images in dark lighting conditions.
We did not ﬁnd signiﬁcant improvement in keypoint detec-
tion.

1. Introduction

Expression recognition is used in many different set-
tings. Medical researchers use expression recognition de-
vice to provide therapy to children and adults with Autism.
Companies and government can use expression detection
to gauge response to products and changes. Thus research
into this ﬁeld can have tremendous impact. Many studies
addressing this subject use images and datasets that are cre-
ated in a lab with uniform lighting conditions[1]. This is un-
derstandable because it allows for accurate evaluation of the
recognition algorithm. However, for most practical applica-
tions, the emotions recognition task is done in real-world
conditions where the lighting is diverse and far from being
uniform. In this study, we aim to study the effects of dif-

ferent lighting/shadowing on the emotions recognition task
and ﬁnd the best techniques to improve emotions recogni-
tion under variable lighting.

Illumination changes have huge effects on facial classiﬁ-
cation tasks. In fact, differences due to varying illumination
can be much larger than difference due to varying emotions
and even larger than differences among faces [2]. Thus it
is important for expression classiﬁcation systems to take
lighting into account.

This poses a problem even to the most sophisticated
computer vision techniques. For instance, convolutional
neural networks are one of the most successful classiﬁca-
tion tools so it is natural to wonder why they are not be-
ing used to solve the problem of emotion detection. One
of the main reasons is that only a small amount of labeled
training data is available. Convolutional neural networks
usually require a large amount of training data in order to
avoid overﬁtting. A common technique is to train the net-
work on a larger data set from a related domain. Once the
network parameters have converged an additional training
step is performed using the in-domain data to ﬁne-tune the
network weights. This allows convolutional networks to be
successfully applied to problems that do not have large la-
beled datasets. For our purposes, however, unavailability of
labeled data was a deterrent from using convolutional neural
networks.

We can address the problem of lighting through prepro-
cessing of our training/test sets or through feature engineer-
ing. We decide to focus on image preprocessing, because
different classiﬁcation systems vary widely in their feature
selection techniques. However, all classiﬁcation systems
can incorporate a preprocessing module into their pipelines

without having to change any other modules.

3.2. Selective Filtering

2. Related Works

Most recent work on lighting normalization techniques
for facial images has been focused on the problem of im-
age recognition. These techniques range from illumination
modeling that needs training datasets to universal image
processing that can be performed without prior training. Be-
cause we lack a big dataset, we focus on image processing
techniques.

Several techniques have been proposed to remove light-
ing or to equalize the effects of lighting. Wang [1] sug-
gests the self-quotient face method which entails dividing
an image by the low-pass ﬁltered version of the same im-
age. Wang achieved ¿90% recognition accuracy on the Yale
B Dataset. Tan and Triggs [3] recommended a pipeline
that performed dynamic range correction, difference of
Gaussian ﬁltering, and contrast equalization . Using their
pipeline, the authors were able to improve a facial recogni-
tion system’s accuracy from 41.6% to 79.0%.

Both of these works suggest that low-pass ﬁlters are ef-
fective in removing light which are usually encoded in the
low frequencies of an image. However, removing too much
data from the lower frequency may remove important shad-
ing from the face. Furthermore, high-pass ﬁlter can help
eliminate noise and the effects of aliasing. However, high-
pass ﬁlters can eliminate details and ﬁner edges, such as
those around the eye and lips. Those facial features may
not be as important for facial recognition, which uses more
face structure and feature locations. However, in expression
recognition, the eyes and mouth region carry a huge amount
of information.

3. Image Processing Pipeline

Our pipeline consists of four stages. We assume that we
are given an image with the background cut out and only the
face. For our training images, we use the Viola-Jones Haar
Cascade Method implemented in OpenCV [4] to do so). We
base our pipeline on the TanTriggs preprocessing [3] with
adjustments suiting the facial expression recognition task.
Our pipeline consists of gamma correction, selective ﬁlter-
ing, and ﬁnally contrast equalization.

3.1. Gamma Adjustment

Both low lighting and high lighting conditions tend to
wash out features of the face. We found that a gamma cor-
rection of γ = 1.6 is optimal to increase the dynamic range
and thus, increase the contrast in the image. This highlights
the edges on the face which can be useful for expression
recognition.

Tan and Triggs apply a Difference of Gaussian ﬁlter in
order to remove the illumination variation in low frequen-
cies and noise/aliasing in the high frequencies. We found in
experiments that the bandpass ﬁlter does not preserve well
the details in the eye and mouth region. To counter against
this problem, we sharpen both the eye and mouth before ap-
plying the bandpass ﬁlter. For the eyes, we apply the Viola-
Jones technique [4] for eye detection and ﬁlter out false pos-
itives by only taking the two largest detected regions. For
the mouth, Viola-Jones does not work very well. Instead we
use the location heuristic that the mouth tends to be in the
lower center region of the face. For a 200 by 200 pixel im-
age, we take a 40 by 100 pixel long rectangle in the center
bottom of the image. After ﬁnding the boxes that contain
the eyes and mouth, we sharpen each region by the Unsharp
Masking technique of subtracting low-pass ﬁltered version
of the images from the image to be sharpened. A Guassian
ﬁlter with σ = 1.2 is used to create the low-pass ﬁltered
image. After sharpening only the eye and mouth, we ap-
ply a Difference of Gaussian ﬁlter to the entire image using
σ1 = 1 and σ2 = 2. The Difference of Gaussian ﬁlter helps
to remove noise and is also a good edge detector due to its
similarity to the Laplacian Filter.

3.3. Contrast Equalization

Contrast equalization is done according to methodology
suggested by Tan-Triggs. Contrast equalization is done in
two stages. For a given image I(x, y), we perform the fol-
lowing

I(x, y) ←

I(x, y) ←

I(x, y)

(mean(|I(x(cid:48), y(cid:48))|α))1/α

I(x, y)

(mean(min(γ,|I(x(cid:48), y(cid:48))|)α))1/α

(1)

(2)

α prevents the mean from being affected negatively by
large outlier values. γ is also a hard threshold used to trun-
cate large values in the normalization.

Figure 1 and Figure 2 show results of preprocessing.

4. Methods

In this section, we discuss the data generation and pre-
processing as well as the metrics we use to evaluate and
improve our pipeline. We test our pipeline on two fronts.
We measure improvements in restoring the SIFT keypoints
that are lost after changing illumination as well as improve-
ments in the classiﬁcation accuracy under varying lighting
conditions.

4.2. SIFT Keypoints

Figure 1. Face before preprocessing

Figure 2. Face after preprocessing

4.1. Data Generation

The starting point

To create different

is the BU-4DFE dataset which
contains 3D video data of 101 subjects (43M, 58F) per-
forming six different emotions: Anger, Disgust, Sadness,
Happiness, Fear, and Surprise[5]. For each subject and
each video, we pick a frame representative of the subject
performing the emotion.
lighting
conditions, we load the frame into the graphics software
Blender and augment the 3D image by placing a point light
in the space with the 3D image and vary the illumination
of the light. We placed the light 45 degrees away from
the plane of the camera on the left side of the face and
used 6 different energy settings for the point light (units
dictated by Blender): 0, 1, 3, 5, 10, 15. In this project, we
use the light intensity of 5 as the control illumination and
other light intensities as test illuminations. We then project
the frontal rendering of the face into a two dimensional
image used for testing. Therefore, for 101 subjects with 6
different emotions and 6 different lighting conditions, we
produced a dataset of 3636 images. Generated images are
shown on top of this page

Figure 3. Repeatability of Sift Keypoints without preprocessing

We evaluate the robustness of SIFT keypoints detector
to illumination changes. We compare the SIFT keypoints
found for our control illumination to the SIFT keypoints
found for our test illuminations. For every keypoint found
in the images with control illumination, we take the loca-
tion (x, y) of that keypoint and search for a keypoint in the
image of a same subject with test illumination that has a lo-
cation within an Euclidean distance of 2 to (x, y). If we ﬁnd
such a keypoint, we declare a match. For each comparison
of test illumination to control illumination of the same sub-
ject, we measure the percentage of matches relative to the
number of keypoints found in the image with control illu-
mination. We are hoping to measure the repeatability of the
SIFT keypoint detector in changing illumination conditions.

4.3. Fisherfaces

We evaluate the effects of preprocessing pipeline on a
classiﬁer that uses Fisherface. Fisherface is very sensitive
to illumination changes, because lighting variations is much
more signiﬁcant than facial feature variations. We form a
baseline by performing Fisherface on unprocessed images.
Then, we measure improvements in the classiﬁcation ac-
curacy to multiple different light intensity after applying
the processing pipeline to the images. Our initial classiﬁ-
cation produced poor accuracy which led us to investigate
our dataset. After examination, we decided to ask human

Image Processing Pipeline for Facial Expression Recognition under Variable

Lighting

Ralph Ma, Amr Mohamed

ralphma@stanford.edu, amr1@stanford.edu

Abstract

Much research has been done in the ﬁeld of automated
facial expression recognition because of the importance of
facial expressions to understanding human interactions and
emotions. While several systems have achieved positive re-
sults using either facial model based classiﬁcation or fea-
ture based classiﬁcation, most of these systems have been
tested on subjects in constant lighting conditions. These
systems may thus be susceptible to lighting changes since
illumination contribute much more to image variation than
facial features. In this report, we augment the BU-4DFE
dataset by adding different lighting conditions to 3D images
of subjects performing different facial expressions. Then we
develop an image processing pipeline to rectify the effects
of illumination on the images, hoping to preserve high clas-
siﬁcation rate even in harsh lighting conditions. Then we
test our pipeline on two measurement: classiﬁcation accu-
racy based on a LDA model and SIFT keypoint repeatabil-
ity. For our results, we found that our image processing
pipeline helped improve classiﬁcation accuracy when per-
forming LDA to identify images in dark lighting conditions.
We did not ﬁnd signiﬁcant improvement in keypoint detec-
tion.

1. Introduction

Expression recognition is used in many different set-
tings. Medical researchers use expression recognition de-
vice to provide therapy to children and adults with Autism.
Companies and government can use expression detection
to gauge response to products and changes. Thus research
into this ﬁeld can have tremendous impact. Many studies
addressing this subject use images and datasets that are cre-
ated in a lab with uniform lighting conditions[1]. This is un-
derstandable because it allows for accurate evaluation of the
recognition algorithm. However, for most practical applica-
tions, the emotions recognition task is done in real-world
conditions where the lighting is diverse and far from being
uniform. In this study, we aim to study the effects of dif-

ferent lighting/shadowing on the emotions recognition task
and ﬁnd the best techniques to improve emotions recogni-
tion under variable lighting.

Illumination changes have huge effects on facial classiﬁ-
cation tasks. In fact, differences due to varying illumination
can be much larger than difference due to varying emotions
and even larger than differences among faces [2]. Thus it
is important for expression classiﬁcation systems to take
lighting into account.

This poses a problem even to the most sophisticated
computer vision techniques. For instance, convolutional
neural networks are one of the most successful classiﬁca-
tion tools so it is natural to wonder why they are not be-
ing used to solve the problem of emotion detection. One
of the main reasons is that only a small amount of labeled
training data is available. Convolutional neural networks
usually require a large amount of training data in order to
avoid overﬁtting. A common technique is to train the net-
work on a larger data set from a related domain. Once the
network parameters have converged an additional training
step is performed using the in-domain data to ﬁne-tune the
network weights. This allows convolutional networks to be
successfully applied to problems that do not have large la-
beled datasets. For our purposes, however, unavailability of
labeled data was a deterrent from using convolutional neural
networks.

We can address the problem of lighting through prepro-
cessing of our training/test sets or through feature engineer-
ing. We decide to focus on image preprocessing, because
different classiﬁcation systems vary widely in their feature
selection techniques. However, all classiﬁcation systems
can incorporate a preprocessing module into their pipelines

without having to change any other modules.

3.2. Selective Filtering

2. Related Works

Most recent work on lighting normalization techniques
for facial images has been focused on the problem of im-
age recognition. These techniques range from illumination
modeling that needs training datasets to universal image
processing that can be performed without prior training. Be-
cause we lack a big dataset, we focus on image processing
techniques.

Several techniques have been proposed to remove light-
ing or to equalize the effects of lighting. Wang [1] sug-
gests the self-quotient face method which entails dividing
an image by the low-pass ﬁltered version of the same im-
age. Wang achieved ¿90% recognition accuracy on the Yale
B Dataset. Tan and Triggs [3] recommended a pipeline
that performed dynamic range correction, difference of
Gaussian ﬁltering, and contrast equalization . Using their
pipeline, the authors were able to improve a facial recogni-
tion system’s accuracy from 41.6% to 79.0%.

Both of these works suggest that low-pass ﬁlters are ef-
fective in removing light which are usually encoded in the
low frequencies of an image. However, removing too much
data from the lower frequency may remove important shad-
ing from the face. Furthermore, high-pass ﬁlter can help
eliminate noise and the effects of aliasing. However, high-
pass ﬁlters can eliminate details and ﬁner edges, such as
those around the eye and lips. Those facial features may
not be as important for facial recognition, which uses more
face structure and feature locations. However, in expression
recognition, the eyes and mouth region carry a huge amount
of information.

3. Image Processing Pipeline

Our pipeline consists of four stages. We assume that we
are given an image with the background cut out and only the
face. For our training images, we use the Viola-Jones Haar
Cascade Method implemented in OpenCV [4] to do so). We
base our pipeline on the TanTriggs preprocessing [3] with
adjustments suiting the facial expression recognition task.
Our pipeline consists of gamma correction, selective ﬁlter-
ing, and ﬁnally contrast equalization.

3.1. Gamma Adjustment

Both low lighting and high lighting conditions tend to
wash out features of the face. We found that a gamma cor-
rection of γ = 1.6 is optimal to increase the dynamic range
and thus, increase the contrast in the image. This highlights
the edges on the face which can be useful for expression
recognition.

Tan and Triggs apply a Difference of Gaussian ﬁlter in
order to remove the illumination variation in low frequen-
cies and noise/aliasing in the high frequencies. We found in
experiments that the bandpass ﬁlter does not preserve well
the details in the eye and mouth region. To counter against
this problem, we sharpen both the eye and mouth before ap-
plying the bandpass ﬁlter. For the eyes, we apply the Viola-
Jones technique [4] for eye detection and ﬁlter out false pos-
itives by only taking the two largest detected regions. For
the mouth, Viola-Jones does not work very well. Instead we
use the location heuristic that the mouth tends to be in the
lower center region of the face. For a 200 by 200 pixel im-
age, we take a 40 by 100 pixel long rectangle in the center
bottom of the image. After ﬁnding the boxes that contain
the eyes and mouth, we sharpen each region by the Unsharp
Masking technique of subtracting low-pass ﬁltered version
of the images from the image to be sharpened. A Guassian
ﬁlter with σ = 1.2 is used to create the low-pass ﬁltered
image. After sharpening only the eye and mouth, we ap-
ply a Difference of Gaussian ﬁlter to the entire image using
σ1 = 1 and σ2 = 2. The Difference of Gaussian ﬁlter helps
to remove noise and is also a good edge detector due to its
similarity to the Laplacian Filter.

3.3. Contrast Equalization

Contrast equalization is done according to methodology
suggested by Tan-Triggs. Contrast equalization is done in
two stages. For a given image I(x, y), we perform the fol-
lowing

I(x, y) ←

I(x, y) ←

I(x, y)

(mean(|I(x(cid:48), y(cid:48))|α))1/α

I(x, y)

(mean(min(γ,|I(x(cid:48), y(cid:48))|)α))1/α

(1)

(2)

α prevents the mean from being affected negatively by
large outlier values. γ is also a hard threshold used to trun-
cate large values in the normalization.

Figure 1 and Figure 2 show results of preprocessing.

4. Methods

In this section, we discuss the data generation and pre-
processing as well as the metrics we use to evaluate and
improve our pipeline. We test our pipeline on two fronts.
We measure improvements in restoring the SIFT keypoints
that are lost after changing illumination as well as improve-
ments in the classiﬁcation accuracy under varying lighting
conditions.

4.2. SIFT Keypoints

Figure 1. Face before preprocessing

Figure 2. Face after preprocessing

4.1. Data Generation

The starting point

To create different

is the BU-4DFE dataset which
contains 3D video data of 101 subjects (43M, 58F) per-
forming six different emotions: Anger, Disgust, Sadness,
Happiness, Fear, and Surprise[5]. For each subject and
each video, we pick a frame representative of the subject
performing the emotion.
lighting
conditions, we load the frame into the graphics software
Blender and augment the 3D image by placing a point light
in the space with the 3D image and vary the illumination
of the light. We placed the light 45 degrees away from
the plane of the camera on the left side of the face and
used 6 different energy settings for the point light (units
dictated by Blender): 0, 1, 3, 5, 10, 15. In this project, we
use the light intensity of 5 as the control illumination and
other light intensities as test illuminations. We then project
the frontal rendering of the face into a two dimensional
image used for testing. Therefore, for 101 subjects with 6
different emotions and 6 different lighting conditions, we
produced a dataset of 3636 images. Generated images are
shown on top of this page

Figure 3. Repeatability of Sift Keypoints without preprocessing

We evaluate the robustness of SIFT keypoints detector
to illumination changes. We compare the SIFT keypoints
found for our control illumination to the SIFT keypoints
found for our test illuminations. For every keypoint found
in the images with control illumination, we take the loca-
tion (x, y) of that keypoint and search for a keypoint in the
image of a same subject with test illumination that has a lo-
cation within an Euclidean distance of 2 to (x, y). If we ﬁnd
such a keypoint, we declare a match. For each comparison
of test illumination to control illumination of the same sub-
ject, we measure the percentage of matches relative to the
number of keypoints found in the image with control illu-
mination. We are hoping to measure the repeatability of the
SIFT keypoint detector in changing illumination conditions.

4.3. Fisherfaces

We evaluate the effects of preprocessing pipeline on a
classiﬁer that uses Fisherface. Fisherface is very sensitive
to illumination changes, because lighting variations is much
more signiﬁcant than facial feature variations. We form a
baseline by performing Fisherface on unprocessed images.
Then, we measure improvements in the classiﬁcation ac-
curacy to multiple different light intensity after applying
the processing pipeline to the images. Our initial classiﬁ-
cation produced poor accuracy which led us to investigate
our dataset. After examination, we decided to ask human

subjects to do the classiﬁcation task. Many human subjects
misclassiﬁed anger, disgust, and fear. The human accuracy
for the dataset is 0.53. We concluded that the dataset was
not great for 6 class classiﬁcation and decided to limit the
classiﬁcation to only 2 classes that are easier to classify:
Happy and Sad.

For our classiﬁcation task, we ﬁrst divide the 101 sub-
jects into a test set of 30 subjects and a training set of 71
subjects. We ﬁnd the Fisherfaces by performing LDA on
all training subjects with illumination 5. Then we ﬁnd a
threshold for the Fisherface projection score that best clas-
siﬁes between happy and sad subjects. Finally, we test the
30 subjects in all 6 illuminations using the model that is
trained. The results are shown in Figure 4.

5. Results

Figure 5. Results for FisherFace Classiﬁcation

Figure 4. Results for FisherFace Classiﬁcation

Figure 6. SIFT Keypoints

Figure 4 shows the the accuracy of Fisherface classiﬁer
for varying light intensities. We see that without prepro-
cessing, Fisherface classiﬁer loses accuracy when the light
intensity is too low. At intensity of 0, no preprocessing Fish-
erface (np-Fisherface) achieves accuracy of .55 and Fisher-
Face with preprocessing (p-Fisherface) achieves accuracy
of .8667. Furthermore, we see that p-Fisherface achieves
higher results in higher lighting conditions (10, 15).

Figure 5 shows the results for SIFT repeatability testing.
Comparing Figure 3 with ﬁgure 5, we see that our prepro-
cessing did not result in better Sift Keypoint detection.

6. Discussion

In order to potentially gain insights on how to design
our processing pipeline, we examined the matched and
missed SIFT keypoints.
In ﬁgure 6, the green points are
the matched SIFT keypoints of the control illumination and
the red points are the missed keypoints. From ﬁgure 6, we

can see that all the keypoints that are on the left hand side
of the face are missed. This is because the shadow created
by the angle of the light caused a loss in some facial fea-
tures. This explains the low accuracy in emotion classiﬁca-
tion under dim lighting conditions. This directed us towards
improving the gamma correction portion of our processing
pipeline as well as sharpening the eyes and mouth because
they tell us the most about emotions and are rarely affected
by extreme lighting/shadows.

Our preprocessing technique improved the results for
classiﬁcation based on Fisherfaces. The Fisherface used for
classiﬁcation is shown in Figure 7 and Figure 8. Figure 7
captures the lip shape of a smile and has cheek lines formed
by the lips pushing upwards. Furthermore, we see the eyes
are smaller and slanted upward. Both the eyes and mouth
are features that are sharpened in our image preprocessing.

Image Processing Pipeline for Facial Expression Recognition under Variable

Lighting

Ralph Ma, Amr Mohamed

ralphma@stanford.edu, amr1@stanford.edu

Abstract

Much research has been done in the ﬁeld of automated
facial expression recognition because of the importance of
facial expressions to understanding human interactions and
emotions. While several systems have achieved positive re-
sults using either facial model based classiﬁcation or fea-
ture based classiﬁcation, most of these systems have been
tested on subjects in constant lighting conditions. These
systems may thus be susceptible to lighting changes since
illumination contribute much more to image variation than
facial features. In this report, we augment the BU-4DFE
dataset by adding different lighting conditions to 3D images
of subjects performing different facial expressions. Then we
develop an image processing pipeline to rectify the effects
of illumination on the images, hoping to preserve high clas-
siﬁcation rate even in harsh lighting conditions. Then we
test our pipeline on two measurement: classiﬁcation accu-
racy based on a LDA model and SIFT keypoint repeatabil-
ity. For our results, we found that our image processing
pipeline helped improve classiﬁcation accuracy when per-
forming LDA to identify images in dark lighting conditions.
We did not ﬁnd signiﬁcant improvement in keypoint detec-
tion.

1. Introduction

Expression recognition is used in many different set-
tings. Medical researchers use expression recognition de-
vice to provide therapy to children and adults with Autism.
Companies and government can use expression detection
to gauge response to products and changes. Thus research
into this ﬁeld can have tremendous impact. Many studies
addressing this subject use images and datasets that are cre-
ated in a lab with uniform lighting conditions[1]. This is un-
derstandable because it allows for accurate evaluation of the
recognition algorithm. However, for most practical applica-
tions, the emotions recognition task is done in real-world
conditions where the lighting is diverse and far from being
uniform. In this study, we aim to study the effects of dif-

ferent lighting/shadowing on the emotions recognition task
and ﬁnd the best techniques to improve emotions recogni-
tion under variable lighting.

Illumination changes have huge effects on facial classiﬁ-
cation tasks. In fact, differences due to varying illumination
can be much larger than difference due to varying emotions
and even larger than differences among faces [2]. Thus it
is important for expression classiﬁcation systems to take
lighting into account.

This poses a problem even to the most sophisticated
computer vision techniques. For instance, convolutional
neural networks are one of the most successful classiﬁca-
tion tools so it is natural to wonder why they are not be-
ing used to solve the problem of emotion detection. One
of the main reasons is that only a small amount of labeled
training data is available. Convolutional neural networks
usually require a large amount of training data in order to
avoid overﬁtting. A common technique is to train the net-
work on a larger data set from a related domain. Once the
network parameters have converged an additional training
step is performed using the in-domain data to ﬁne-tune the
network weights. This allows convolutional networks to be
successfully applied to problems that do not have large la-
beled datasets. For our purposes, however, unavailability of
labeled data was a deterrent from using convolutional neural
networks.

We can address the problem of lighting through prepro-
cessing of our training/test sets or through feature engineer-
ing. We decide to focus on image preprocessing, because
different classiﬁcation systems vary widely in their feature
selection techniques. However, all classiﬁcation systems
can incorporate a preprocessing module into their pipelines

without having to change any other modules.

3.2. Selective Filtering

2. Related Works

Most recent work on lighting normalization techniques
for facial images has been focused on the problem of im-
age recognition. These techniques range from illumination
modeling that needs training datasets to universal image
processing that can be performed without prior training. Be-
cause we lack a big dataset, we focus on image processing
techniques.

Several techniques have been proposed to remove light-
ing or to equalize the effects of lighting. Wang [1] sug-
gests the self-quotient face method which entails dividing
an image by the low-pass ﬁltered version of the same im-
age. Wang achieved ¿90% recognition accuracy on the Yale
B Dataset. Tan and Triggs [3] recommended a pipeline
that performed dynamic range correction, difference of
Gaussian ﬁltering, and contrast equalization . Using their
pipeline, the authors were able to improve a facial recogni-
tion system’s accuracy from 41.6% to 79.0%.

Both of these works suggest that low-pass ﬁlters are ef-
fective in removing light which are usually encoded in the
low frequencies of an image. However, removing too much
data from the lower frequency may remove important shad-
ing from the face. Furthermore, high-pass ﬁlter can help
eliminate noise and the effects of aliasing. However, high-
pass ﬁlters can eliminate details and ﬁner edges, such as
those around the eye and lips. Those facial features may
not be as important for facial recognition, which uses more
face structure and feature locations. However, in expression
recognition, the eyes and mouth region carry a huge amount
of information.

3. Image Processing Pipeline

Our pipeline consists of four stages. We assume that we
are given an image with the background cut out and only the
face. For our training images, we use the Viola-Jones Haar
Cascade Method implemented in OpenCV [4] to do so). We
base our pipeline on the TanTriggs preprocessing [3] with
adjustments suiting the facial expression recognition task.
Our pipeline consists of gamma correction, selective ﬁlter-
ing, and ﬁnally contrast equalization.

3.1. Gamma Adjustment

Both low lighting and high lighting conditions tend to
wash out features of the face. We found that a gamma cor-
rection of γ = 1.6 is optimal to increase the dynamic range
and thus, increase the contrast in the image. This highlights
the edges on the face which can be useful for expression
recognition.

Tan and Triggs apply a Difference of Gaussian ﬁlter in
order to remove the illumination variation in low frequen-
cies and noise/aliasing in the high frequencies. We found in
experiments that the bandpass ﬁlter does not preserve well
the details in the eye and mouth region. To counter against
this problem, we sharpen both the eye and mouth before ap-
plying the bandpass ﬁlter. For the eyes, we apply the Viola-
Jones technique [4] for eye detection and ﬁlter out false pos-
itives by only taking the two largest detected regions. For
the mouth, Viola-Jones does not work very well. Instead we
use the location heuristic that the mouth tends to be in the
lower center region of the face. For a 200 by 200 pixel im-
age, we take a 40 by 100 pixel long rectangle in the center
bottom of the image. After ﬁnding the boxes that contain
the eyes and mouth, we sharpen each region by the Unsharp
Masking technique of subtracting low-pass ﬁltered version
of the images from the image to be sharpened. A Guassian
ﬁlter with σ = 1.2 is used to create the low-pass ﬁltered
image. After sharpening only the eye and mouth, we ap-
ply a Difference of Gaussian ﬁlter to the entire image using
σ1 = 1 and σ2 = 2. The Difference of Gaussian ﬁlter helps
to remove noise and is also a good edge detector due to its
similarity to the Laplacian Filter.

3.3. Contrast Equalization

Contrast equalization is done according to methodology
suggested by Tan-Triggs. Contrast equalization is done in
two stages. For a given image I(x, y), we perform the fol-
lowing

I(x, y) ←

I(x, y) ←

I(x, y)

(mean(|I(x(cid:48), y(cid:48))|α))1/α

I(x, y)

(mean(min(γ,|I(x(cid:48), y(cid:48))|)α))1/α

(1)

(2)

α prevents the mean from being affected negatively by
large outlier values. γ is also a hard threshold used to trun-
cate large values in the normalization.

Figure 1 and Figure 2 show results of preprocessing.

4. Methods

In this section, we discuss the data generation and pre-
processing as well as the metrics we use to evaluate and
improve our pipeline. We test our pipeline on two fronts.
We measure improvements in restoring the SIFT keypoints
that are lost after changing illumination as well as improve-
ments in the classiﬁcation accuracy under varying lighting
conditions.

4.2. SIFT Keypoints

Figure 1. Face before preprocessing

Figure 2. Face after preprocessing

4.1. Data Generation

The starting point

To create different

is the BU-4DFE dataset which
contains 3D video data of 101 subjects (43M, 58F) per-
forming six different emotions: Anger, Disgust, Sadness,
Happiness, Fear, and Surprise[5]. For each subject and
each video, we pick a frame representative of the subject
performing the emotion.
lighting
conditions, we load the frame into the graphics software
Blender and augment the 3D image by placing a point light
in the space with the 3D image and vary the illumination
of the light. We placed the light 45 degrees away from
the plane of the camera on the left side of the face and
used 6 different energy settings for the point light (units
dictated by Blender): 0, 1, 3, 5, 10, 15. In this project, we
use the light intensity of 5 as the control illumination and
other light intensities as test illuminations. We then project
the frontal rendering of the face into a two dimensional
image used for testing. Therefore, for 101 subjects with 6
different emotions and 6 different lighting conditions, we
produced a dataset of 3636 images. Generated images are
shown on top of this page

Figure 3. Repeatability of Sift Keypoints without preprocessing

We evaluate the robustness of SIFT keypoints detector
to illumination changes. We compare the SIFT keypoints
found for our control illumination to the SIFT keypoints
found for our test illuminations. For every keypoint found
in the images with control illumination, we take the loca-
tion (x, y) of that keypoint and search for a keypoint in the
image of a same subject with test illumination that has a lo-
cation within an Euclidean distance of 2 to (x, y). If we ﬁnd
such a keypoint, we declare a match. For each comparison
of test illumination to control illumination of the same sub-
ject, we measure the percentage of matches relative to the
number of keypoints found in the image with control illu-
mination. We are hoping to measure the repeatability of the
SIFT keypoint detector in changing illumination conditions.

4.3. Fisherfaces

We evaluate the effects of preprocessing pipeline on a
classiﬁer that uses Fisherface. Fisherface is very sensitive
to illumination changes, because lighting variations is much
more signiﬁcant than facial feature variations. We form a
baseline by performing Fisherface on unprocessed images.
Then, we measure improvements in the classiﬁcation ac-
curacy to multiple different light intensity after applying
the processing pipeline to the images. Our initial classiﬁ-
cation produced poor accuracy which led us to investigate
our dataset. After examination, we decided to ask human

subjects to do the classiﬁcation task. Many human subjects
misclassiﬁed anger, disgust, and fear. The human accuracy
for the dataset is 0.53. We concluded that the dataset was
not great for 6 class classiﬁcation and decided to limit the
classiﬁcation to only 2 classes that are easier to classify:
Happy and Sad.

For our classiﬁcation task, we ﬁrst divide the 101 sub-
jects into a test set of 30 subjects and a training set of 71
subjects. We ﬁnd the Fisherfaces by performing LDA on
all training subjects with illumination 5. Then we ﬁnd a
threshold for the Fisherface projection score that best clas-
siﬁes between happy and sad subjects. Finally, we test the
30 subjects in all 6 illuminations using the model that is
trained. The results are shown in Figure 4.

5. Results

Figure 5. Results for FisherFace Classiﬁcation

Figure 4. Results for FisherFace Classiﬁcation

Figure 6. SIFT Keypoints

Figure 4 shows the the accuracy of Fisherface classiﬁer
for varying light intensities. We see that without prepro-
cessing, Fisherface classiﬁer loses accuracy when the light
intensity is too low. At intensity of 0, no preprocessing Fish-
erface (np-Fisherface) achieves accuracy of .55 and Fisher-
Face with preprocessing (p-Fisherface) achieves accuracy
of .8667. Furthermore, we see that p-Fisherface achieves
higher results in higher lighting conditions (10, 15).

Figure 5 shows the results for SIFT repeatability testing.
Comparing Figure 3 with ﬁgure 5, we see that our prepro-
cessing did not result in better Sift Keypoint detection.

6. Discussion

In order to potentially gain insights on how to design
our processing pipeline, we examined the matched and
missed SIFT keypoints.
In ﬁgure 6, the green points are
the matched SIFT keypoints of the control illumination and
the red points are the missed keypoints. From ﬁgure 6, we

can see that all the keypoints that are on the left hand side
of the face are missed. This is because the shadow created
by the angle of the light caused a loss in some facial fea-
tures. This explains the low accuracy in emotion classiﬁca-
tion under dim lighting conditions. This directed us towards
improving the gamma correction portion of our processing
pipeline as well as sharpening the eyes and mouth because
they tell us the most about emotions and are rarely affected
by extreme lighting/shadows.

Our preprocessing technique improved the results for
classiﬁcation based on Fisherfaces. The Fisherface used for
classiﬁcation is shown in Figure 7 and Figure 8. Figure 7
captures the lip shape of a smile and has cheek lines formed
by the lips pushing upwards. Furthermore, we see the eyes
are smaller and slanted upward. Both the eyes and mouth
are features that are sharpened in our image preprocessing.

9. Acknowledgements

We like to thank the fall 2015 staff of EE368. We also

want to thank Azar Fazel for helping us with Blender.

References
[1] Yangsheng Wang Haitao Wang, Stan Z Li. Face recog-
nition under varying lighting conditions using self quo-
tient image. Proceedings of the Sixth IEEE Inter-
national Conference on Automatic Face and Gesture
Recognition, 2004.

[2] Beat Fasel and Juergen Luettin. Automatic facial ex-
Pattern Recognition,

pression analysis: A survey.
36(1), 1999.

[3] Xiaoyang Tan and B. Triggs. Enhanced local texture
feature sets for face recognition under difﬁcult lighting
IEEE Transactions on Image Processing,
conditions.
19(6).

[4] Michael Jones Paul Viola. Rapid object detection using
a boosted cascade of simple features. Computer Vision
and Pattern Recognition, 2001.

[5] Lijun Yin; Xiaozhou Wei; Yi Sun;

Jun Wang;
Matthew J. Rosato. A 3d facial expression database for
facial behavior research. 7th International Conference
on Automatic Face and Gesture Recognition, 2006.

Figure 7. Fisherface with preprocessing

Figure 8. Fisherface without preprocessing

7. Conclusion

We were able to improve the classiﬁcation accuracy for
dim lighting conditions by studying the missed keypoints
and missed features and adding preprocessing modules to
ﬁx them. A future extension of this project would be to ex-
tend the improvements to a range of complex human emo-
tions, such as stress, wonder, pride, etc. Another future
extension that can be done is to improve the robustness of
the preprocessing module by including more varied lighting
and shadows from different angles and from more than one
source to create complex lighting conditions.

8. Work Division

Ralph Ma: Simulated lighting conditions in blender,

Implemented Fisherfaces, Incorporated Tan Triggs

Amr Mohamed: Implemented SIFT Keypoints detector,

Tuned processing parameters

